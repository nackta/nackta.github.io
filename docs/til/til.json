[
  {
    "path": "til/2022-05-20-pytorch-tutorial/",
    "title": "Pytorch tutorial",
    "description": "ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ íŒŒì´í† ì¹˜ì— ëŒ€í•´ ì•Œì•„ë³´ì",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-20",
    "categories": [
      "pytorch"
    ],
    "contents": "\r\n\r\nContents\r\ntorch tensor\r\ntensorì˜ parameter\r\ntensorì˜ ìƒì„±\r\ndevice ì§€ì •\r\ntensor ë‹¤ë£¨ê¸°\r\ntonsorì˜ ì—°ì‚°\r\n\r\nDataset\r\nDataLoader\r\ntorch.nn.Module\r\n\r\nsocar ë¶€íŠ¸ìº í”„ì˜ ê°•ì˜ ì¤‘ ì¼ë¶€ë¥¼ ì •ë¦¬í•œ ë‚´ìš©ì…ë‹ˆë‹¤.\r\ntorch tensor\r\ntensorëŠ” numpyì˜ ndarrayì²˜ëŸ¼ ë‹¤ì°¨ì› ë°ì´í„° ë°°ì—´ì…ë‹ˆë‹¤. tensorë¥¼\r\nìƒì„±í•  ë•ŒëŠ” listë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆì§€ë§Œ ndarrayë¥¼ ì‚¬ìš©í•  ìˆ˜ë„\r\nìˆìŠµë‹ˆë‹¤.\r\nndarrayì™€ tensorì˜ ì°¨ì´ì ì€ tensorì—ëŠ” back propagation(ì—­ì „íŒŒ)ë¥¼\r\në‹¤ë£¨ê¸° ìœ„í•´ forward pass(ìˆœì „íŒŒ)ì—ì„œ ì „ë‹¬ëœ ê°’ê³¼ ì—°ì‚°ì˜ ì¢…ë¥˜ë¥¼ ê¸°ì–µí•  ìˆ˜\r\nìˆìŠµë‹ˆë‹¤.(gradient ê°’ ì €ì¥)\r\ntensorì˜ parameter\r\ndata : listë‚˜ ndarray ë“± arrayë°ì´í„°\r\ndtype : ë°ì´í„° íƒ€ì…\r\ndevice : tensorê°€ ìœ„ì¹˜í•´ìˆëŠ” device ì§€ì •\r\nrequires_grad : gradient ê°’ ì €ì¥ ìœ ë¬´\r\npin_memory : Trueì‹œ pinned memoryì— í• ë‹¹, CPU tensorì—ì„œ\r\nê°€ëŠ¥\r\ntensorì˜ ìƒì„±\r\n\r\na = torch.tensor([[1.0, 4.0], [4.0, 3.0]])\r\nb = torch.tensor([[4, 3], [1, 4], [1, 2]])\r\nprint(a)\r\ntensor([[1., 4.],\r\n        [4., 3.]])\r\nprint(b)\r\ntensor([[4, 3],\r\n        [1, 4],\r\n        [1, 2]])\r\n\r\nndarrayì™€ ë¹„ìŠ·í•œ ìƒì„± ë°©ë²•ì…ë‹ˆë‹¤.\r\n\r\nprint(a.dtype, a.shape)\r\ntorch.float32 torch.Size([2, 2])\r\nprint(b.dtype, b.shape)\r\ntorch.int64 torch.Size([3, 2])\r\n\r\ntorch.ones(*size) : ëª¨ë“  ì›ì†Œê°€ 1ì¸ tensor\r\ntorch.zeros(*size) : ëª¨ë“  ì›ì†Œê°€ 0ì¸ tensor\r\ntorch.eye(n, m) : ëŒ€ê° ì›ì†Œê°€ 1ì´ê³  ë‚˜ë¨¸ì§€ê°€ 0ì¸ \\(n*m\\) tensorë¥¼ ìƒì„±. m = Noneì´ë©´ \\(n*n\\)tensorë¥¼ ìƒì„±\r\ntorch.rand(*size) : ëª¨ë“  ì›ì†Œë¥¼ ëœë¤í•œ ê°’ìœ¼ë¡œ ì±„ì›Œì§„ tensor.\r\ndtypeì„ intë¡œ ì§€ì •ì‹œ ì—ëŸ¬ê°€ ë°œìƒ\r\n\r\na = torch.ones([2, 3])\r\nb = torch.zeros([3, 2], dtype=torch.int64)\r\nc = torch.eye(4,3)\r\nd = torch.rand([2, 4, 3], dtype=torch.float)\r\n\r\nprint(a)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.]])\r\nprint(b)\r\ntensor([[0, 0],\r\n        [0, 0],\r\n        [0, 0]])\r\nprint(c)\r\ntensor([[1., 0., 0.],\r\n        [0., 1., 0.],\r\n        [0., 0., 1.],\r\n        [0., 0., 0.]])\r\nprint(d)\r\ntensor([[[0.9169, 0.3100, 0.5730],\r\n         [0.0963, 0.4855, 0.2943],\r\n         [0.8188, 0.8513, 0.0031],\r\n         [0.4675, 0.6421, 0.8515]],\r\n\r\n        [[0.2244, 0.8874, 0.0065],\r\n         [0.8683, 0.6122, 0.9006],\r\n         [0.5063, 0.8211, 0.0119],\r\n         [0.5276, 0.5330, 0.3839]]])\r\n\r\nì•ì„œ ì–¸ê¸‰í–ˆë“¯ì´ ndarrayë¡œë„ tensorë¥¼ ìƒì„±í•  ìˆ˜ ìˆë‹¤. as_tensor()ì™€\r\nfrom_numpy()ëŠ” ì›ë³¸ì˜ ê°’ì„ ì°¸ì¡°í•˜ê¸° ë•Œë¬¸ì— ì›ë³¸ì˜ ê°’ì„ ë°”ê¾¸ë©´ ê°™ì´\r\në³€í•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n\r\nimport numpy as np\r\n\r\nd = np.array([[2, 5, 4], [4, 2, 1], [5, 7, 4]])\r\n\r\na = torch.tensor(d)\r\nb = torch.as_tensor(d)\r\nc = torch.from_numpy(d)\r\n\r\nprint(a, id(a))\r\ntensor([[2, 5, 4],\r\n        [4, 2, 1],\r\n        [5, 7, 4]], dtype=torch.int32) 883421328\r\nprint(b, id(b))\r\ntensor([[2, 5, 4],\r\n        [4, 2, 1],\r\n        [5, 7, 4]], dtype=torch.int32) 883414736\r\nprint(c, id(c))\r\ntensor([[2, 5, 4],\r\n        [4, 2, 1],\r\n        [5, 7, 4]], dtype=torch.int32) 883177568\r\nprint(d, id(d))\r\n[[2 5 4]\r\n [4 2 1]\r\n [5 7 4]] 883130064\r\nd[0,0] = 0\r\n\r\nprint(a)\r\ntensor([[2, 5, 4],\r\n        [4, 2, 1],\r\n        [5, 7, 4]], dtype=torch.int32)\r\nprint(b)\r\ntensor([[0, 5, 4],\r\n        [4, 2, 1],\r\n        [5, 7, 4]], dtype=torch.int32)\r\nprint(c)\r\ntensor([[0, 5, 4],\r\n        [4, 2, 1],\r\n        [5, 7, 4]], dtype=torch.int32)\r\nprint(d)\r\n[[0 5 4]\r\n [4 2 1]\r\n [5 7 4]]\r\n\r\ntensor.shapeì„ í†µí•´ ë™ì¼í•œ sizeì˜ tensorë¥¼ ë§Œë“¤ ìˆ˜ë„ ìˆì§€ë§Œ\r\n_like(tensor) ë°©ë²•ìœ¼ë¡œë„ ë™ì¼í•œ sizeì˜ tensorë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n\r\na = torch.ones([2, 3])\r\nprint(a)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.]])\r\nb = torch.ones_like(a)\r\nc = torch.zeros_like(a, dtype=torch.float)\r\nd = torch.rand_like(a, dtype=torch.float)\r\n\r\nprint(b)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.]])\r\nprint(c)\r\ntensor([[0., 0., 0.],\r\n        [0., 0., 0.]])\r\nprint(d)\r\ntensor([[0.0043, 0.2561, 0.6409],\r\n        [0.9898, 0.8409, 0.6337]])\r\n\r\ndevice ì§€ì •\r\ngpuì—°ì‚°ì„ ìœ„í•´ì„œëŠ” tensorì˜ deviceë¥¼ cudaë¡œ ì§€ì •í•´ì¤˜ì•¼í•©ë‹ˆë‹¤.\r\ntorch.cuda.is_available()ë¥¼ í†µí•´ gpuê°€ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸\r\ní›„ tensor.to(â€˜cudaâ€™)ë¥¼ í†µí•´ deviceë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n\r\ntorch.cuda.is_available() # ì €ëŠ” gpuê°€ ì—†ì–´ìš” ã… ã…œ\r\nFalse\r\nif torch.cuda.is_available():\r\n    device = \"cuda\"\r\nelse:\r\n    device = \"cpu\"\r\n    \r\na.to(device)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.]])\r\na.device\r\ndevice(type='cpu')\r\n\r\ntensor ë‹¤ë£¨ê¸°\r\n\r\na = torch.ones([2, 3])\r\nb = torch.zeros([2, 3])\r\n\r\ntorch.cat()\r\n\r\nc = torch.cat([a, b], dim=0) # ì—´ë°©í–¥ìœ¼ë¡œ í•©ì¹˜ê¸°\r\nd = torch.cat([a, b], dim=1) # í–‰ë°©í–¥ìœ¼ë¡œ í•©ì¹˜ê¸°\r\nprint(c)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.],\r\n        [0., 0., 0.],\r\n        [0., 0., 0.]])\r\nprint(c.shape)\r\ntorch.Size([4, 3])\r\nprint(d)\r\ntensor([[1., 1., 1., 0., 0., 0.],\r\n        [1., 1., 1., 0., 0., 0.]])\r\nprint(d.shape)\r\ntorch.Size([2, 6])\r\n\r\ntorch.stack\r\nìƒˆë¡œìš´ ì°¨ì›(dim)ì— ë”°ë¼ tensorë“¤ ìŒ“ì•„ì¤ë‹ˆë‹¤. dim=0ì¼ë•Œì—ëŠ” tensor\r\nì „ì²´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìŒ“ê³ , dim=1ì¼ë•ŒëŠ” tensorì˜ ë‹¤ìŒ ì°¨ì›ì„ ê¸°ì¤€ìœ¼ë¡œ\r\nìŒ“ëŠ”ì‹ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ dimì€ 0ë¶€í„° tensorì˜ ì°¨ì›ì˜ ìˆ˜ë¥¼ ë„˜ì„ ìˆ˜ ì—†ê³ ,\r\nstack ì•ˆì˜ tensorë“¤ì€ ì„œë¡œ sizeê°€ ê°™ì•„ì•¼ í•©ë‹ˆë‹¤.\r\n\r\nc = torch.stack([a, b], dim=0)\r\nd = torch.stack([a, b], dim=1)\r\ne = torch.stack([a, b], dim=2)\r\nprint(c)\r\ntensor([[[1., 1., 1.],\r\n         [1., 1., 1.]],\r\n\r\n        [[0., 0., 0.],\r\n         [0., 0., 0.]]])\r\nprint(c.shape)\r\ntorch.Size([2, 2, 3])\r\nprint(d)\r\ntensor([[[1., 1., 1.],\r\n         [0., 0., 0.]],\r\n\r\n        [[1., 1., 1.],\r\n         [0., 0., 0.]]])\r\nprint(d.shape)\r\ntorch.Size([2, 2, 3])\r\nprint(e)\r\ntensor([[[1., 0.],\r\n         [1., 0.],\r\n         [1., 0.]],\r\n\r\n        [[1., 0.],\r\n         [1., 0.],\r\n         [1., 0.]]])\r\nprint(e.shape)\r\ntorch.Size([2, 3, 2])\r\n\r\nhstack : torch.cat([a, b], dim=0)\r\nvstack : torch.cat([a, b], dim=1)\r\n\r\nc = torch.hstack([a, b])\r\nprint(c)\r\ntensor([[1., 1., 1., 0., 0., 0.],\r\n        [1., 1., 1., 0., 0., 0.]])\r\nprint(c.shape)\r\ntorch.Size([2, 6])\r\nd = torch.vstack([a, b])\r\nprint(d)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.],\r\n        [0., 0., 0.],\r\n        [0., 0., 0.]])\r\nprint(d.shape)\r\ntorch.Size([4, 3])\r\n\r\ntorch.unsqueeze() : ë”ë¯¸ì°¨ì› ì‚­ì œ\r\ntorch.squeeze() : ë”ë¯¸ì°¨ì› ì¶”ê°€\r\n\r\n# unsqueeze and squeeze\r\nprint(a)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.]])\r\na = torch.unsqueeze(a, dim=1)\r\nprint(a)\r\ntensor([[[1., 1., 1.]],\r\n\r\n        [[1., 1., 1.]]])\r\nprint(a.shape)\r\ntorch.Size([2, 1, 3])\r\nprint(torch.squeeze(a))\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.]])\r\nprint(a.shape)\r\ntorch.Size([2, 1, 3])\r\n\r\ntonsorì˜ ì—°ì‚°\r\ní–‰ë ¬ ê³±\r\n\r\na = torch.tensor(np.array(list(range(12)))).reshape(3, 4)\r\nprint(a)\r\ntensor([[ 0,  1,  2,  3],\r\n        [ 4,  5,  6,  7],\r\n        [ 8,  9, 10, 11]], dtype=torch.int32)\r\nb = torch.tensor(np.array(list(range(8)))).reshape(4, 2)\r\nprint(b)\r\ntensor([[0, 1],\r\n        [2, 3],\r\n        [4, 5],\r\n        [6, 7]], dtype=torch.int32)\r\nc = a @ b\r\nprint(c)\r\ntensor([[ 28,  34],\r\n        [ 76,  98],\r\n        [124, 162]], dtype=torch.int32)\r\nd = torch.matmul(a, b)\r\nprint(d)\r\ntensor([[ 28,  34],\r\n        [ 76,  98],\r\n        [124, 162]], dtype=torch.int32)\r\n\r\nelement-wise product\r\n\r\na = torch.tensor(np.array(list(range(6)))).reshape(2, 3)\r\nprint(a)\r\ntensor([[0, 1, 2],\r\n        [3, 4, 5]], dtype=torch.int32)\r\nb = torch.tensor(list(range(10, 13))).reshape(1, 3)\r\nprint(b)\r\ntensor([[10, 11, 12]])\r\nc = a * b\r\nprint(c)\r\ntensor([[ 0, 11, 24],\r\n        [30, 44, 60]])\r\nd = a.mul(b)\r\nprint(d)\r\ntensor([[ 0, 11, 24],\r\n        [30, 44, 60]])\r\n\r\nitem() : tensorì˜ ì›ì†Œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. tensor ì•ˆì— ì›ì†Œê°€ 1ê°œë§Œ ìˆì–´ì•¼\r\ní•©ë‹ˆë‹¤.\r\n\r\n# item\r\nagg = d.sum()\r\nv = agg.item()\r\nprint(v, type(v))\r\n169 <class 'int'>\r\n\r\ninplace operations\r\n\r\nprint(a)\r\ntensor([[0, 1, 2],\r\n        [3, 4, 5]], dtype=torch.int32)\r\na.add_(5)\r\ntensor([[ 5,  6,  7],\r\n        [ 8,  9, 10]], dtype=torch.int32)\r\nprint(a)\r\ntensor([[ 5,  6,  7],\r\n        [ 8,  9, 10]], dtype=torch.int32)\r\na.add_(b)\r\ntensor([[15, 17, 19],\r\n        [18, 20, 22]], dtype=torch.int32)\r\nprint(a)\r\ntensor([[15, 17, 19],\r\n        [18, 20, 22]], dtype=torch.int32)\r\n\r\nDataset\r\ndatasetì€ ì „ì²´ ë°ì´í„°ë¥¼ sample ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•´ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.\r\n\r\nclass LionDataset(Dataset):\r\n    def __init__(self, data, target, transform=None, target_transform=None):\r\n        self.data = data\r\n        self.target = target\r\n        self.transform = transform\r\n        self.target_transform = target_transform\r\n        pass\r\n    \r\n    def __len__(self):\r\n        return len(self.data)\r\n    def __getitem__(self, idx):\r\n        x = self.data[idx]\r\n        y = self.target[idx]\r\n        \r\n        if self.transform:\r\n          x = self.transform(x)\r\n        if self.target_transform:\r\n          y = self.target_transform(y)\r\n        \r\n        return x, y\r\n\r\n\r\ndata = np.array(list(range(100))).reshape(-1, 2)\r\ntarget = np.array([[i] * 5 for i in range(10)]).reshape(-1)\r\n\r\nprint(data)\r\n[[ 0  1]\r\n [ 2  3]\r\n [ 4  5]\r\n [ 6  7]\r\n [ 8  9]\r\n [10 11]\r\n [12 13]\r\n [14 15]\r\n [16 17]\r\n [18 19]\r\n [20 21]\r\n [22 23]\r\n [24 25]\r\n [26 27]\r\n [28 29]\r\n [30 31]\r\n [32 33]\r\n [34 35]\r\n [36 37]\r\n [38 39]\r\n [40 41]\r\n [42 43]\r\n [44 45]\r\n [46 47]\r\n [48 49]\r\n [50 51]\r\n [52 53]\r\n [54 55]\r\n [56 57]\r\n [58 59]\r\n [60 61]\r\n [62 63]\r\n [64 65]\r\n [66 67]\r\n [68 69]\r\n [70 71]\r\n [72 73]\r\n [74 75]\r\n [76 77]\r\n [78 79]\r\n [80 81]\r\n [82 83]\r\n [84 85]\r\n [86 87]\r\n [88 89]\r\n [90 91]\r\n [92 93]\r\n [94 95]\r\n [96 97]\r\n [98 99]]\r\nprint(target)\r\n[0 0 0 0 0 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 4 4 4 4 4 5 5 5 5 5 6 6 6 6 6 7 7\r\n 7 7 7 8 8 8 8 8 9 9 9 9 9]\r\n\r\n\r\nlion = LionDataset(data=data, target=target)\r\nprint(lion[0:4])\r\n(array([[0, 1],\r\n       [2, 3],\r\n       [4, 5],\r\n       [6, 7]]), array([0, 0, 0, 0]))\r\nprint(len(lion))\r\n50\r\n\r\nDataLoader\r\ndatasetì„ batch ë‹¨ìœ„ë¡œ ë¬¶ì–´ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.\r\n\r\nloader = DataLoader(dataset=lion, batch_size=10, shuffle=True)\r\n\r\nfor i, batch in enumerate(loader):\r\n    x, y = batch\r\n    if i == 0:\r\n        print(x)\r\n        print(y)\r\n    print(x.shape)\r\ntensor([[86, 87],\r\n        [12, 13],\r\n        [72, 73],\r\n        [68, 69],\r\n        [60, 61],\r\n        [32, 33],\r\n        [80, 81],\r\n        [90, 91],\r\n        [46, 47],\r\n        [20, 21]], dtype=torch.int32)\r\ntensor([8, 1, 7, 6, 6, 3, 8, 9, 4, 2], dtype=torch.int32)\r\ntorch.Size([10, 2])\r\ntorch.Size([10, 2])\r\ntorch.Size([10, 2])\r\ntorch.Size([10, 2])\r\ntorch.Size([10, 2])\r\n\r\ntorch.nn.Module\r\npytorch ëª¨ë¸ì€ parametersë¥¼ ì¶”ì í•˜ë©° forward passë¥¼ ì§„í–‰í•œ ë’¤ back\r\npropagationì„ í†µí•´ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤. torch.nn.Moduleì€ ì—¬ëŸ¬ ì¸µì˜\r\nlayerë¡œ ì´ë¤„ì§„ ëª¨ë¸ì„ ì‰½ê²Œ ê´€ë¦¬í•  ìˆ˜ ìˆëŠ” classì…ë‹ˆë‹¤.\r\n\r\nclass LionLinear(nn.Module):\r\n    def __init__(self, input_dim, output_dim):\r\n        super().__init__()\r\n        self.input_dim = input_dim\r\n        self.output_dim = output_dim\r\n        \r\n        self.flatten = nn.Flatten() # 1ì°¨ì› tensorë¡œ í‰íƒ„í™”í•´ì¤€ë‹¤.\r\n        self.classifier = nn.Linear(input_dim, output_dim)\r\n        self.act = nn.ReLU()\r\n        \r\n    def forward(self, x):\r\n        x = self.flatten(x) \r\n        x = self.classifier(x)\r\n        x = self.act(x)\r\n        \r\n        return x\r\n\r\n\r\nlinear_model = LionLinear(28*28, 10).to(device)\r\nprint(linear_model)\r\nLionLinear(\r\n  (flatten): Flatten(start_dim=1, end_dim=-1)\r\n  (classifier): Linear(in_features=784, out_features=10, bias=True)\r\n  (act): ReLU()\r\n)\r\nprint(linear_model.classifier)\r\nLinear(in_features=784, out_features=10, bias=True)\r\n\r\nì´ì²˜ëŸ¼ nn.Moduleë¥¼ ìƒì†ë°›ì•„ forward()í†µí•´ ëª¨ë¸ì„ ì •ì˜í•  ìˆ˜\r\nìˆìŠµë‹ˆë‹¤.\r\në‹¤ìŒì€ MLPë¥¼ êµ¬í˜„í•˜ëŠ” moduleì…ë‹ˆë‹¤.\r\n\r\nclass LionLayer(nn.Module):\r\n    def __init__(self, input_dim, output_dim):\r\n        super().__init__()\r\n        self.input_dim = input_dim\r\n        self.output_dim = output_dim\r\n        self.layer = nn.Linear(self.input_dim, self.output_dim)\r\n        pass\r\n    def forward(self, x):\r\n        assert x.shape[-1] == self.input_dim, \"Input dimension mismatch\"\r\n        return self.layer(x)\r\n\r\n\r\nclass LionMLP(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim):\r\n        super().__init__()\r\n        self.input_dim = input_dim\r\n        self.hidden_dim = hidden_dim\r\n        self.output_dim = output_dim\r\n        self.flatten = nn.Flatten()\r\n        self.linear_1 = LionLayer(self.input_dim, self.hidden_dim)\r\n        self.linear_2 = LionLayer(self.hidden_dim, self.output_dim)\r\n        self.act_1 = nn.ReLU()\r\n        self.act_2 = nn.Softmax()\r\n        pass\r\n    \r\n    def forward(self, x):\r\n        x = self.flatten(x)\r\n        x = self.act_1(self.linear_1(x))\r\n        x = self.act_2(self.linear_2(x))\r\n        return x\r\n\r\n\r\nmlp = LionMLP(28*28, 50, 10)\r\nprint(mlp)\r\nLionMLP(\r\n  (flatten): Flatten(start_dim=1, end_dim=-1)\r\n  (linear_1): LionLayer(\r\n    (layer): Linear(in_features=784, out_features=50, bias=True)\r\n  )\r\n  (linear_2): LionLayer(\r\n    (layer): Linear(in_features=50, out_features=10, bias=True)\r\n  )\r\n  (act_1): ReLU()\r\n  (act_2): Softmax(dim=None)\r\n)\r\n\r\nnn.Sequential()ì„ í†µí•´ ë” ê°„ê²°í•˜ê²Œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n\r\nclass LionMLP(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim):\r\n        super().__init__()\r\n        self.input_dim = input_dim\r\n        self.hidden_dim = hidden_dim\r\n        self.output_dim = output_dim\r\n        self.flatten = nn.Flatten()\r\n        self.linear_1 = LionLayer(self.input_dim, self.hidden_dim)\r\n        self.linear_2 = LionLayer(self.hidden_dim, self.output_dim)\r\n        self.act_1 = nn.ReLU()\r\n        self.act_2 = nn.Softmax()\r\n        self.model = nn.Sequential(self.flatten, self.linear_1, self.act_1, self.linear_2, self.act_2)\r\n        pass\r\n    def forward(self, x):\r\n        return self.model(x)\r\n\r\n\r\nmlp = LionMLP(28 * 28, 40, 10)\r\nprint(mlp)\r\nLionMLP(\r\n  (flatten): Flatten(start_dim=1, end_dim=-1)\r\n  (linear_1): LionLayer(\r\n    (layer): Linear(in_features=784, out_features=40, bias=True)\r\n  )\r\n  (linear_2): LionLayer(\r\n    (layer): Linear(in_features=40, out_features=10, bias=True)\r\n  )\r\n  (act_1): ReLU()\r\n  (act_2): Softmax(dim=None)\r\n  (model): Sequential(\r\n    (0): Flatten(start_dim=1, end_dim=-1)\r\n    (1): LionLayer(\r\n      (layer): Linear(in_features=784, out_features=40, bias=True)\r\n    )\r\n    (2): ReLU()\r\n    (3): LionLayer(\r\n      (layer): Linear(in_features=40, out_features=10, bias=True)\r\n    )\r\n    (4): Softmax(dim=None)\r\n  )\r\n)\r\n\r\n\r\nfor name, param in mlp.named_parameters():\r\n    print(f\"Layer: {name} | Size: {param.size()} | Values: {param[:2]} \\n\")\r\nLayer: linear_1.layer.weight | Size: torch.Size([40, 784]) | Values: tensor([[-0.0320, -0.0161,  0.0018,  ...,  0.0278,  0.0135,  0.0355],\r\n        [-0.0096,  0.0356, -0.0273,  ..., -0.0196,  0.0305, -0.0003]],\r\n       grad_fn=<SliceBackward0>) \r\n\r\nLayer: linear_1.layer.bias | Size: torch.Size([40]) | Values: tensor([0.0305, 0.0006], grad_fn=<SliceBackward0>) \r\n\r\nLayer: linear_2.layer.weight | Size: torch.Size([10, 40]) | Values: tensor([[ 0.1549,  0.1293, -0.1401, -0.0738,  0.0004,  0.0726,  0.0289,  0.0976,\r\n         -0.0857, -0.1468, -0.0863, -0.0898,  0.0983, -0.0270, -0.0402,  0.0588,\r\n         -0.0031, -0.1369,  0.1563,  0.0316, -0.0642,  0.0575,  0.0104,  0.1135,\r\n          0.1001, -0.0564,  0.1311,  0.0601,  0.1249,  0.1569, -0.1053,  0.0030,\r\n         -0.0938,  0.1343, -0.1414, -0.0034,  0.1165, -0.1074, -0.0518, -0.1441],\r\n        [ 0.0381, -0.1287,  0.1479,  0.1138, -0.1021,  0.1123, -0.0741,  0.0752,\r\n         -0.1153, -0.0909,  0.1182, -0.1152, -0.1060,  0.1236,  0.0429, -0.1258,\r\n         -0.0227, -0.1014,  0.1515, -0.0474, -0.0308,  0.0486,  0.0399, -0.0094,\r\n          0.1450,  0.0633, -0.0504,  0.1421,  0.0489,  0.0882, -0.1149, -0.1266,\r\n         -0.1398,  0.0022,  0.0165,  0.0197, -0.0585, -0.0427, -0.1185, -0.0144]],\r\n       grad_fn=<SliceBackward0>) \r\n\r\nLayer: linear_2.layer.bias | Size: torch.Size([10]) | Values: tensor([-0.1406, -0.0314], grad_fn=<SliceBackward0>) \r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-21T11:49:59+09:00",
    "input_file": "pytorch-tutorial.knit.md"
  },
  {
    "path": "til/2022-05-19-cotepart1/",
    "title": "[ë°±ì¤€ ë¬¸ì œí’€ì´] ê¸°ë³¸ê¸°",
    "description": "ì½”ë”©í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ë°±ì¤€ ë¬¸ì œ í’€ì–´ë³´ê¸°",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-19",
    "categories": [
      "coding test"
    ],
    "contents": "\r\n\r\nContents\r\nì•½ìˆ˜ êµ¬í•˜ê¸°\r\nì´ì§„ìˆ˜\r\nbin() ì§ì ‘ êµ¬í˜„í•˜ê¸°(ì–‘ì˜\r\nì •ìˆ˜ì¼ë•Œë§Œ)\r\n\r\nìµœëŒ€, ìµœì†Œ\r\n\r\nì•½ìˆ˜ êµ¬í•˜ê¸°\r\në‘ ê°œì˜ ìì—°ìˆ˜ Nê³¼ Kê°€ ì£¼ì–´ì¡Œì„ ë•Œ, Nì˜ ì•½ìˆ˜ë“¤ ì¤‘ Kë²ˆì§¸ë¡œ ì‘ì€ ìˆ˜ë¥¼\r\nì¶œë ¥í•˜ëŠ” í”„ë¡œê·¸ë¨ì„ ì‘ì„±í•˜ì‹œì˜¤.\r\nì…ë ¥\r\nì²«ì§¸ ì¤„ì— Nê³¼ Kê°€ ë¹ˆì¹¸ì„ ì‚¬ì´ì— ë‘ê³  ì£¼ì–´ì§„ë‹¤. Nì€ 1 ì´ìƒ 10,000\r\nì´í•˜ì´ë‹¤. KëŠ” 1 ì´ìƒ N ì´í•˜ì´ë‹¤.\r\nì¶œë ¥\r\nì²«ì§¸ ì¤„ì— Nì˜ ì•½ìˆ˜ë“¤ ì¤‘ Kë²ˆì§¸ë¡œ ì‘ì€ ìˆ˜ë¥¼ ì¶œë ¥í•œë‹¤. ë§Œì¼ Nì˜ ì•½ìˆ˜ì˜\r\nê°œìˆ˜ê°€ Kê°œë³´ë‹¤ ì ì–´ì„œ Kë²ˆì§¸ ì•½ìˆ˜ê°€ ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš°ì—ëŠ” 0ì„\r\nì¶œë ¥í•˜ì‹œì˜¤.\r\n\r\nN, K = map(int, input().split())\r\n\r\nresult = 0\r\n\r\nfor i in range(1, N + 1):\r\n    if N % i == 0:\r\n        K -= 1\r\n        if K == 0:\r\n            result = i\r\n            break\r\n\r\nprint(result)\r\n\r\nì´ì§„ìˆ˜\r\nì–‘ì˜ ì •ìˆ˜ nì´ ì£¼ì–´ì¡Œì„ ë•Œ, ì´ë¥¼ ì´ì§„ìˆ˜ë¡œ ë‚˜íƒ€ëƒˆì„ ë•Œ 1ì˜ ìœ„ì¹˜ë¥¼ ëª¨ë‘\r\nì°¾ëŠ” í”„ë¡œê·¸ë¨ì„ ì‘ì„±í•˜ì‹œì˜¤. ìµœí•˜ìœ„ ë¹„íŠ¸(least significant bit, lsb)ì˜\r\nìœ„ì¹˜ëŠ” 0ì´ë‹¤.\r\nì…ë ¥\r\nì²«ì§¸ ì¤„ì— í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì˜ ê°œìˆ˜ Tê°€ ì£¼ì–´ì§„ë‹¤. ê° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ëŠ” í•œ\r\nì¤„ë¡œ ì´ë£¨ì–´ì ¸ ìˆê³ , nì´ ì£¼ì–´ì§„ë‹¤. \\((1 â‰¤ T â‰¤\r\n10, 1 â‰¤ n â‰¤ 10^6)\\)\r\nì¶œë ¥\r\nê° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì— ëŒ€í•´ì„œ, 1ì˜ ìœ„ì¹˜ë¥¼ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„í•´ì„œ ì¤„ í•˜ë‚˜ì—\r\nì¶œë ¥í•œë‹¤. ìœ„ì¹˜ê°€ ë‚®ì€ ê²ƒë¶€í„° ì¶œë ¥í•œë‹¤.\r\n\r\nt = int(input())\r\n\r\nfor _ in range(t):\r\n    n = int(input())\r\n    b = bin(n)[2:]\r\n    \r\n    for i in range(len(b)):\r\n        if b[::-1][i] == '1':\r\n            print(i, end=' ')\r\n\r\nbin() ì§ì ‘ êµ¬í˜„í•˜ê¸°(ì–‘ì˜\r\nì •ìˆ˜ì¼ë•Œë§Œ)\r\n\r\ndef my_bin(num):\r\n    a = num//2\r\n    b = num%2\r\n    if a == 0:\r\n        return '1'\r\n    else:\r\n        return my_bin(a)+str(b)\r\n\r\nìµœëŒ€, ìµœì†Œ\r\nNê°œì˜ ì •ìˆ˜ê°€ ì£¼ì–´ì§„ë‹¤. ì´ë•Œ, ìµœì†Ÿê°’ê³¼ ìµœëŒ“ê°’ì„ êµ¬í•˜ëŠ” í”„ë¡œê·¸ë¨ì„\r\nì‘ì„±í•˜ì‹œì˜¤.\r\nì…ë ¥\r\nì²«ì§¸ ì¤„ì— ì •ìˆ˜ì˜ ê°œìˆ˜ N (1 â‰¤ N â‰¤ 1,000,000)ì´ ì£¼ì–´ì§„ë‹¤. ë‘˜ì§¸ ì¤„ì—ëŠ”\r\nNê°œì˜ ì •ìˆ˜ë¥¼ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„í•´ì„œ ì£¼ì–´ì§„ë‹¤. ëª¨ë“  ì •ìˆ˜ëŠ” -1,000,000ë³´ë‹¤\r\ní¬ê±°ë‚˜ ê°™ê³ , 1,000,000ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ì€ ì •ìˆ˜ì´ë‹¤.\r\nì¶œë ¥\r\nì²«ì§¸ ì¤„ì— ì£¼ì–´ì§„ ì •ìˆ˜ Nê°œì˜ ìµœì†Ÿê°’ê³¼ ìµœëŒ“ê°’ì„ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„í•´\r\nì¶œë ¥í•œë‹¤.\r\n\r\ninput()\r\nl = list(map(int, input().split()))\r\n\r\nhigh = low = l[0]\r\n\r\nfor i in l[1:]:\r\n    if high < i:\r\n        high = i\r\n    if low > i:\r\n        low = i\r\n        \r\nprint(low, high)\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-20T19:04:09+09:00",
    "input_file": {}
  },
  {
    "path": "til/2022-05-18-dlfromscratchch2/",
    "title": "í¼ì…‰íŠ¸ë¡ ì´ë€",
    "description": "ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹ì„ ì½ê³  ë”¥ëŸ¬ë‹ì˜ ê¸°ì›ì´ ë˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì¸ í¼ì…‰íŠ¸ë¡ ì— ëŒ€í•´ ì •ë¦¬í•´ë´¤ìŠµë‹ˆë‹¤.",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-18",
    "categories": [
      "ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹"
    ],
    "contents": "\r\n\r\nContents\r\n1ï¸âƒ£ í¼ì…‰íŠ¸ë¡ ì´ë€\r\n2ï¸âƒ£ í¼ì…‰íŠ¸ë¡ ì˜ í•œê³„\r\n3ï¸âƒ£ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ \r\nâœ… ìš”ì•½\r\n\r\n1ï¸âƒ£ í¼ì…‰íŠ¸ë¡ ì´ë€\r\ní¼ì…‰íŠ¸ë¡ ì€ ë‹¤ìˆ˜ì˜ ì‹ í˜¸ë¥¼ ì…ë ¥(input)ìœ¼ë¡œ ë°›ì•„ í•˜ë‚˜ì˜ ì‹ í˜¸ë¥¼\r\nì¶œë ¥(output)í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.\r\n2ê°œì˜ ì…ë ¥ì„ ë°›ì•„ í•˜ë‚˜ì˜ ì‹ í˜¸ë¥¼ ì¶œë ¥í•˜ëŠ” í¼ì…‰íŠ¸ë¡ ì„ ìˆ˜ì‹ìœ¼ë¡œ\r\në‚˜íƒ€ë‚´ë©´,\r\n\\[y = \\begin{cases}0,(w_{1}x_{1} +\r\nw_{2}x_{2}\\leq\\theta)\\\\1,(w_{1}x_{1} +\r\nw_{2}x_{2}>\\theta)\\\\\\end{cases}\\] ë˜ëŠ” \\(\\theta\\)ë¥¼ \\(-b\\)ë¡œ ì¹˜í™˜í•˜ì—¬,\r\n\\[y = \\begin{cases}0,(b+w_{1}x_{1} +\r\nw_{2}x_{2}\\leq0)\\\\1,(b+w_{1}x_{1} +\r\nw_{2}x_{2}>0)\\\\\\end{cases}\\]\r\n\\(w\\) : weight, \\(b\\): bias\r\n2ï¸âƒ£ í¼ì…‰íŠ¸ë¡ ì˜ í•œê³„\r\ní¼ì…‰íŠ¸ë¡ ì´ í’€ ìˆ˜ ìˆëŠ” (ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ”) ë¬¸ì œëŠ” AND, NAND,\r\nORì…ë‹ˆë‹¤.\r\nAND\r\n\r\nx1x2y000100010111\r\n\r\nNAND\r\n\r\nx1x2y001101011110\r\n\r\nOR\r\n\r\nx1x2y000101011111\r\n\r\nìœ„ ì„¸ê°€ì§€ ë¬¸ì œëŠ” \\((w_1, w_2,\r\ntheta)\\)ì— ê°ê° \\((0.5, 0.5, 0.7),\r\n(-0.5, -0.5, -0.7), (1.5, 1.5, 0.7)\\) ë“±ì„ ëŒ€ì…í•˜ë©´ í’€ ìˆ˜\r\nìˆìŠµë‹ˆë‹¤.\r\nì´ë¥¼ íŒŒì´ì¬ í•¨ìˆ˜ë¡œ êµ¬í˜„í•˜ë©´,\r\n\r\ndef AND(x1, x2):\r\n    w1, w2, theta = 0.5, 0.5, 0.5\r\n    tmp = x1*w1+x2*w2\r\n    if tmp <= theta:\r\n        return 0\r\n    else:\r\n        return 1\r\n\r\ndef NAND(x1, x2):\r\n    w1, w2, theta = -0.5, -0.5, -0.7\r\n    tmp = x1*w1+x2*w2\r\n    if tmp <= theta:\r\n        return 0\r\n    else:\r\n        return 1\r\n    \r\ndef OR(x1, x2):\r\n    w1, w2, theta = 1.5, 1.5, 0.7\r\n    tmp = x1*w1+x2*w2\r\n    if tmp <= theta:\r\n        return 0\r\n    else:\r\n        return 1\r\n\r\nprint('x1=0, x2=0 : ', AND(0,0))\r\nx1=0, x2=0 :  0\r\nprint('x1=0, x2=1 : ', AND(0,1))\r\nx1=0, x2=1 :  0\r\nprint('x1=1, x2=0 : ', AND(1,0))\r\nx1=1, x2=0 :  0\r\nprint('x1=1, x2=1 : ', AND(1,1))\r\nx1=1, x2=1 :  1\r\nprint('x1=0, x2=0 : ', NAND(0,0))\r\nx1=0, x2=0 :  1\r\nprint('x1=0, x2=1 : ', NAND(0,1))\r\nx1=0, x2=1 :  1\r\nprint('x1=1, x2=0 : ', NAND(1,0))\r\nx1=1, x2=0 :  1\r\nprint('x1=1, x2=1 : ', NAND(1,1))\r\nx1=1, x2=1 :  0\r\nprint('x1=0, x2=0 : ', OR(0,0))\r\nx1=0, x2=0 :  0\r\nprint('x1=0, x2=1 : ', OR(0,1))\r\nx1=0, x2=1 :  1\r\nprint('x1=1, x2=0 : ', OR(1,0))\r\nx1=1, x2=0 :  1\r\nprint('x1=1, x2=1 : ', OR(1,1))\r\nx1=1, x2=1 :  1\r\n\r\ní•˜ì§€ë§Œ, í¼ì…‰íŠ¸ë¡ ìœ¼ë¡œ XORì€ í•´ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\r\nXOR\r\n\r\nx1x2y000101011110\r\n\r\nìœ„ ë¬¸ì œë¥¼ ê·¸ë¦¼ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ì´ ë¹¨ê°„ì ê³¼ ê²€ì€ì ì„ êµ¬ë¶„í•  ìˆ˜\r\nìˆëŠ” ì„ ì„ ê·¸ì„ ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\r\n\r\n\r\n\r\nê³¡ì„ ìœ¼ë¡œ ì„ ì„ ê·¸ë¦°ë‹¤ë©´ êµ¬ë¶„í•  ìˆ˜ ìˆê² ì§€ë§Œ í¼ì…‰íŠ¸ë¡ ìœ¼ë¡œëŠ” ì§ì„ ë°–ì—\r\nê·¸ë¦´ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ í•´ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¦‰, í¼ì…‰íŠ¸ë¡ ì€ ë¹„ì„ í˜•ì˜ ì˜ì—­ì„\r\ní‘œí˜„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\r\n3ï¸âƒ£ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ \r\nì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‚˜ì˜¨ ê²ƒì´ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ ì…ë‹ˆë‹¤. ë§ ê·¸ëŒ€ë¡œ í¼ì…‰íŠ¸ë¡ \r\nì¸µì„ ì—¬ëŸ¬ ê°œ ê°–ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë©° ì•ì—ì„œ ì–¸ê¸‰í–ˆë˜ í¼ì…‰íŠ¸ë¡ ì€ ì •í™•íˆ ë§í•˜ë©´\r\në‹¨ì¸µ í¼ì…‰íŠ¸ë¡ ì…ë‹ˆë‹¤.\r\nXORë¬¸ì œë¥¼ í•´ê²°í•˜ë ¤ë©´ ìœ„ì—ì„œ AND, NAND, OR ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´\r\në§Œë“¤ì—ˆë˜ í¼ì…‰íŠ¸ë¡ ì„ ì ì ˆíˆ ìŒ“ì•„ ë§Œë“¤ë©´ ë©ë‹ˆë‹¤.\r\n\r\ndef XOR(x1, x2):\r\n    s1 = NAND(x1, x2)\r\n    s2 = OR(x1, x2)\r\n    y = AND(s1, s2)\r\n    return y\r\n\r\nprint('x1=0, x2=0 : ', XOR(0,0))\r\nx1=0, x2=0 :  0\r\nprint('x1=0, x2=1 : ', XOR(0,1))\r\nx1=0, x2=1 :  1\r\nprint('x1=1, x2=0 : ', XOR(1,0))\r\nx1=1, x2=0 :  1\r\nprint('x1=1, x2=1 : ', XOR(1,1))\r\nx1=1, x2=1 :  0\r\n\r\nì²«ë²ˆì§¸ ì¸µì— NAND, ORë¥¼ í’€ê¸° ìœ„í•´ ë§Œë“¤ì—ˆë˜ í¼ì…‰íŠ¸ë¡ ì„ ìŒ“ê³  ë‘ë²ˆì§¸ ì¸µì—\r\nANDë¥¼ í’€ê¸° ìœ„í•´ ë§Œë“¤ì—ˆë˜ í¼ì…‰íŠ¸ë¡ ì„ ìŒ“ì•„ XORë¬¸ì œë¥¼ í•´ê²°í–ˆìŠµë‹ˆë‹¤.\r\nì´ì²˜ëŸ¼ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ ì€ ë¹„ì„ í˜•ì˜ ì˜ì—­ê¹Œì§€ë„ í‘œí˜„í•  ìˆ˜ ìˆìŒì„ ì•Œ ìˆ˜\r\nìˆìŠµë‹ˆë‹¤.\r\nâœ… ìš”ì•½\r\ní¼ì…‰íŠ¸ë¡ ì€ ì…ì¶œë ¥ì„ ê°–ì¶˜ ì•Œê³ ë¦¬ì¦˜\r\në‹¨ì¸µ í¼ì…‰íŠ¸ë¡ ì€ ì„ í˜• ì˜ì—­ë§Œ í‘œí˜„í•  ìˆ˜ ìˆê³ , ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ ì€\r\në¹„ì„ í˜• ì˜ì—­ë„ í‘œí˜„ ê°€ëŠ¥\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-19T00:59:30+09:00",
    "input_file": {}
  },
  {
    "path": "til/2022-05-13-first-til/",
    "title": "TIL(Today I Learn) í˜ì´ì§€ ê°œì„¤!!",
    "description": "ë§¤ì¼ ê³µë¶€í•˜ëŠ” ê²ƒì„ ê¸°ë¡í•˜ê¸°",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-13",
    "categories": [],
    "contents": "\r\në§¤ì¼ë§¤ì¼ ê³µë¶€í•œ ê²ƒì„ ê¸°ë¡í•˜ëŠ” í˜ì´ì§€ë¥¼ ë§Œë“¤ì—ˆë‹¤. ì¼ê¸°ì²˜ëŸ¼ ë‚˜ì¤‘ì— ë´ë„\r\në¿Œë“¯í•œ í˜ì´ì§€ê°€ ë˜ê¸¸..\r\nğŸ˜„í™”ì´íŒ…!!\r\n\r\n\r\n\r\n",
    "preview": "til/2022-05-13-first-til/images/owl.png",
    "last_modified": "2022-05-16T00:44:05+09:00",
    "input_file": {},
    "preview_width": 640,
    "preview_height": 514
  }
]

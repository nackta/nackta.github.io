[
  {
    "path": "til/2022-05-23-dlfromscratchch3/",
    "title": "ì‹ ê²½ë§ê³¼ í™œì„±í™” í•¨ìˆ˜",
    "description": "ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹ì„ ì½ê³  ì‹ ê²½ë§ê³¼ í™œì„±í™” í•¨ìˆ˜ì— ëŒ€í•´ ì •ë¦¬í•´ë´¤ìŠµë‹ˆë‹¤.",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-23",
    "categories": [
      "ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹"
    ],
    "contents": "\r\n\r\nContents\r\n1ï¸âƒ£ì‹ ê²½ë§ì´ë€\r\n2ï¸âƒ£í™œì„±í™” í•¨ìˆ˜ì˜ ì¢…ë¥˜\r\nê³„ë‹¨í•¨ìˆ˜\r\nì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜\r\nReLU í•¨ìˆ˜\r\ní•­ë“± í•¨ìˆ˜\r\nì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜\r\n\r\n3ï¸âƒ£ë¹„ì„ í˜• í•¨ìˆ˜\r\n4ï¸âƒ£numpyë¡œ ì‹ ê²½ë§ êµ¬í˜„\r\n5ï¸âƒ£ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ì˜ íŠ¹ì§•\r\n6ï¸âƒ£ë¶„ë¥˜ ì‹ ê²½ë§ êµ¬í˜„\r\n7ï¸âƒ£ë°°ì¹˜ ì²˜ë¦¬\r\n\r\n1ï¸âƒ£ì‹ ê²½ë§ì´ë€\r\në‹¨ì¸µ í¼ì…‰íŠ¸ë¡ ì€ ë‹¤ìŒê³¼ ê°™ì€ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„ë©ë‹ˆë‹¤.\r\n\\[y = \\begin{cases}0,(b+w_{1}x_{1} +\r\nw_{2}x_{2}\\leq0)\\\\1,(b+w_{1}x_{1} +\r\nw_{2}x_{2}>0)\\\\\\end{cases}\\]\r\n\\(z = b+w_{1}x_{1} + w_{2}x_{2}\\)ë¡œ\r\në‚˜íƒ€ë‚¸ë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n\\[y =\r\n\\begin{cases}0,z\\leq0)\\\\1,z>0\\\\\\end{cases}\\]\r\nì¦‰, ì…ë ¥ì‹ í˜¸ì˜ ê°€ì¤‘í•©ì´ 0ì„ ë„˜ìœ¼ë©´ 1ì„ ë°˜í™˜í•˜ê³  ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ 0ì„\r\në°˜í™˜í•©ë‹ˆë‹¤. ì´ì²˜ëŸ¼ ì…ë ¥ì‹ í˜¸ë“¤ì˜ ê°’ì„ ì¶œë ¥ì‹ í˜¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ í™œì„±í™”\r\ní•¨ìˆ˜ë¼ê³  í•˜ë©° ë‹¨ì¸µ í¼ì…‰íŠ¸ë¡ ì—ì„œ ì‚¬ìš©í•œ í•¨ìˆ˜ë¥¼ ê³„ë‹¨ í•¨ìˆ˜ë¼ê³  í•©ë‹ˆë‹¤.\r\në‹¨ì¸µ í¼ì…‰íŠ¸ë¡ ì€ í™œì„±í™” í•¨ìˆ˜ë¡œ ê³„ë‹¨ í•¨ìˆ˜ ì‚¬ìš©í•˜ì§€ë§Œ ì‹ ê²½ë§ì€ ë‹¤ë¥¸\r\ní™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆê³  ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ ì²˜ëŸ¼ ì—¬ëŸ¬ ì¸µì„ ìŒ“ì•„ ë§Œë“ \r\nì•Œê³ ë¦¬ì¦˜ì´ë¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\r\nì‹ ê²½ë§ì˜ ì˜ˆìœ„ ê·¸ë¦¼ì²˜ëŸ¼ ì‹ ê²½ë§ì€ í¬ê²Œ ì¶œë ¥ì¸µ(input layer), ì€ë‹‰ì¸µ(hidden layer),\r\nì¶œë ¥ì¸µ(output layer)ë¡œ ì´ë¤„ì ¸ ìˆìœ¼ë©° ê° ì›ì„ ë…¸ë“œ(node)ë¼ê³ \r\në¶€ë¦…ë‹ˆë‹¤.\r\n2ï¸âƒ£í™œì„±í™” í•¨ìˆ˜ì˜ ì¢…ë¥˜\r\nê³„ë‹¨í•¨ìˆ˜\r\ní¼ì…‰íŠ¸ë¡ ì—ì„œ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜. ì¶œë ¥ëŠ” 0 ë˜ëŠ” 1ì´ë‹¤.\r\n\\[h(z) =\r\n\\begin{cases}0,z\\leq0)\\\\1,z>0\\\\\\end{cases}\\]\r\nì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜\r\nì¶œë ¥ê°’ì˜ ë²”ìœ„ê°€ 0ê³¼ 1ì‚¬ì´ì— ì¡´ì¬í•œë‹¤.\r\n\\[h(z) = 1/(1+exp(-z))\\]\r\nReLU í•¨ìˆ˜\r\n\\[h(z) =\r\n\\begin{cases}0,z\\leq0)\\\\z,z>0\\\\\\end{cases}\\]\r\ní•­ë“± í•¨ìˆ˜\r\n\\[ h(z) = z\\]\r\nì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜\r\n\\(h(z_k)\\)ëŠ” kë²ˆì§¸ ì¶œë ¥ê°’ì„ ì˜ë¯¸\r\n\\[h(z_k) = exp(z_k)/\\sum\r\nexp(z_i)\\]\r\n3ï¸âƒ£ë¹„ì„ í˜• í•¨ìˆ˜\r\nì‹ ê²½ë§ì—ì„œ ì¸µì„ ìŒ“ì„ ë•Œ í™œì„±í™” í•¨ìˆ˜ëŠ” ë¹„ì„ í˜• í•¨ìˆ˜ì´ì–´ì•¼ í•©ë‹ˆë‹¤. ë§Œì•½\r\ní™œì„±í™” í•¨ìˆ˜ë¡œ ë‹¤ìŒê³¼ ê°™ì€ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤ë©´,\r\n\\[ h(z) = cz\\]\r\nì¸µì„ ì•„ë¬´ë¦¬ ìŒ“ì•„ë„ ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœë¡œ ë‹¨ìˆœí™” ì‹œí‚¬ ìˆ˜ ìˆê³  ê²°êµ­ ì¸µì„\r\nì•ˆìŒ“ì€ ê²ƒê³¼ ë™ì¼í•œ íš¨ê³¼ë¥¼ ê°–ê²Œ ë©ë‹ˆë‹¤.\r\n\\[ h(h(h(z))) = c^3z\\]\r\nê·¸ëŸ¬ë¯€ë¡œ ì¸µì„ ìŒ“ëŠ” í˜œíƒì„ ì–»ê¸° ìœ„í•´ì„œëŠ” í™œì„±í™” í•¨ìˆ˜ë¡œ ë¹„ì„ í˜• í•¨ìˆ˜ë¥¼\r\nì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.\r\n4ï¸âƒ£numpyë¡œ ì‹ ê²½ë§ êµ¬í˜„\r\nì´ ê¸€ ì²˜ìŒì— ë“±ì¥í•œ ê·¸ë¦¼ì€ 3ê°œì˜ ì…ë ¥ì„ ë°›ì•„ 4ê°œì˜ ë…¸ë“œê°€ ì¡´ì¬í•˜ëŠ”\r\nì€ë‹‰ì¸µ í•˜ë‚˜ë¥¼ ê±°ì³ 2ê°œë¥¼ ì¶œë ¥í•˜ëŠ” ì‹ ê²½ë§ì„ ë„ì‹í™”í•œ ê²ƒì…ë‹ˆë‹¤. numpyë¥¼\r\ní™œìš©í•´ ì´ë¥¼ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤.\r\nê° ë…¸ë“œì˜ ê°€ì¤‘ì¹˜ëŠ” ì„ì˜ë¡œ ë„£ê³  ì€ë‹‰ì¸µì˜ í™œì„±í™” í•¨ìˆ˜ëŠ” ì‹œê·¸ëª¨ì´ë“œ,\r\nì¶œë ¥ì¸µì˜ í™œì„±í™” í•¨ìˆ˜ëŠ” í•­ë“±í•¨ìˆ˜ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.\r\n\r\nimport numpy as np\r\n\r\ndef init_network():\r\n    network = {}\r\n    network[\"W1\"] = np.array([[0.1,0.3,0.1,0.5],[0.2,0.4,0.3,0.4],[0.1,0.2,0.3,0.5]]) # ì€ë‹‰ì¸µ\r\n    network[\"W2\"] = np.array([[0.2,0.4],[0.1,0.2],[0.1,0.3],[0.5,0.5]]) # ì¶œë ¥ì¸µ\r\n    \r\n    return network\r\n\r\ndef sigmoid(x):\r\n    return 1/(1+np.exp(-x))\r\n\r\ndef identity_function(x):\r\n    return x\r\n\r\ndef forward(network, x):\r\n    W1, W2 = network[\"W1\"], network[\"W2\"]\r\n    x = np.dot(x,W1)\r\n    x = sigmoid(x)\r\n    x = np.dot(x,W2)\r\n    x = identity_function(x)\r\n    \r\n    return x\r\n\r\n\r\nnetwork = init_network()\r\nforward(network, np.array([0.1, 0.5, 0.7]))\r\narray([0.55009573, 0.8365092 ])\r\n\r\n5ï¸âƒ£ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ì˜ íŠ¹ì§•\r\nìœ„ ì‹ ê²½ë§ì—ì„œ ì¶œë ¥ì¸µì€ í•­ë“±í•¨ìˆ˜ë¡œ êµ¬í˜„ëœ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\nì¼ë°˜ì ìœ¼ë¡œ íšŒê·€ì—ëŠ” í•­ë“±í•¨ìˆ˜ë¥¼, ë¶„ë¥˜ì—ëŠ” ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\r\nê·¸ ì´ìœ ëŠ” ì†Œí”„íŠ¸ë§¥ìŠ¤ì˜ ì¶œë ¥ê°’ë“¤ì˜ í•©ì€ 1ì´ì–´ì„œ ê° ì¶œë ¥ê°’ë“¤ì„ ë¶„ë¥˜í™•ë¥ ë¡œ\r\ní•´ì„í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì†Œí”„íŠ¸ë§¥ìŠ¤ì˜ ì¶œë ¥ê°’ìœ¼ë¡œ costë¥¼\r\nê³„ì‚°í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. í•˜ì§€ë§Œ í•™ìŠµëœ ëª¨ë¸ì„ ì ìš©í•  ë•ŒëŠ” ì¶œë ¥ì¸µì—\r\nì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì‚¬ìš©í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ëŠ” ë‹¨ì¡°ì¦ê°€ í•¨ìˆ˜ì´ê³ \r\ncostë¥¼ ê³„ì‚°í•  í•„ìš”ê°€ ì—†ê¸° ë•Œë¬¸ì— ê°€ì¥ í° ê°’ì„ ê·¸ëŒ€ë¡œ ë¶„ë¥˜í•˜ë©´ ë©ë‹ˆë‹¤.\r\nì§€ìˆ˜í•¨ìˆ˜ ê³„ì‚°ì— ë“œëŠ” ë¹„ìš©ë„ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\nì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì½”ë“œë¡œ êµ¬í˜„ ì‹œ ì˜¤ë²„í”Œë¡œ ë¬¸ì œê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ì´ ë¬¸ì œë¥¼\r\ní•´ê²°í•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ ìˆ˜ì‹ìœ¼ë¡œ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ ê°œì„ í•  ìˆ˜\r\nìˆìŠµë‹ˆë‹¤.\r\n\\[h(z_k) = exp(z_k)/\\sum exp(z_i) \\\\ =\r\nCexp(z_k)/C\\sum exp(z_i) \\\\ = exp(z_k+log(C))/\\sum exp(z_i+log(C)), \\\\ C\r\n= -max(z_i)\\] ì¦‰, ì†Œí”„íŠ¸ë§¥ìŠ¤ì˜ ì…ë ¥ê°’ì— ìƒìˆ˜ë¥¼ ë”í•´ë„ ê·¸ ê°’ì˜\r\nì°¨ì´ê°€ ì—†ìœ¼ë¯€ë¡œ ì…ë ¥ê°’ë“¤ ì¤‘ ìµœëŒ€ê°’ì„ ê° ì…ë ¥ê°’ì— ë¹¼ì£¼ë©´ ì˜¤ë²„í”Œë¡œ ë¬¸ì œë¥¼\r\ní•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n6ï¸âƒ£ë¶„ë¥˜ ì‹ ê²½ë§ êµ¬í˜„\r\niris ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ Speciesë¥¼ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ì–´\r\në³´ê² ìŠµë‹ˆë‹¤.\r\ndata load\r\n\r\n\r\nlibrary(reticulate)\r\npy$iris <- iris\r\n\r\n\r\n\r\n\r\nimport pandas as pd\r\niris = pd.DataFrame(iris)\r\n\r\n\r\niris\r\n     Sepal.Length  Sepal.Width  Petal.Length  Petal.Width    Species\r\n0             5.1          3.5           1.4          0.2     setosa\r\n1             4.9          3.0           1.4          0.2     setosa\r\n2             4.7          3.2           1.3          0.2     setosa\r\n3             4.6          3.1           1.5          0.2     setosa\r\n4             5.0          3.6           1.4          0.2     setosa\r\n..            ...          ...           ...          ...        ...\r\n145           6.7          3.0           5.2          2.3  virginica\r\n146           6.3          2.5           5.0          1.9  virginica\r\n147           6.5          3.0           5.2          2.0  virginica\r\n148           6.2          3.4           5.4          2.3  virginica\r\n149           5.9          3.0           5.1          1.8  virginica\r\n\r\n[150 rows x 5 columns]\r\n\r\none-hot_encoding\r\n\r\nx_data = iris.drop([\"Species\"], axis=1)\r\nx_data = x_data.values\r\ny_data = pd.get_dummies(iris['Species'])\r\ny_data\r\n     setosa  versicolor  virginica\r\n0         1           0          0\r\n1         1           0          0\r\n2         1           0          0\r\n3         1           0          0\r\n4         1           0          0\r\n..      ...         ...        ...\r\n145       0           0          1\r\n146       0           0          1\r\n147       0           0          1\r\n148       0           0          1\r\n149       0           0          1\r\n\r\n[150 rows x 3 columns]\r\ny_data = y_data.values\r\n\r\nmodeling\r\nì„ì˜ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ì—¬ ëª¨ë¸ì„ ë§Œë“­ë‹ˆë‹¤. ì´ì „ ëª¨ë¸ê³¼ í° ì°¨ì´ëŠ”\r\nì—†ì§€ë§Œ ì…ë ¥ ë…¸ë“œì™€ ì¶œë ¥ ë…¸ë“œê°€ í•˜ë‚˜ì”© ë” ë“¤ì–´ê°€ìˆê³  ì¶œë ¥ì¸µì˜ í™œì„±í™”\r\ní•¨ìˆ˜ê°€ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¡œ ë°”ë€Œì—ˆìŠµë‹ˆë‹¤.\r\n\r\ndef init_network():\r\n    network = {}\r\n    network[\"W1\"] = np.array([[0.1,0.3,0.1,0.5],[0.2,0.4,0.3,0.4],[0.5,0.2,0.3,0.5],[0.7,0.5,0.2,0.5]]) # ì€ë‹‰ì¸µ\r\n    network[\"W2\"] = np.array([[0.5,0.9,0.1],[0.1,0.2,0.7],[0.2,0.5,0.5],[0.5,0.5,0.5]]) # ì¶œë ¥ì¸µ\r\n    \r\n    return network\r\n\r\ndef sigmoid(x):\r\n    return 1/(1+np.exp(-x))\r\n\r\ndef softmax(x):\r\n    c = np.max(x)\r\n    exp_x = np.exp(x-c)\r\n    sum_exp_x = np.sum(exp_x)\r\n    \r\n    return exp_x/sum_exp_x\r\n\r\ndef forward(network, x):\r\n    W1, W2 = network[\"W1\"], network[\"W2\"]\r\n    x = np.dot(x,W1)\r\n    x = sigmoid(x)\r\n    x = np.dot(x,W2)\r\n    x = softmax(x)\r\n    \r\n    return x\r\n\r\nì²«ë²ˆì§¸ ì…ë ¥ê°’ë§Œ ë„£ì–´ ì¶œë ¥ì„ í™•ì¸í•´ë³´ë©´ 2ë²ˆì§¸ ê°’ì˜ ë¶„ë¥˜í™•ë¥ ì´ ê°€ì¥\r\në†’ì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n\r\nnetwork = init_network()\r\nforward(network, x_data[0])\r\narray([0.21381633, 0.43739548, 0.34878819])\r\n\r\nAccuacyë¥¼ í™•ì¸í•´ë³´ë©´ 33%ì¸ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ì§ í•™ìŠµê³¼ì •ì„\r\ní†µí•´ ê°€ì¤‘ì¹˜ë¥¼ ìµœì í™” í•˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\r\n\r\nnetwork = init_network()\r\n\r\ncount = 0\r\nfor i in range(len(x_data)):\r\n    pred_p = forward(network, x_data[i])\r\n    index_max = np.argmax(pred_p)\r\n    \r\n    if index_max == np.argmax(y_data[i]):\r\n        count += 1\r\n\r\nprint(\"Accuacy: \", count/len(x_data))\r\nAccuacy:  0.3333333333333333\r\n\r\n7ï¸âƒ£ë°°ì¹˜ ì²˜ë¦¬\r\nì•„ê¹Œ êµ¬í˜„í–ˆë˜ ëª¨ë¸ì—ì„œ ì…ë ¥ê°’ì˜ shape, ê° ê°€ì¤‘ì¹˜ì˜ shape, ì¶œë ¥ê°’ì˜\r\nshapeì„ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.\r\n\r\ndef forward(network, x):\r\n    W1, W2 = network[\"W1\"], network[\"W2\"]\r\n    print(\"input : \", x.shape)\r\n    print(\"W1 : \", W1.shape)\r\n    print(\"W2 : \", W2.shape)\r\n    x = np.dot(x,W1)\r\n    x = sigmoid(x)\r\n    x = np.dot(x,W2)\r\n    x = softmax(x)\r\n    print(\"output : \", x.shape)\r\n    return x\r\n\r\nforward(network, x_data[0])\r\ninput :  (4,)\r\nW1 :  (4, 4)\r\nW2 :  (4, 3)\r\noutput :  (3,)\r\narray([0.21381633, 0.43739548, 0.34878819])\r\n\r\ní–‰ë ¬ì˜ ê³± ì›ë¦¬ì— ë”°ë¼ (1x4)(4x4)(4x3)=(1x3)ì˜ arrayê°€ ì¶œë ¥ë˜ëŠ” ê²ƒì„\r\nì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ë§Œì•½ inputì„ (3x4)ë¡œ ì£¼ê²Œ ëœë‹¤ë©´ outputì€\r\n(3x3)arrayë¡œ ì¶œë ¥ë  ê²ƒì…ë‹ˆë‹¤.\r\nì´ì²˜ëŸ¼ ì…ë ¥ë°ì´í„°ë¥¼ ë¬¶ì–´ì„œ ê³„ì‚°í•  ìˆ˜ ìˆìœ¼ë©° ì…ë ¥ ë°ì´í„° ë¬¶ìŒì„\r\në°°ì¹˜ë¼ê³  í•˜ê³  ë°°ì¹˜ ì•ˆì— ë°ì´í„°ì˜ ìˆ˜ë¥¼ ë°°ì¹˜ í¬ê¸°(batch size)ë¼ê³  í•©ë‹ˆë‹¤.\r\nê·¸ëŸ¼ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤.\r\nì´ì „ ì½”ë“œì™€ ë‹¤ë¥¸ ì ì€ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ê°€ ë°°ì¹˜ì²˜ë¦¬ë¥¼ í•  ìˆ˜ ìˆë„ë¡\r\nìˆ˜ì •í–ˆê³  ì˜ˆì¸¡ì„ ìˆ˜í–‰í•  ë•Œë„ ë°°ì¹˜ ë³„ë¡œ ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡ í–ˆìŠµë‹ˆë‹¤.\r\n\r\ndef init_network():\r\n    network = {}\r\n    network[\"W1\"] = np.array([[0.1,0.3,0.1,0.5],[0.2,0.4,0.3,0.4],[0.5,0.2,0.3,0.5],[0.7,0.5,0.2,0.5]]) # ì€ë‹‰ì¸µ\r\n    network[\"W2\"] = np.array([[0.5,0.9,0.1],[0.1,0.2,0.7],[0.2,0.5,0.5],[0.5,0.5,0.5]]) # ì¶œë ¥ì¸µ\r\n    \r\n    return network\r\n\r\ndef sigmoid(x):\r\n    return 1/(1+np.exp(-x))\r\n\r\ndef softmax(x):\r\n    c = np.max(x)\r\n    exp_x = np.exp(x-c)\r\n    \r\n    return np.multiply(exp_x.T, 1/np.sum(exp_x, axis=1)).T\r\n\r\ndef forward(network, x):\r\n    W1, W2 = network[\"W1\"], network[\"W2\"]\r\n    x = np.dot(x,W1)\r\n    x = sigmoid(x)\r\n    x = np.dot(x,W2)\r\n    x = softmax(x)\r\n    \r\n    return x\r\n\r\nnetwork = init_network()\r\nbatch_size = 10\r\n\r\ncount = 0\r\nfor i in range(0, len(x_data), batch_size):\r\n    x_batch = x_data[i:i+batch_size]\r\n    pred_p = forward(network, x_batch)\r\n    count += (np.argmax(pred_p, axis=1) == np.argmax(y_data[i:i+batch_size], axis=1)).sum()\r\n    \r\nprint(\"Accuacy: \", count/len(x_data))\r\nAccuacy:  0.3333333333333333\r\n\r\në°°ì¹˜ ì²˜ë¦¬ëŠ” ì»´í“¨í„°ë¡œ ê³„ì‚°í•  ë•Œ í° ì´ì ì„ ì¤ë‹ˆë‹¤. ë°°ì¹˜ì²˜ë¦¬ë¥¼ í•˜ì§€\r\nì•Šì•˜ì„ ë•ŒëŠ” forë¬¸ ë£¨í”„ê°€ ë°ì´í„° ìˆ˜ë§Œí¼ ëŒì•˜ì§€ë§Œ ë°°ì¹˜í¬ê¸°ë¥¼ 10ê°œë¡œ í–ˆì„\r\në•Œì—ëŠ” 10ë¶„ì˜ 1ë§Œí¼ ê°ì†Œí–ˆìŠµë‹ˆë‹¤. ì¦‰, ë” í° ë°°ì—´ë¡œ í•œë²ˆì— ê³„ì‚°í•˜ëŠ” ê²ƒì´\r\nì»´í“¨í„°ëŠ” ë” ë¹ ë¦…ë‹ˆë‹¤.\r\në‹¤ìŒì¥ì€ ì‹ ê²½ë§ í•™ìŠµê³¼ì •ì„ í†µí•´ ê°€ì¤‘ì¹˜ë¥¼ ìµœì í™”í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ìœ„\r\nëª¨ë¸ë„ í•™ìŠµê³¼ì •ì„ í†µí•´ Accuacyë¥¼ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤!\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-26T00:42:26+09:00",
    "input_file": {}
  },
  {
    "path": "til/2022-05-20-pytorch-tutorial/",
    "title": "Pytorch tutorial",
    "description": "ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ íŒŒì´í† ì¹˜ì— ëŒ€í•´ ì•Œì•„ë³´ì",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-20",
    "categories": [
      "pytorch"
    ],
    "contents": "\r\n\r\nContents\r\ntorch tensor\r\ntensorì˜ parameter\r\ntensorì˜ ìƒì„±\r\ndevice ì§€ì •\r\ntensor ë‹¤ë£¨ê¸°\r\ntonsorì˜ ì—°ì‚°\r\n\r\nDataset\r\nDataLoader\r\ntorch.nn.Module\r\nreference\r\n\r\nsocar ë¶€íŠ¸ìº í”„ì˜ ê°•ì˜ ì¤‘ ì¼ë¶€ë¥¼ ì •ë¦¬í•œ ë‚´ìš©ì…ë‹ˆë‹¤.\r\n\r\nimport numpy as np\r\n\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.utils.data import Dataset, DataLoader\r\n\r\nfrom torchvision import datasets, transforms\r\n\r\nimport matplotlib.pyplot as plt\r\n\r\ntorch tensor\r\ntensorëŠ” numpyì˜ ndarrayì²˜ëŸ¼ ë‹¤ì°¨ì› ë°ì´í„° ë°°ì—´ì…ë‹ˆë‹¤. tensorë¥¼\r\nìƒì„±í•  ë•ŒëŠ” listë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆì§€ë§Œ ndarrayë¥¼ ì‚¬ìš©í•  ìˆ˜ë„\r\nìˆìŠµë‹ˆë‹¤.\r\nndarrayì™€ tensorì˜ ì°¨ì´ì ì€ tensorì—ëŠ” back propagation(ì—­ì „íŒŒ)ë¥¼\r\në‹¤ë£¨ê¸° ìœ„í•´ forward pass(ìˆœì „íŒŒ)ì—ì„œ ì „ë‹¬ëœ ê°’ê³¼ ì—°ì‚°ì˜ ì¢…ë¥˜ë¥¼ ê¸°ì–µí•  ìˆ˜\r\nìˆìŠµë‹ˆë‹¤.(gradient ê°’ ì €ì¥)\r\ntensorì˜ parameter\r\ndata : listë‚˜ ndarray ë“± arrayë°ì´í„°\r\ndtype : ë°ì´í„° íƒ€ì…\r\ndevice : tensorê°€ ìœ„ì¹˜í•´ìˆëŠ” device ì§€ì •\r\nrequires_grad : gradient ê°’ ì €ì¥ ìœ ë¬´\r\ntensorì˜ ìƒì„±\r\nndarrayì™€ ë¹„ìŠ·í•œ ìƒì„± ë°©ë²•ì…ë‹ˆë‹¤.\r\n\r\na = torch.tensor([[1.0, 4.0], [4.0, 3.0]])\r\nb = torch.tensor([[4, 3], [1, 4], [1, 2]])\r\nprint(a)\r\ntensor([[1., 4.],\r\n        [4., 3.]])\r\nprint(b)\r\ntensor([[4, 3],\r\n        [1, 4],\r\n        [1, 2]])\r\n\r\ntensor.dtype : tensorì˜ ìë£Œí˜• í™•ì¸\r\ntensor.shape : tensorì˜ size í™•ì¸\r\n\r\nprint(a.dtype, a.shape)\r\ntorch.float32 torch.Size([2, 2])\r\nprint(b.dtype, b.shape)\r\ntorch.int64 torch.Size([3, 2])\r\n\r\ntorch.ones(*size) : ëª¨ë“  ì›ì†Œê°€ 1ì¸ tensor\r\ntorch.zeros(*size) : ëª¨ë“  ì›ì†Œê°€ 0ì¸ tensor\r\ntorch.eye(n, m) : ëŒ€ê° ì›ì†Œê°€ 1ì´ê³  ë‚˜ë¨¸ì§€ê°€ 0ì¸ \\(n*m\\) tensorë¥¼ ìƒì„±. m = Noneì´ë©´ \\(n*n\\)tensorë¥¼ ìƒì„±\r\ntorch.rand(*size) : ëª¨ë“  ì›ì†Œë¥¼ ëœë¤í•œ ê°’ìœ¼ë¡œ ì±„ì›Œì§„ tensor.\r\ndtypeì„ intë¡œ ì§€ì •ì‹œ ì—ëŸ¬ê°€ ë°œìƒ\r\n\r\na = torch.ones([2, 3])\r\nb = torch.zeros([3, 2], dtype=torch.int64)\r\nc = torch.eye(4,3)\r\nd = torch.rand([2, 4, 3], dtype=torch.float)\r\n\r\nprint(a)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.]])\r\nprint(b)\r\ntensor([[0, 0],\r\n        [0, 0],\r\n        [0, 0]])\r\nprint(c)\r\ntensor([[1., 0., 0.],\r\n        [0., 1., 0.],\r\n        [0., 0., 1.],\r\n        [0., 0., 0.]])\r\nprint(d)\r\ntensor([[[0.1721, 0.5773, 0.8588],\r\n         [0.8192, 0.3488, 0.5716],\r\n         [0.3033, 0.4843, 0.1258],\r\n         [0.6734, 0.4262, 0.9605]],\r\n\r\n        [[0.0436, 0.7960, 0.0340],\r\n         [0.6968, 0.8373, 0.5634],\r\n         [0.0413, 0.0030, 0.7226],\r\n         [0.0329, 0.8479, 0.7469]]])\r\n\r\nì•ì„œ ì–¸ê¸‰í–ˆë“¯ì´ ndarrayë¡œë„ tensorë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. tensor()ëŠ”\r\nì›ë³¸ì˜ ê°’ì„ ë³µì‚¬í•˜ëŠ” ë°˜ë©´ as_tensor()ì™€ from_numpy()ëŠ” ì›ë³¸ì˜ ê°’ì„\r\nì°¸ì¡°í•˜ê¸° ë•Œë¬¸ì— ì›ë³¸ì˜ ê°’ì„ ë°”ê¾¸ë©´ ê°™ì´ ë³€í•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n\r\nimport numpy as np\r\n\r\nd = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])\r\n\r\na = torch.tensor(d)\r\nb = torch.as_tensor(d)\r\nc = torch.from_numpy(d)\r\n\r\nprint(a, id(a))\r\ntensor([[1, 1, 1],\r\n        [2, 2, 2],\r\n        [3, 3, 3]], dtype=torch.int32) 883303344\r\nprint(b, id(b))\r\ntensor([[1, 1, 1],\r\n        [2, 2, 2],\r\n        [3, 3, 3]], dtype=torch.int32) 883288720\r\nprint(c, id(c))\r\ntensor([[1, 1, 1],\r\n        [2, 2, 2],\r\n        [3, 3, 3]], dtype=torch.int32) 883073808\r\nprint(d, id(d))\r\n[[1 1 1]\r\n [2 2 2]\r\n [3 3 3]] 883007184\r\nd[0,0] = 0\r\n\r\nprint(a)\r\ntensor([[1, 1, 1],\r\n        [2, 2, 2],\r\n        [3, 3, 3]], dtype=torch.int32)\r\nprint(b)\r\ntensor([[0, 1, 1],\r\n        [2, 2, 2],\r\n        [3, 3, 3]], dtype=torch.int32)\r\nprint(c)\r\ntensor([[0, 1, 1],\r\n        [2, 2, 2],\r\n        [3, 3, 3]], dtype=torch.int32)\r\nprint(d)\r\n[[0 1 1]\r\n [2 2 2]\r\n [3 3 3]]\r\n\r\ntensor.shapeì„ í†µí•´ ë™ì¼í•œ sizeì˜ tensorë¥¼ ë§Œë“¤ ìˆ˜ë„ ìˆì§€ë§Œ\r\n_like(tensor) ë°©ë²•ìœ¼ë¡œë„ ë™ì¼í•œ sizeì˜ tensorë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n\r\na = torch.ones(a.shape)\r\nprint(a)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.],\r\n        [1., 1., 1.]])\r\nb = torch.ones_like(a)\r\nc = torch.zeros_like(a, dtype=torch.float)\r\nd = torch.rand_like(a, dtype=torch.float)\r\n\r\nprint(b)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.],\r\n        [1., 1., 1.]])\r\nprint(c)\r\ntensor([[0., 0., 0.],\r\n        [0., 0., 0.],\r\n        [0., 0., 0.]])\r\nprint(d)\r\ntensor([[0.0823, 0.9254, 0.6186],\r\n        [0.4919, 0.9770, 0.6451],\r\n        [0.4601, 0.9505, 0.9724]])\r\n\r\ndevice ì§€ì •\r\ngpuì—°ì‚°ì„ ìœ„í•´ì„œëŠ” tensorì˜ deviceë¥¼ cudaë¡œ ì§€ì •í•´ì¤˜ì•¼í•©ë‹ˆë‹¤.\r\ntorch.cuda.is_available()ë¥¼ í†µí•´ gpuê°€ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸\r\ní›„ tensor.to(â€˜cudaâ€™)ë¥¼ í†µí•´ deviceë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n\r\ntorch.cuda.is_available() # ì €ëŠ” gpuê°€ ì—†ì–´ìš” ã… ã…œ\r\nFalse\r\nif torch.cuda.is_available():\r\n    device = \"cuda\"\r\nelse:\r\n    device = \"cpu\"\r\n    \r\na.to(device)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.],\r\n        [1., 1., 1.]])\r\na.device\r\ndevice(type='cpu')\r\n\r\ntensor ë‹¤ë£¨ê¸°\r\n\r\na = torch.ones([2, 3])\r\nb = torch.zeros([2, 3])\r\n\r\ntorch.cat() : í…ì„œ í•©ì¹˜ê¸°\r\n\r\nc = torch.cat([a, b], dim=0) # ì—´ë°©í–¥ìœ¼ë¡œ í•©ì¹˜ê¸°\r\nd = torch.cat([a, b], dim=1) # í–‰ë°©í–¥ìœ¼ë¡œ í•©ì¹˜ê¸°\r\nprint(c)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.],\r\n        [0., 0., 0.],\r\n        [0., 0., 0.]])\r\nprint(c.shape)\r\ntorch.Size([4, 3])\r\nprint(d)\r\ntensor([[1., 1., 1., 0., 0., 0.],\r\n        [1., 1., 1., 0., 0., 0.]])\r\nprint(d.shape)\r\ntorch.Size([2, 6])\r\n\r\ntorch.stack : í…ì„œ ìŒ“ê¸°\r\nìƒˆë¡œìš´ ì°¨ì›(dim)ì— ë”°ë¼ tensorë“¤ ìŒ“ì•„ì¤ë‹ˆë‹¤. dim=0ì¼ë•Œì—ëŠ” tensor\r\nì „ì²´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìŒ“ê³ , dim=1ì¼ë•ŒëŠ” tensorì˜ ë‹¤ìŒ ì°¨ì›ì„ ê¸°ì¤€ìœ¼ë¡œ\r\nìŒ“ëŠ”ì‹ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ dimì€ 0ë¶€í„° tensorì˜ ì°¨ì›ì˜ ìˆ˜ë¥¼ ë„˜ì„ ìˆ˜ ì—†ê³ ,\r\nstack ì•ˆì˜ tensorë“¤ì€ ì„œë¡œ sizeê°€ ê°™ì•„ì•¼ í•©ë‹ˆë‹¤.\r\n\r\nc = torch.stack([a, b], dim=0)\r\nd = torch.stack([a, b], dim=1)\r\ne = torch.stack([a, b], dim=2)\r\nprint(c)\r\ntensor([[[1., 1., 1.],\r\n         [1., 1., 1.]],\r\n\r\n        [[0., 0., 0.],\r\n         [0., 0., 0.]]])\r\nprint(c.shape)\r\ntorch.Size([2, 2, 3])\r\nprint(d)\r\ntensor([[[1., 1., 1.],\r\n         [0., 0., 0.]],\r\n\r\n        [[1., 1., 1.],\r\n         [0., 0., 0.]]])\r\nprint(d.shape)\r\ntorch.Size([2, 2, 3])\r\nprint(e)\r\ntensor([[[1., 0.],\r\n         [1., 0.],\r\n         [1., 0.]],\r\n\r\n        [[1., 0.],\r\n         [1., 0.],\r\n         [1., 0.]]])\r\nprint(e.shape)\r\ntorch.Size([2, 3, 2])\r\n\r\nhstack : torch.cat([a, b], dim=0)\r\nvstack : torch.cat([a, b], dim=1)\r\n\r\nc = torch.hstack([a, b])\r\nprint(c)\r\ntensor([[1., 1., 1., 0., 0., 0.],\r\n        [1., 1., 1., 0., 0., 0.]])\r\nprint(c.shape)\r\ntorch.Size([2, 6])\r\nd = torch.vstack([a, b])\r\nprint(d)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.],\r\n        [0., 0., 0.],\r\n        [0., 0., 0.]])\r\nprint(d.shape)\r\ntorch.Size([4, 3])\r\n\r\ntorch.unsqueeze() : ë”ë¯¸ì°¨ì› ì‚­ì œ\r\ntorch.squeeze() : ë”ë¯¸ì°¨ì› ì¶”ê°€\r\n\r\n# unsqueeze and squeeze\r\nprint(a)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.]])\r\na = torch.unsqueeze(a, dim=1)\r\nprint(a)\r\ntensor([[[1., 1., 1.]],\r\n\r\n        [[1., 1., 1.]]])\r\nprint(a.shape)\r\ntorch.Size([2, 1, 3])\r\nprint(torch.squeeze(a))\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.]])\r\nprint(a.shape)\r\ntorch.Size([2, 1, 3])\r\n\r\ntonsorì˜ ì—°ì‚°\r\ní–‰ë ¬ ê³±\r\n\r\na = torch.tensor(np.array(list(range(12)))).reshape(3, 4)\r\nprint(a)\r\ntensor([[ 0,  1,  2,  3],\r\n        [ 4,  5,  6,  7],\r\n        [ 8,  9, 10, 11]], dtype=torch.int32)\r\nb = torch.tensor(np.array(list(range(8)))).reshape(4, 2)\r\nprint(b)\r\ntensor([[0, 1],\r\n        [2, 3],\r\n        [4, 5],\r\n        [6, 7]], dtype=torch.int32)\r\nc = a @ b\r\nprint(c)\r\ntensor([[ 28,  34],\r\n        [ 76,  98],\r\n        [124, 162]], dtype=torch.int32)\r\nd = torch.matmul(a, b)\r\nprint(d)\r\ntensor([[ 28,  34],\r\n        [ 76,  98],\r\n        [124, 162]], dtype=torch.int32)\r\n\r\nelement-wise product\r\n\r\na = torch.tensor(np.array(list(range(6)))).reshape(2, 3)\r\nprint(a)\r\ntensor([[0, 1, 2],\r\n        [3, 4, 5]], dtype=torch.int32)\r\nb = torch.tensor(list(range(10, 13))).reshape(1, 3)\r\nprint(b)\r\ntensor([[10, 11, 12]])\r\nc = a * b\r\nprint(c)\r\ntensor([[ 0, 11, 24],\r\n        [30, 44, 60]])\r\nd = a.mul(b)\r\nprint(d)\r\ntensor([[ 0, 11, 24],\r\n        [30, 44, 60]])\r\n\r\nitem() : tensorì˜ ì›ì†Œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. tensor ì•ˆì— ì›ì†Œê°€ 1ê°œë§Œ ìˆì–´ì•¼\r\ní•©ë‹ˆë‹¤.\r\n\r\n# item\r\nagg = d.sum()\r\nv = agg.item()\r\nprint(v, type(v))\r\n169 <class 'int'>\r\n\r\ninplace operations\r\n\r\nprint(a)\r\ntensor([[0, 1, 2],\r\n        [3, 4, 5]], dtype=torch.int32)\r\na.add(5) \r\ntensor([[ 5,  6,  7],\r\n        [ 8,  9, 10]], dtype=torch.int32)\r\nprint(a)\r\ntensor([[0, 1, 2],\r\n        [3, 4, 5]], dtype=torch.int32)\r\na.add_(5) # inplace operations\r\ntensor([[ 5,  6,  7],\r\n        [ 8,  9, 10]], dtype=torch.int32)\r\nprint(a)\r\ntensor([[ 5,  6,  7],\r\n        [ 8,  9, 10]], dtype=torch.int32)\r\n\r\nDataset\r\níŒŒì´í† ì¹˜ì—ì„œ Datasetì€ ì „ì²´ ë°ì´í„°ë¥¼ sample ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•´ì£¼ëŠ” ì—­í• ì„\r\ní•©ë‹ˆë‹¤. Datasetì„ ìƒì†ë°›ì•„ ì˜¤ë²„ë¼ì´ë”©ì„ í†µí•´ ì»¤ìŠ¤í…€ Datasetì„\r\në§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤.\r\nì»¤ìŠ¤í…€ Dataset êµ¬ì¡°\r\n\r\nclass CustomDataset(torch.utils.data.Dataset): \r\n    def __init__(self): \r\n        # ë°ì´í„°ì…‹ì˜ ì „ì²˜ë¦¬ë¥¼ í•´ì£¼ëŠ” ë¶€ë¶„\r\n        \r\n    def __len__(self):\r\n        # ë°ì´í„°ì…‹ì˜ ê¸¸ì´ë¥¼ ë°˜í™˜í•´ì£¼ëŠ” ë¶€ë¶„\r\n        \r\n    def __getitem__(self, idx):\r\n        # ë°ì´í„°ì…‹ì—ì„œ ìƒ˜í”Œì„ ì¶”ì¶œí•´ì£¼ëŠ” ë¶€ë¶„\r\n\r\në‹¤ìŒ ì»¤ìŠ¤í…€ ë°ì´í„° ì…‹ì„ í™•ì¸í•´ë³´ë©´ __init__ì—ì„œ\r\nfeaturesì™€ target ë°ì´í„°, ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì €ì¥í•˜ë„ë¡ ì •ì˜ ë˜ìˆê³ ,\r\n__len__ì—ì„œ dataì˜ ê¸¸ì´ë¥¼ ë°˜í™˜í•´ì£¼ë„ë¡ ì •ì˜ë˜ìˆìŠµë‹ˆë‹¤.\r\në§ˆì§€ë§‰ìœ¼ë¡œ __getitem__ì—ì„œ ì €ì¥ëœ ë°ì´í„°ë¥¼ ì¸ë±ì‹± í›„ ì •ì˜ëœ\r\nì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ê±°ì³ ë°˜í™˜ë˜ë„ë¡ êµ¬í˜„ë˜ìˆìŠµë‹ˆë‹¤.\r\n\r\nclass LionDataset(Dataset):\r\n    def __init__(self, data, target, transform=None, target_transform=None):\r\n        self.data = data # feature data\r\n        self.target = target # target data\r\n        self.transform = transform # featrue\r\n        self.target_transform = target_transform\r\n        pass\r\n    \r\n    def __len__(self):\r\n        return len(self.data)\r\n    def __getitem__(self, idx):\r\n        x = self.data[idx]\r\n        y = self.target[idx]\r\n        \r\n        if self.transform:\r\n          x = self.transform(x)\r\n        if self.target_transform:\r\n          y = self.target_transform(y)\r\n        \r\n        return x, y\r\n\r\n\r\ndata = np.array(list(range(100))).reshape(-1, 2)\r\ntarget = np.array([[i] * 5 for i in range(10)]).reshape(-1)\r\n\r\nprint(data)\r\n[[ 0  1]\r\n [ 2  3]\r\n [ 4  5]\r\n [ 6  7]\r\n [ 8  9]\r\n [10 11]\r\n [12 13]\r\n [14 15]\r\n [16 17]\r\n [18 19]\r\n [20 21]\r\n [22 23]\r\n [24 25]\r\n [26 27]\r\n [28 29]\r\n [30 31]\r\n [32 33]\r\n [34 35]\r\n [36 37]\r\n [38 39]\r\n [40 41]\r\n [42 43]\r\n [44 45]\r\n [46 47]\r\n [48 49]\r\n [50 51]\r\n [52 53]\r\n [54 55]\r\n [56 57]\r\n [58 59]\r\n [60 61]\r\n [62 63]\r\n [64 65]\r\n [66 67]\r\n [68 69]\r\n [70 71]\r\n [72 73]\r\n [74 75]\r\n [76 77]\r\n [78 79]\r\n [80 81]\r\n [82 83]\r\n [84 85]\r\n [86 87]\r\n [88 89]\r\n [90 91]\r\n [92 93]\r\n [94 95]\r\n [96 97]\r\n [98 99]]\r\nprint(target)\r\n[0 0 0 0 0 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 4 4 4 4 4 5 5 5 5 5 6 6 6 6 6 7 7\r\n 7 7 7 8 8 8 8 8 9 9 9 9 9]\r\n\r\n\r\nlion = LionDataset(data=data, target=target)\r\nprint(lion[0:4])\r\n(array([[0, 1],\r\n       [2, 3],\r\n       [4, 5],\r\n       [6, 7]]), array([0, 0, 0, 0]))\r\nprint(len(lion))\r\n50\r\n\r\nDataLoader\r\nDataLoaderëŠ” datasetì„ batch ë‹¨ìœ„ë¡œ ë¬¶ì–´ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.\r\nbatch_size : batch_size\r\nshuffle : Trueì‹œ epochë§ˆë‹¤ ë°ì´í„°ê°€ í•™ìŠµë˜ëŠ” ìˆœì„œê°€ ì„ì„\r\n\r\nloader = DataLoader(dataset=lion, batch_size=10, shuffle=True)\r\n\r\nfor i, batch in enumerate(loader):\r\n    x, y = batch\r\n    if i == 0:\r\n        print(x)\r\n        print(y)\r\n    print(x.shape)\r\ntensor([[38, 39],\r\n        [ 4,  5],\r\n        [88, 89],\r\n        [62, 63],\r\n        [84, 85],\r\n        [44, 45],\r\n        [40, 41],\r\n        [20, 21],\r\n        [66, 67],\r\n        [68, 69]], dtype=torch.int32)\r\ntensor([3, 0, 8, 6, 8, 4, 4, 2, 6, 6], dtype=torch.int32)\r\ntorch.Size([10, 2])\r\ntorch.Size([10, 2])\r\ntorch.Size([10, 2])\r\ntorch.Size([10, 2])\r\ntorch.Size([10, 2])\r\n\r\ntorch.nn.Module\r\npytorch ëª¨ë¸ì€ parametersë¥¼ ì¶”ì í•˜ë©° forward passë¥¼ ì§„í–‰í•œ ë’¤ back\r\npropagationì„ í†µí•´ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤. torch.nn.Moduleì€ ì—¬ëŸ¬ ì¸µì˜\r\nlayerë¡œ ì´ë¤„ì§„ ëª¨ë¸ì„ ì‰½ê²Œ ê´€ë¦¬í•  ìˆ˜ ìˆëŠ” classì…ë‹ˆë‹¤.\r\npytorch ëª¨ë¸ì˜ ê¸°ë³¸êµ¬ì¡°\r\n\r\nclass Model_Name(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        \"\"\"\r\n        ëª¨ë¸ì— ì‚¬ìš©ë  Layer(nn.Linear, nn.Conv2d)ì™€ \r\n        activation function(nn.functional.relu, nn.functional.sigmoid)ë“±ì„ ì •ì˜\r\n        \"\"\"\r\n\r\n    def forward(self, x):\r\n        \"\"\"\r\n        ëª¨ë¸ì—ì„œ ì‹¤í–‰ë˜ì–´ì•¼í•˜ëŠ” ê³„ì‚°ì„ ì •ì˜\r\n        \"\"\"\r\n        return x\r\n\r\nexample\r\n\r\nclass LionLinear(nn.Module):\r\n    def __init__(self, input_dim, output_dim):\r\n        super().__init__()\r\n        self.input_dim = input_dim   # ì…ë ¥ì°¨ì› \r\n        self.output_dim = output_dim # ì¶œë ¥ì°¨ì›\r\n        \r\n        self.flatten = nn.Flatten()  # tensor í‰íƒ„í™” ì •ì˜\r\n        self.classifier = nn.Linear(input_dim, output_dim) # Linear layer ì •ì˜\r\n        self.act = nn.ReLU() # activation function(ReLU) ì •ì˜\r\n        \r\n    def forward(self, x):\r\n        x = self.flatten(x)    # dataë¥¼ linear layersì— ë§ê²Œ í‰íƒ„í™” í›„\r\n        x = self.classifier(x) # linear layerë¥¼ í†µê³¼,\r\n        x = self.act(x)        # activation functionì„ í†µí•´ ì¶œë ¥\r\n        \r\n        return x\r\n\r\npytorch ëª¨ë¸ì€ ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ì‰½ê²Œ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n\r\nlinear_model = LionLinear(28*28, 10).to(device)\r\nprint(linear_model)\r\nLionLinear(\r\n  (flatten): Flatten(start_dim=1, end_dim=-1)\r\n  (classifier): Linear(in_features=784, out_features=10, bias=True)\r\n  (act): ReLU()\r\n)\r\nprint(linear_model.classifier)\r\nLinear(in_features=784, out_features=10, bias=True)\r\n\r\në‹¤ìŒì€ MLPë¥¼ êµ¬í˜„í•˜ëŠ” moduleì…ë‹ˆë‹¤.\r\nì½”ë“œë¥¼ ë” ê°„ê²°í•˜ê²Œ í•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì´ ì¼ë¶€ layer ë“±ì„ ë”°ë¡œ ëª¨ë“ˆë¡œ\r\nêµ¬í˜„ í›„ ì „ì²´ ëª¨ë“ˆì— í•©ì³ì„œ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n\r\nclass LionLayer(nn.Module):\r\n    def __init__(self, input_dim, output_dim):\r\n        super().__init__()\r\n        self.input_dim = input_dim\r\n        self.output_dim = output_dim\r\n        self.layer = nn.Linear(self.input_dim, self.output_dim)\r\n        pass\r\n\r\n    def forward(self, x):\r\n        assert x.shape[-1] == self.input_dim, \"Input dimension mismatch\"\r\n        return self.layer(x)\r\n\r\n\r\nclass LionMLP(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim):\r\n        super().__init__()\r\n        self.input_dim = input_dim\r\n        self.hidden_dim = hidden_dim\r\n        self.output_dim = output_dim\r\n        self.flatten = nn.Flatten()\r\n        self.linear_1 = LionLayer(self.input_dim, self.hidden_dim)\r\n        self.linear_2 = LionLayer(self.hidden_dim, self.output_dim)\r\n        self.act_1 = nn.ReLU()\r\n        self.act_2 = nn.Softmax()\r\n        pass\r\n    \r\n    def forward(self, x):\r\n        x = self.flatten(x)\r\n        x = self.act_1(self.linear_1(x))\r\n        x = self.act_2(self.linear_2(x))\r\n        return x\r\n\r\n\r\nmlp = LionMLP(28*28, 50, 10)\r\nprint(mlp)\r\nLionMLP(\r\n  (flatten): Flatten(start_dim=1, end_dim=-1)\r\n  (linear_1): LionLayer(\r\n    (layer): Linear(in_features=784, out_features=50, bias=True)\r\n  )\r\n  (linear_2): LionLayer(\r\n    (layer): Linear(in_features=50, out_features=10, bias=True)\r\n  )\r\n  (act_1): ReLU()\r\n  (act_2): Softmax(dim=None)\r\n)\r\n\r\nnn.Sequential()ì„ í†µí•´ forward() ë¶€ë¶„ì„ ì§§ê²Œ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n\r\nclass LionMLP(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim):\r\n        super().__init__()\r\n        self.input_dim = input_dim\r\n        self.hidden_dim = hidden_dim\r\n        self.output_dim = output_dim\r\n        self.flatten = nn.Flatten()\r\n        self.linear_1 = LionLayer(self.input_dim, self.hidden_dim)\r\n        self.linear_2 = LionLayer(self.hidden_dim, self.output_dim)\r\n        self.act_1 = nn.ReLU()\r\n        self.act_2 = nn.Softmax()\r\n        self.model = nn.Sequential(self.flatten, self.linear_1, self.act_1, self.linear_2, self.act_2)\r\n        pass\r\n    def forward(self, x):\r\n        return self.model(x)\r\n\r\n\r\nmlp = LionMLP(28 * 28, 40, 10)\r\nprint(mlp)\r\nLionMLP(\r\n  (flatten): Flatten(start_dim=1, end_dim=-1)\r\n  (linear_1): LionLayer(\r\n    (layer): Linear(in_features=784, out_features=40, bias=True)\r\n  )\r\n  (linear_2): LionLayer(\r\n    (layer): Linear(in_features=40, out_features=10, bias=True)\r\n  )\r\n  (act_1): ReLU()\r\n  (act_2): Softmax(dim=None)\r\n  (model): Sequential(\r\n    (0): Flatten(start_dim=1, end_dim=-1)\r\n    (1): LionLayer(\r\n      (layer): Linear(in_features=784, out_features=40, bias=True)\r\n    )\r\n    (2): ReLU()\r\n    (3): LionLayer(\r\n      (layer): Linear(in_features=40, out_features=10, bias=True)\r\n    )\r\n    (4): Softmax(dim=None)\r\n  )\r\n)\r\n\r\nëª¨ë¸ íŒŒë¼ë¯¸í„° í™•ì¸\r\n\r\nfor name, param in mlp.named_parameters():\r\n    print(f\"Layer: {name} | Size: {param.size()} | Values: {param[:2]} \\n\")\r\nLayer: linear_1.layer.weight | Size: torch.Size([40, 784]) | Values: tensor([[ 0.0178, -0.0108, -0.0017,  ...,  0.0248, -0.0011,  0.0218],\r\n        [-0.0343,  0.0235, -0.0201,  ...,  0.0321, -0.0141,  0.0180]],\r\n       grad_fn=<SliceBackward0>) \r\n\r\nLayer: linear_1.layer.bias | Size: torch.Size([40]) | Values: tensor([ 0.0207, -0.0041], grad_fn=<SliceBackward0>) \r\n\r\nLayer: linear_2.layer.weight | Size: torch.Size([10, 40]) | Values: tensor([[ 0.0301, -0.0820, -0.1123, -0.0777, -0.0678, -0.1305, -0.0293,  0.0596,\r\n         -0.0557, -0.1315, -0.1003,  0.0239,  0.1351,  0.0894, -0.0559, -0.0034,\r\n          0.0682,  0.1417,  0.0025, -0.0303, -0.0004, -0.0029,  0.0281,  0.0950,\r\n          0.0631, -0.0958, -0.0276, -0.1566,  0.1130, -0.0361,  0.0906, -0.0657,\r\n          0.1251,  0.0872, -0.1033,  0.0821,  0.0856, -0.1505,  0.1350,  0.1250],\r\n        [ 0.0562,  0.0557,  0.0730, -0.1076,  0.0850, -0.1426,  0.1005,  0.1108,\r\n          0.0231,  0.1560,  0.1185, -0.0472,  0.0049,  0.0836,  0.0008, -0.1055,\r\n          0.1363,  0.1266, -0.0864,  0.1325,  0.1444,  0.1412,  0.1253, -0.0832,\r\n          0.0536,  0.0351, -0.1228, -0.0855, -0.0909,  0.0840, -0.0335,  0.0978,\r\n         -0.0824,  0.1048,  0.0254, -0.0287,  0.0238,  0.1337, -0.1085,  0.1532]],\r\n       grad_fn=<SliceBackward0>) \r\n\r\nLayer: linear_2.layer.bias | Size: torch.Size([10]) | Values: tensor([ 0.0235, -0.0153], grad_fn=<SliceBackward0>) \r\n\r\nreference\r\nPyTorchë¡œ ì‹œì‘í•˜ëŠ” ë”¥ ëŸ¬ë‹ ì…ë¬¸ : https://wikidocs.net/57165\r\nhttps://anweh.tistory.com/21\r\n\r\n\r\n\r\n",
    "preview": "til/2022-05-20-pytorch-tutorial/images/pytorch.jpg",
    "last_modified": "2022-05-22T15:10:31+09:00",
    "input_file": {}
  },
  {
    "path": "til/2022-05-19-cotepart1/",
    "title": "[ë°±ì¤€ ë¬¸ì œí’€ì´] ê¸°ë³¸ê¸°",
    "description": "ì½”ë”©í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ë°±ì¤€ ë¬¸ì œ í’€ì–´ë³´ê¸°",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-19",
    "categories": [
      "coding test"
    ],
    "contents": "\r\n\r\nContents\r\nì•½ìˆ˜ êµ¬í•˜ê¸°\r\nì´ì§„ìˆ˜\r\nbin() ì§ì ‘ êµ¬í˜„í•˜ê¸°(ì–‘ì˜\r\nì •ìˆ˜ì¼ë•Œë§Œ)\r\n\r\nìµœëŒ€, ìµœì†Œ\r\n\r\nì•½ìˆ˜ êµ¬í•˜ê¸°\r\në‘ ê°œì˜ ìì—°ìˆ˜ Nê³¼ Kê°€ ì£¼ì–´ì¡Œì„ ë•Œ, Nì˜ ì•½ìˆ˜ë“¤ ì¤‘ Kë²ˆì§¸ë¡œ ì‘ì€ ìˆ˜ë¥¼\r\nì¶œë ¥í•˜ëŠ” í”„ë¡œê·¸ë¨ì„ ì‘ì„±í•˜ì‹œì˜¤.\r\nì…ë ¥\r\nì²«ì§¸ ì¤„ì— Nê³¼ Kê°€ ë¹ˆì¹¸ì„ ì‚¬ì´ì— ë‘ê³  ì£¼ì–´ì§„ë‹¤. Nì€ 1 ì´ìƒ 10,000\r\nì´í•˜ì´ë‹¤. KëŠ” 1 ì´ìƒ N ì´í•˜ì´ë‹¤.\r\nì¶œë ¥\r\nì²«ì§¸ ì¤„ì— Nì˜ ì•½ìˆ˜ë“¤ ì¤‘ Kë²ˆì§¸ë¡œ ì‘ì€ ìˆ˜ë¥¼ ì¶œë ¥í•œë‹¤. ë§Œì¼ Nì˜ ì•½ìˆ˜ì˜\r\nê°œìˆ˜ê°€ Kê°œë³´ë‹¤ ì ì–´ì„œ Kë²ˆì§¸ ì•½ìˆ˜ê°€ ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš°ì—ëŠ” 0ì„\r\nì¶œë ¥í•˜ì‹œì˜¤.\r\n\r\nN, K = map(int, input().split())\r\n\r\nresult = 0\r\n\r\nfor i in range(1, N + 1):\r\n    if N % i == 0:\r\n        K -= 1\r\n        if K == 0:\r\n            result = i\r\n            break\r\n\r\nprint(result)\r\n\r\nì´ì§„ìˆ˜\r\nì–‘ì˜ ì •ìˆ˜ nì´ ì£¼ì–´ì¡Œì„ ë•Œ, ì´ë¥¼ ì´ì§„ìˆ˜ë¡œ ë‚˜íƒ€ëƒˆì„ ë•Œ 1ì˜ ìœ„ì¹˜ë¥¼ ëª¨ë‘\r\nì°¾ëŠ” í”„ë¡œê·¸ë¨ì„ ì‘ì„±í•˜ì‹œì˜¤. ìµœí•˜ìœ„ ë¹„íŠ¸(least significant bit, lsb)ì˜\r\nìœ„ì¹˜ëŠ” 0ì´ë‹¤.\r\nì…ë ¥\r\nì²«ì§¸ ì¤„ì— í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì˜ ê°œìˆ˜ Tê°€ ì£¼ì–´ì§„ë‹¤. ê° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ëŠ” í•œ\r\nì¤„ë¡œ ì´ë£¨ì–´ì ¸ ìˆê³ , nì´ ì£¼ì–´ì§„ë‹¤. \\((1 â‰¤ T â‰¤\r\n10, 1 â‰¤ n â‰¤ 10^6)\\)\r\nì¶œë ¥\r\nê° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì— ëŒ€í•´ì„œ, 1ì˜ ìœ„ì¹˜ë¥¼ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„í•´ì„œ ì¤„ í•˜ë‚˜ì—\r\nì¶œë ¥í•œë‹¤. ìœ„ì¹˜ê°€ ë‚®ì€ ê²ƒë¶€í„° ì¶œë ¥í•œë‹¤.\r\n\r\nt = int(input())\r\n\r\nfor _ in range(t):\r\n    n = int(input())\r\n    b = bin(n)[2:]\r\n    \r\n    for i in range(len(b)):\r\n        if b[::-1][i] == '1':\r\n            print(i, end=' ')\r\n\r\nbin() ì§ì ‘ êµ¬í˜„í•˜ê¸°(ì–‘ì˜\r\nì •ìˆ˜ì¼ë•Œë§Œ)\r\n\r\ndef my_bin(num):\r\n    a = num//2\r\n    b = num%2\r\n    if a == 0:\r\n        return '1'\r\n    else:\r\n        return my_bin(a)+str(b)\r\n\r\nìµœëŒ€, ìµœì†Œ\r\nNê°œì˜ ì •ìˆ˜ê°€ ì£¼ì–´ì§„ë‹¤. ì´ë•Œ, ìµœì†Ÿê°’ê³¼ ìµœëŒ“ê°’ì„ êµ¬í•˜ëŠ” í”„ë¡œê·¸ë¨ì„\r\nì‘ì„±í•˜ì‹œì˜¤.\r\nì…ë ¥\r\nì²«ì§¸ ì¤„ì— ì •ìˆ˜ì˜ ê°œìˆ˜ N (1 â‰¤ N â‰¤ 1,000,000)ì´ ì£¼ì–´ì§„ë‹¤. ë‘˜ì§¸ ì¤„ì—ëŠ”\r\nNê°œì˜ ì •ìˆ˜ë¥¼ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„í•´ì„œ ì£¼ì–´ì§„ë‹¤. ëª¨ë“  ì •ìˆ˜ëŠ” -1,000,000ë³´ë‹¤\r\ní¬ê±°ë‚˜ ê°™ê³ , 1,000,000ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ì€ ì •ìˆ˜ì´ë‹¤.\r\nì¶œë ¥\r\nì²«ì§¸ ì¤„ì— ì£¼ì–´ì§„ ì •ìˆ˜ Nê°œì˜ ìµœì†Ÿê°’ê³¼ ìµœëŒ“ê°’ì„ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„í•´\r\nì¶œë ¥í•œë‹¤.\r\n\r\ninput()\r\nl = list(map(int, input().split()))\r\n\r\nhigh = low = l[0]\r\n\r\nfor i in l[1:]:\r\n    if high < i:\r\n        high = i\r\n    if low > i:\r\n        low = i\r\n        \r\nprint(low, high)\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-20T19:04:09+09:00",
    "input_file": {}
  },
  {
    "path": "til/2022-05-18-dlfromscratchch2/",
    "title": "í¼ì…‰íŠ¸ë¡ ì´ë€",
    "description": "ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹ì„ ì½ê³  ë”¥ëŸ¬ë‹ì˜ ê¸°ì›ì´ ë˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì¸ í¼ì…‰íŠ¸ë¡ ì— ëŒ€í•´ ì •ë¦¬í•´ë´¤ìŠµë‹ˆë‹¤.",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-18",
    "categories": [
      "ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹"
    ],
    "contents": "\r\n\r\nContents\r\n1ï¸âƒ£ í¼ì…‰íŠ¸ë¡ ì´ë€\r\n2ï¸âƒ£ í¼ì…‰íŠ¸ë¡ ì˜ í•œê³„\r\n3ï¸âƒ£ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ \r\nâœ… ìš”ì•½\r\n\r\n1ï¸âƒ£ í¼ì…‰íŠ¸ë¡ ì´ë€\r\ní¼ì…‰íŠ¸ë¡ ì€ ë‹¤ìˆ˜ì˜ ì‹ í˜¸ë¥¼ ì…ë ¥(input)ìœ¼ë¡œ ë°›ì•„ í•˜ë‚˜ì˜ ì‹ í˜¸ë¥¼\r\nì¶œë ¥(output)í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.\r\n2ê°œì˜ ì…ë ¥ì„ ë°›ì•„ í•˜ë‚˜ì˜ ì‹ í˜¸ë¥¼ ì¶œë ¥í•˜ëŠ” í¼ì…‰íŠ¸ë¡ ì„ ìˆ˜ì‹ìœ¼ë¡œ\r\në‚˜íƒ€ë‚´ë©´,\r\n\\[y = \\begin{cases}0,(w_{1}x_{1} +\r\nw_{2}x_{2}\\leq\\theta)\\\\1,(w_{1}x_{1} +\r\nw_{2}x_{2}>\\theta)\\\\\\end{cases}\\] ë˜ëŠ” \\(\\theta\\)ë¥¼ \\(-b\\)ë¡œ ì¹˜í™˜í•˜ì—¬,\r\n\\[y = \\begin{cases}0,(b+w_{1}x_{1} +\r\nw_{2}x_{2}\\leq0)\\\\1,(b+w_{1}x_{1} +\r\nw_{2}x_{2}>0)\\\\\\end{cases}\\]\r\n\\(w\\) : weight, \\(b\\): bias\r\n2ï¸âƒ£ í¼ì…‰íŠ¸ë¡ ì˜ í•œê³„\r\ní¼ì…‰íŠ¸ë¡ ì´ í’€ ìˆ˜ ìˆëŠ” (ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ”) ë¬¸ì œëŠ” AND, NAND,\r\nORì…ë‹ˆë‹¤.\r\nAND\r\n\r\nx1x2y000100010111\r\n\r\nNAND\r\n\r\nx1x2y001101011110\r\n\r\nOR\r\n\r\nx1x2y000101011111\r\n\r\nìœ„ ì„¸ê°€ì§€ ë¬¸ì œëŠ” \\((w_1, w_2,\r\ntheta)\\)ì— ê°ê° \\((0.5, 0.5, 0.7),\r\n(-0.5, -0.5, -0.7), (1.5, 1.5, 0.7)\\) ë“±ì„ ëŒ€ì…í•˜ë©´ í’€ ìˆ˜\r\nìˆìŠµë‹ˆë‹¤.\r\nì´ë¥¼ íŒŒì´ì¬ í•¨ìˆ˜ë¡œ êµ¬í˜„í•˜ë©´,\r\n\r\ndef AND(x1, x2):\r\n    w1, w2, theta = 0.5, 0.5, 0.5\r\n    tmp = x1*w1+x2*w2\r\n    if tmp <= theta:\r\n        return 0\r\n    else:\r\n        return 1\r\n\r\ndef NAND(x1, x2):\r\n    w1, w2, theta = -0.5, -0.5, -0.7\r\n    tmp = x1*w1+x2*w2\r\n    if tmp <= theta:\r\n        return 0\r\n    else:\r\n        return 1\r\n    \r\ndef OR(x1, x2):\r\n    w1, w2, theta = 1.5, 1.5, 0.7\r\n    tmp = x1*w1+x2*w2\r\n    if tmp <= theta:\r\n        return 0\r\n    else:\r\n        return 1\r\n\r\nprint('x1=0, x2=0 : ', AND(0,0))\r\nx1=0, x2=0 :  0\r\nprint('x1=0, x2=1 : ', AND(0,1))\r\nx1=0, x2=1 :  0\r\nprint('x1=1, x2=0 : ', AND(1,0))\r\nx1=1, x2=0 :  0\r\nprint('x1=1, x2=1 : ', AND(1,1))\r\nx1=1, x2=1 :  1\r\nprint('x1=0, x2=0 : ', NAND(0,0))\r\nx1=0, x2=0 :  1\r\nprint('x1=0, x2=1 : ', NAND(0,1))\r\nx1=0, x2=1 :  1\r\nprint('x1=1, x2=0 : ', NAND(1,0))\r\nx1=1, x2=0 :  1\r\nprint('x1=1, x2=1 : ', NAND(1,1))\r\nx1=1, x2=1 :  0\r\nprint('x1=0, x2=0 : ', OR(0,0))\r\nx1=0, x2=0 :  0\r\nprint('x1=0, x2=1 : ', OR(0,1))\r\nx1=0, x2=1 :  1\r\nprint('x1=1, x2=0 : ', OR(1,0))\r\nx1=1, x2=0 :  1\r\nprint('x1=1, x2=1 : ', OR(1,1))\r\nx1=1, x2=1 :  1\r\n\r\ní•˜ì§€ë§Œ, í¼ì…‰íŠ¸ë¡ ìœ¼ë¡œ XORì€ í•´ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\r\nXOR\r\n\r\nx1x2y000101011110\r\n\r\nìœ„ ë¬¸ì œë¥¼ ê·¸ë¦¼ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ì´ ë¹¨ê°„ì ê³¼ ê²€ì€ì ì„ êµ¬ë¶„í•  ìˆ˜\r\nìˆëŠ” ì„ ì„ ê·¸ì„ ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\r\n\r\n\r\n\r\nê³¡ì„ ìœ¼ë¡œ ì„ ì„ ê·¸ë¦°ë‹¤ë©´ êµ¬ë¶„í•  ìˆ˜ ìˆê² ì§€ë§Œ í¼ì…‰íŠ¸ë¡ ìœ¼ë¡œëŠ” ì§ì„ ë°–ì—\r\nê·¸ë¦´ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ í•´ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¦‰, í¼ì…‰íŠ¸ë¡ ì€ ë¹„ì„ í˜•ì˜ ì˜ì—­ì„\r\ní‘œí˜„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\r\n3ï¸âƒ£ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ \r\nì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‚˜ì˜¨ ê²ƒì´ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ ì…ë‹ˆë‹¤. ë§ ê·¸ëŒ€ë¡œ í¼ì…‰íŠ¸ë¡ \r\nì¸µì„ ì—¬ëŸ¬ ê°œ ê°–ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë©° ì•ì—ì„œ ì–¸ê¸‰í–ˆë˜ í¼ì…‰íŠ¸ë¡ ì€ ì •í™•íˆ ë§í•˜ë©´\r\në‹¨ì¸µ í¼ì…‰íŠ¸ë¡ ì…ë‹ˆë‹¤.\r\nXORë¬¸ì œë¥¼ í•´ê²°í•˜ë ¤ë©´ ìœ„ì—ì„œ AND, NAND, OR ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´\r\në§Œë“¤ì—ˆë˜ í¼ì…‰íŠ¸ë¡ ì„ ì ì ˆíˆ ìŒ“ì•„ ë§Œë“¤ë©´ ë©ë‹ˆë‹¤.\r\n\r\ndef XOR(x1, x2):\r\n    s1 = NAND(x1, x2)\r\n    s2 = OR(x1, x2)\r\n    y = AND(s1, s2)\r\n    return y\r\n\r\nprint('x1=0, x2=0 : ', XOR(0,0))\r\nx1=0, x2=0 :  0\r\nprint('x1=0, x2=1 : ', XOR(0,1))\r\nx1=0, x2=1 :  1\r\nprint('x1=1, x2=0 : ', XOR(1,0))\r\nx1=1, x2=0 :  1\r\nprint('x1=1, x2=1 : ', XOR(1,1))\r\nx1=1, x2=1 :  0\r\n\r\nì²«ë²ˆì§¸ ì¸µì— NAND, ORë¥¼ í’€ê¸° ìœ„í•´ ë§Œë“¤ì—ˆë˜ í¼ì…‰íŠ¸ë¡ ì„ ìŒ“ê³  ë‘ë²ˆì§¸ ì¸µì—\r\nANDë¥¼ í’€ê¸° ìœ„í•´ ë§Œë“¤ì—ˆë˜ í¼ì…‰íŠ¸ë¡ ì„ ìŒ“ì•„ XORë¬¸ì œë¥¼ í•´ê²°í–ˆìŠµë‹ˆë‹¤.\r\nì´ì²˜ëŸ¼ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ ì€ ë¹„ì„ í˜•ì˜ ì˜ì—­ê¹Œì§€ë„ í‘œí˜„í•  ìˆ˜ ìˆìŒì„ ì•Œ ìˆ˜\r\nìˆìŠµë‹ˆë‹¤.\r\nâœ… ìš”ì•½\r\ní¼ì…‰íŠ¸ë¡ ì€ ì…ì¶œë ¥ì„ ê°–ì¶˜ ì•Œê³ ë¦¬ì¦˜\r\në‹¨ì¸µ í¼ì…‰íŠ¸ë¡ ì€ ì„ í˜• ì˜ì—­ë§Œ í‘œí˜„í•  ìˆ˜ ìˆê³ , ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ ì€\r\në¹„ì„ í˜• ì˜ì—­ë„ í‘œí˜„ ê°€ëŠ¥\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-19T00:59:30+09:00",
    "input_file": {}
  },
  {
    "path": "til/2022-05-13-first-til/",
    "title": "ë©‹ìŸì´ì‚¬ìì²˜ëŸ¼! TIL(Today I Learn) í˜ì´ì§€ ê°œì„¤!!",
    "description": "ë§¤ì¼ ê³µë¶€í•˜ëŠ” ê²ƒì„ ê¸°ë¡í•˜ê¸°",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-13",
    "categories": [],
    "contents": "\r\në§¤ì¼ë§¤ì¼ ê³µë¶€í•œ ê²ƒì„ ê¸°ë¡í•˜ëŠ” í˜ì´ì§€ë¥¼ ë§Œë“¤ì—ˆë‹¤. ì¼ê¸°ì²˜ëŸ¼ ë‚˜ì¤‘ì— ë´ë„\r\në¿Œë“¯í•œ í˜ì´ì§€ê°€ ë˜ê¸¸..\r\nğŸ˜„í™”ì´íŒ…!!\r\n\r\n\r\n\r\n",
    "preview": "til/2022-05-13-first-til/images/owl.png",
    "last_modified": "2022-05-26T18:50:34+09:00",
    "input_file": {},
    "preview_width": 640,
    "preview_height": 514
  }
]

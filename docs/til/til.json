[
  {
    "path": "til/2022-05-20-pytorch-tutorial/",
    "title": "Pytorch tutorial",
    "description": "딥러닝 프레임워크 파이토치에 대해 알아보자",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-20",
    "categories": [
      "pytorch"
    ],
    "contents": "\r\n\r\nContents\r\ntorch tensor\r\ntensor의 parameter\r\ntensor의 생성\r\ndevice 지정\r\ntensor 다루기\r\ntonsor의 연산\r\n\r\nDataset\r\nDataLoader\r\ntorch.nn.Module\r\n\r\nsocar 부트캠프의 강의 중 일부를 정리한 내용입니다.\r\ntorch tensor\r\ntensor는 numpy의 ndarray처럼 다차원 데이터 배열입니다. tensor를\r\n생성할 때는 list를 사용할 수도 있지만 ndarray를 사용할 수도\r\n있습니다.\r\nndarray와 tensor의 차이점은 tensor에는 back propagation(역전파)를\r\n다루기 위해 forward pass(순전파)에서 전달된 값과 연산의 종류를 기억할 수\r\n있습니다.(gradient 값 저장)\r\ntensor의 parameter\r\ndata : list나 ndarray 등 array데이터\r\ndtype : 데이터 타입\r\ndevice : tensor가 위치해있는 device 지정\r\nrequires_grad : gradient 값 저장 유무\r\npin_memory : True시 pinned memory에 할당, CPU tensor에서\r\n가능\r\ntensor의 생성\r\n\r\na = torch.tensor([[1.0, 4.0], [4.0, 3.0]])\r\nb = torch.tensor([[4, 3], [1, 4], [1, 2]])\r\nprint(a)\r\ntensor([[1., 4.],\r\n        [4., 3.]])\r\nprint(b)\r\ntensor([[4, 3],\r\n        [1, 4],\r\n        [1, 2]])\r\n\r\nndarray와 비슷한 생성 방법입니다.\r\n\r\nprint(a.dtype, a.shape)\r\ntorch.float32 torch.Size([2, 2])\r\nprint(b.dtype, b.shape)\r\ntorch.int64 torch.Size([3, 2])\r\n\r\ntorch.ones(*size) : 모든 원소가 1인 tensor\r\ntorch.zeros(*size) : 모든 원소가 0인 tensor\r\ntorch.eye(n, m) : 대각 원소가 1이고 나머지가 0인 \\(n*m\\) tensor를 생성. m = None이면 \\(n*n\\)tensor를 생성\r\ntorch.rand(*size) : 모든 원소를 랜덤한 값으로 채워진 tensor.\r\ndtype을 int로 지정시 에러가 발생\r\n\r\na = torch.ones([2, 3])\r\nb = torch.zeros([3, 2], dtype=torch.int64)\r\nc = torch.eye(4,3)\r\nd = torch.rand([2, 4, 3], dtype=torch.float)\r\n\r\nprint(a)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.]])\r\nprint(b)\r\ntensor([[0, 0],\r\n        [0, 0],\r\n        [0, 0]])\r\nprint(c)\r\ntensor([[1., 0., 0.],\r\n        [0., 1., 0.],\r\n        [0., 0., 1.],\r\n        [0., 0., 0.]])\r\nprint(d)\r\ntensor([[[0.9169, 0.3100, 0.5730],\r\n         [0.0963, 0.4855, 0.2943],\r\n         [0.8188, 0.8513, 0.0031],\r\n         [0.4675, 0.6421, 0.8515]],\r\n\r\n        [[0.2244, 0.8874, 0.0065],\r\n         [0.8683, 0.6122, 0.9006],\r\n         [0.5063, 0.8211, 0.0119],\r\n         [0.5276, 0.5330, 0.3839]]])\r\n\r\n앞서 언급했듯이 ndarray로도 tensor를 생성할 수 있다. as_tensor()와\r\nfrom_numpy()는 원본의 값을 참조하기 때문에 원본의 값을 바꾸면 같이\r\n변하는 것을 알 수 있습니다.\r\n\r\nimport numpy as np\r\n\r\nd = np.array([[2, 5, 4], [4, 2, 1], [5, 7, 4]])\r\n\r\na = torch.tensor(d)\r\nb = torch.as_tensor(d)\r\nc = torch.from_numpy(d)\r\n\r\nprint(a, id(a))\r\ntensor([[2, 5, 4],\r\n        [4, 2, 1],\r\n        [5, 7, 4]], dtype=torch.int32) 883421328\r\nprint(b, id(b))\r\ntensor([[2, 5, 4],\r\n        [4, 2, 1],\r\n        [5, 7, 4]], dtype=torch.int32) 883414736\r\nprint(c, id(c))\r\ntensor([[2, 5, 4],\r\n        [4, 2, 1],\r\n        [5, 7, 4]], dtype=torch.int32) 883177568\r\nprint(d, id(d))\r\n[[2 5 4]\r\n [4 2 1]\r\n [5 7 4]] 883130064\r\nd[0,0] = 0\r\n\r\nprint(a)\r\ntensor([[2, 5, 4],\r\n        [4, 2, 1],\r\n        [5, 7, 4]], dtype=torch.int32)\r\nprint(b)\r\ntensor([[0, 5, 4],\r\n        [4, 2, 1],\r\n        [5, 7, 4]], dtype=torch.int32)\r\nprint(c)\r\ntensor([[0, 5, 4],\r\n        [4, 2, 1],\r\n        [5, 7, 4]], dtype=torch.int32)\r\nprint(d)\r\n[[0 5 4]\r\n [4 2 1]\r\n [5 7 4]]\r\n\r\ntensor.shape을 통해 동일한 size의 tensor를 만들 수도 있지만\r\n_like(tensor) 방법으로도 동일한 size의 tensor를 만들 수 있습니다.\r\n\r\na = torch.ones([2, 3])\r\nprint(a)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.]])\r\nb = torch.ones_like(a)\r\nc = torch.zeros_like(a, dtype=torch.float)\r\nd = torch.rand_like(a, dtype=torch.float)\r\n\r\nprint(b)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.]])\r\nprint(c)\r\ntensor([[0., 0., 0.],\r\n        [0., 0., 0.]])\r\nprint(d)\r\ntensor([[0.0043, 0.2561, 0.6409],\r\n        [0.9898, 0.8409, 0.6337]])\r\n\r\ndevice 지정\r\ngpu연산을 위해서는 tensor의 device를 cuda로 지정해줘야합니다.\r\ntorch.cuda.is_available()를 통해 gpu가 사용 가능한지 확인\r\n후 tensor.to(‘cuda’)를 통해 device를 지정할 수 있습니다.\r\n\r\ntorch.cuda.is_available() # 저는 gpu가 없어요 ㅠㅜ\r\nFalse\r\nif torch.cuda.is_available():\r\n    device = \"cuda\"\r\nelse:\r\n    device = \"cpu\"\r\n    \r\na.to(device)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.]])\r\na.device\r\ndevice(type='cpu')\r\n\r\ntensor 다루기\r\n\r\na = torch.ones([2, 3])\r\nb = torch.zeros([2, 3])\r\n\r\ntorch.cat()\r\n\r\nc = torch.cat([a, b], dim=0) # 열방향으로 합치기\r\nd = torch.cat([a, b], dim=1) # 행방향으로 합치기\r\nprint(c)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.],\r\n        [0., 0., 0.],\r\n        [0., 0., 0.]])\r\nprint(c.shape)\r\ntorch.Size([4, 3])\r\nprint(d)\r\ntensor([[1., 1., 1., 0., 0., 0.],\r\n        [1., 1., 1., 0., 0., 0.]])\r\nprint(d.shape)\r\ntorch.Size([2, 6])\r\n\r\ntorch.stack\r\n새로운 차원(dim)에 따라 tensor들 쌓아줍니다. dim=0일때에는 tensor\r\n전체를 기준으로 쌓고, dim=1일때는 tensor의 다음 차원을 기준으로\r\n쌓는식입니다. 그러므로 dim은 0부터 tensor의 차원의 수를 넘을 수 없고,\r\nstack 안의 tensor들은 서로 size가 같아야 합니다.\r\n\r\nc = torch.stack([a, b], dim=0)\r\nd = torch.stack([a, b], dim=1)\r\ne = torch.stack([a, b], dim=2)\r\nprint(c)\r\ntensor([[[1., 1., 1.],\r\n         [1., 1., 1.]],\r\n\r\n        [[0., 0., 0.],\r\n         [0., 0., 0.]]])\r\nprint(c.shape)\r\ntorch.Size([2, 2, 3])\r\nprint(d)\r\ntensor([[[1., 1., 1.],\r\n         [0., 0., 0.]],\r\n\r\n        [[1., 1., 1.],\r\n         [0., 0., 0.]]])\r\nprint(d.shape)\r\ntorch.Size([2, 2, 3])\r\nprint(e)\r\ntensor([[[1., 0.],\r\n         [1., 0.],\r\n         [1., 0.]],\r\n\r\n        [[1., 0.],\r\n         [1., 0.],\r\n         [1., 0.]]])\r\nprint(e.shape)\r\ntorch.Size([2, 3, 2])\r\n\r\nhstack : torch.cat([a, b], dim=0)\r\nvstack : torch.cat([a, b], dim=1)\r\n\r\nc = torch.hstack([a, b])\r\nprint(c)\r\ntensor([[1., 1., 1., 0., 0., 0.],\r\n        [1., 1., 1., 0., 0., 0.]])\r\nprint(c.shape)\r\ntorch.Size([2, 6])\r\nd = torch.vstack([a, b])\r\nprint(d)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.],\r\n        [0., 0., 0.],\r\n        [0., 0., 0.]])\r\nprint(d.shape)\r\ntorch.Size([4, 3])\r\n\r\ntorch.unsqueeze() : 더미차원 삭제\r\ntorch.squeeze() : 더미차원 추가\r\n\r\n# unsqueeze and squeeze\r\nprint(a)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.]])\r\na = torch.unsqueeze(a, dim=1)\r\nprint(a)\r\ntensor([[[1., 1., 1.]],\r\n\r\n        [[1., 1., 1.]]])\r\nprint(a.shape)\r\ntorch.Size([2, 1, 3])\r\nprint(torch.squeeze(a))\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.]])\r\nprint(a.shape)\r\ntorch.Size([2, 1, 3])\r\n\r\ntonsor의 연산\r\n행렬 곱\r\n\r\na = torch.tensor(np.array(list(range(12)))).reshape(3, 4)\r\nprint(a)\r\ntensor([[ 0,  1,  2,  3],\r\n        [ 4,  5,  6,  7],\r\n        [ 8,  9, 10, 11]], dtype=torch.int32)\r\nb = torch.tensor(np.array(list(range(8)))).reshape(4, 2)\r\nprint(b)\r\ntensor([[0, 1],\r\n        [2, 3],\r\n        [4, 5],\r\n        [6, 7]], dtype=torch.int32)\r\nc = a @ b\r\nprint(c)\r\ntensor([[ 28,  34],\r\n        [ 76,  98],\r\n        [124, 162]], dtype=torch.int32)\r\nd = torch.matmul(a, b)\r\nprint(d)\r\ntensor([[ 28,  34],\r\n        [ 76,  98],\r\n        [124, 162]], dtype=torch.int32)\r\n\r\nelement-wise product\r\n\r\na = torch.tensor(np.array(list(range(6)))).reshape(2, 3)\r\nprint(a)\r\ntensor([[0, 1, 2],\r\n        [3, 4, 5]], dtype=torch.int32)\r\nb = torch.tensor(list(range(10, 13))).reshape(1, 3)\r\nprint(b)\r\ntensor([[10, 11, 12]])\r\nc = a * b\r\nprint(c)\r\ntensor([[ 0, 11, 24],\r\n        [30, 44, 60]])\r\nd = a.mul(b)\r\nprint(d)\r\ntensor([[ 0, 11, 24],\r\n        [30, 44, 60]])\r\n\r\nitem() : tensor의 원소를 반환합니다. tensor 안에 원소가 1개만 있어야\r\n합니다.\r\n\r\n# item\r\nagg = d.sum()\r\nv = agg.item()\r\nprint(v, type(v))\r\n169 <class 'int'>\r\n\r\ninplace operations\r\n\r\nprint(a)\r\ntensor([[0, 1, 2],\r\n        [3, 4, 5]], dtype=torch.int32)\r\na.add_(5)\r\ntensor([[ 5,  6,  7],\r\n        [ 8,  9, 10]], dtype=torch.int32)\r\nprint(a)\r\ntensor([[ 5,  6,  7],\r\n        [ 8,  9, 10]], dtype=torch.int32)\r\na.add_(b)\r\ntensor([[15, 17, 19],\r\n        [18, 20, 22]], dtype=torch.int32)\r\nprint(a)\r\ntensor([[15, 17, 19],\r\n        [18, 20, 22]], dtype=torch.int32)\r\n\r\nDataset\r\ndataset은 전체 데이터를 sample 단위로 처리해주는 역할을 합니다.\r\n\r\nclass LionDataset(Dataset):\r\n    def __init__(self, data, target, transform=None, target_transform=None):\r\n        self.data = data\r\n        self.target = target\r\n        self.transform = transform\r\n        self.target_transform = target_transform\r\n        pass\r\n    \r\n    def __len__(self):\r\n        return len(self.data)\r\n    def __getitem__(self, idx):\r\n        x = self.data[idx]\r\n        y = self.target[idx]\r\n        \r\n        if self.transform:\r\n          x = self.transform(x)\r\n        if self.target_transform:\r\n          y = self.target_transform(y)\r\n        \r\n        return x, y\r\n\r\n\r\ndata = np.array(list(range(100))).reshape(-1, 2)\r\ntarget = np.array([[i] * 5 for i in range(10)]).reshape(-1)\r\n\r\nprint(data)\r\n[[ 0  1]\r\n [ 2  3]\r\n [ 4  5]\r\n [ 6  7]\r\n [ 8  9]\r\n [10 11]\r\n [12 13]\r\n [14 15]\r\n [16 17]\r\n [18 19]\r\n [20 21]\r\n [22 23]\r\n [24 25]\r\n [26 27]\r\n [28 29]\r\n [30 31]\r\n [32 33]\r\n [34 35]\r\n [36 37]\r\n [38 39]\r\n [40 41]\r\n [42 43]\r\n [44 45]\r\n [46 47]\r\n [48 49]\r\n [50 51]\r\n [52 53]\r\n [54 55]\r\n [56 57]\r\n [58 59]\r\n [60 61]\r\n [62 63]\r\n [64 65]\r\n [66 67]\r\n [68 69]\r\n [70 71]\r\n [72 73]\r\n [74 75]\r\n [76 77]\r\n [78 79]\r\n [80 81]\r\n [82 83]\r\n [84 85]\r\n [86 87]\r\n [88 89]\r\n [90 91]\r\n [92 93]\r\n [94 95]\r\n [96 97]\r\n [98 99]]\r\nprint(target)\r\n[0 0 0 0 0 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 4 4 4 4 4 5 5 5 5 5 6 6 6 6 6 7 7\r\n 7 7 7 8 8 8 8 8 9 9 9 9 9]\r\n\r\n\r\nlion = LionDataset(data=data, target=target)\r\nprint(lion[0:4])\r\n(array([[0, 1],\r\n       [2, 3],\r\n       [4, 5],\r\n       [6, 7]]), array([0, 0, 0, 0]))\r\nprint(len(lion))\r\n50\r\n\r\nDataLoader\r\ndataset을 batch 단위로 묶어주는 역할을 합니다.\r\n\r\nloader = DataLoader(dataset=lion, batch_size=10, shuffle=True)\r\n\r\nfor i, batch in enumerate(loader):\r\n    x, y = batch\r\n    if i == 0:\r\n        print(x)\r\n        print(y)\r\n    print(x.shape)\r\ntensor([[86, 87],\r\n        [12, 13],\r\n        [72, 73],\r\n        [68, 69],\r\n        [60, 61],\r\n        [32, 33],\r\n        [80, 81],\r\n        [90, 91],\r\n        [46, 47],\r\n        [20, 21]], dtype=torch.int32)\r\ntensor([8, 1, 7, 6, 6, 3, 8, 9, 4, 2], dtype=torch.int32)\r\ntorch.Size([10, 2])\r\ntorch.Size([10, 2])\r\ntorch.Size([10, 2])\r\ntorch.Size([10, 2])\r\ntorch.Size([10, 2])\r\n\r\ntorch.nn.Module\r\npytorch 모델은 parameters를 추적하며 forward pass를 진행한 뒤 back\r\npropagation을 통해 학습을 진행합니다. torch.nn.Module은 여러 층의\r\nlayer로 이뤄진 모델을 쉽게 관리할 수 있는 class입니다.\r\n\r\nclass LionLinear(nn.Module):\r\n    def __init__(self, input_dim, output_dim):\r\n        super().__init__()\r\n        self.input_dim = input_dim\r\n        self.output_dim = output_dim\r\n        \r\n        self.flatten = nn.Flatten() # 1차원 tensor로 평탄화해준다.\r\n        self.classifier = nn.Linear(input_dim, output_dim)\r\n        self.act = nn.ReLU()\r\n        \r\n    def forward(self, x):\r\n        x = self.flatten(x) \r\n        x = self.classifier(x)\r\n        x = self.act(x)\r\n        \r\n        return x\r\n\r\n\r\nlinear_model = LionLinear(28*28, 10).to(device)\r\nprint(linear_model)\r\nLionLinear(\r\n  (flatten): Flatten(start_dim=1, end_dim=-1)\r\n  (classifier): Linear(in_features=784, out_features=10, bias=True)\r\n  (act): ReLU()\r\n)\r\nprint(linear_model.classifier)\r\nLinear(in_features=784, out_features=10, bias=True)\r\n\r\n이처럼 nn.Module를 상속받아 forward()통해 모델을 정의할 수\r\n있습니다.\r\n다음은 MLP를 구현하는 module입니다.\r\n\r\nclass LionLayer(nn.Module):\r\n    def __init__(self, input_dim, output_dim):\r\n        super().__init__()\r\n        self.input_dim = input_dim\r\n        self.output_dim = output_dim\r\n        self.layer = nn.Linear(self.input_dim, self.output_dim)\r\n        pass\r\n    def forward(self, x):\r\n        assert x.shape[-1] == self.input_dim, \"Input dimension mismatch\"\r\n        return self.layer(x)\r\n\r\n\r\nclass LionMLP(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim):\r\n        super().__init__()\r\n        self.input_dim = input_dim\r\n        self.hidden_dim = hidden_dim\r\n        self.output_dim = output_dim\r\n        self.flatten = nn.Flatten()\r\n        self.linear_1 = LionLayer(self.input_dim, self.hidden_dim)\r\n        self.linear_2 = LionLayer(self.hidden_dim, self.output_dim)\r\n        self.act_1 = nn.ReLU()\r\n        self.act_2 = nn.Softmax()\r\n        pass\r\n    \r\n    def forward(self, x):\r\n        x = self.flatten(x)\r\n        x = self.act_1(self.linear_1(x))\r\n        x = self.act_2(self.linear_2(x))\r\n        return x\r\n\r\n\r\nmlp = LionMLP(28*28, 50, 10)\r\nprint(mlp)\r\nLionMLP(\r\n  (flatten): Flatten(start_dim=1, end_dim=-1)\r\n  (linear_1): LionLayer(\r\n    (layer): Linear(in_features=784, out_features=50, bias=True)\r\n  )\r\n  (linear_2): LionLayer(\r\n    (layer): Linear(in_features=50, out_features=10, bias=True)\r\n  )\r\n  (act_1): ReLU()\r\n  (act_2): Softmax(dim=None)\r\n)\r\n\r\nnn.Sequential()을 통해 더 간결하게 표현할 수 있습니다.\r\n\r\nclass LionMLP(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim):\r\n        super().__init__()\r\n        self.input_dim = input_dim\r\n        self.hidden_dim = hidden_dim\r\n        self.output_dim = output_dim\r\n        self.flatten = nn.Flatten()\r\n        self.linear_1 = LionLayer(self.input_dim, self.hidden_dim)\r\n        self.linear_2 = LionLayer(self.hidden_dim, self.output_dim)\r\n        self.act_1 = nn.ReLU()\r\n        self.act_2 = nn.Softmax()\r\n        self.model = nn.Sequential(self.flatten, self.linear_1, self.act_1, self.linear_2, self.act_2)\r\n        pass\r\n    def forward(self, x):\r\n        return self.model(x)\r\n\r\n\r\nmlp = LionMLP(28 * 28, 40, 10)\r\nprint(mlp)\r\nLionMLP(\r\n  (flatten): Flatten(start_dim=1, end_dim=-1)\r\n  (linear_1): LionLayer(\r\n    (layer): Linear(in_features=784, out_features=40, bias=True)\r\n  )\r\n  (linear_2): LionLayer(\r\n    (layer): Linear(in_features=40, out_features=10, bias=True)\r\n  )\r\n  (act_1): ReLU()\r\n  (act_2): Softmax(dim=None)\r\n  (model): Sequential(\r\n    (0): Flatten(start_dim=1, end_dim=-1)\r\n    (1): LionLayer(\r\n      (layer): Linear(in_features=784, out_features=40, bias=True)\r\n    )\r\n    (2): ReLU()\r\n    (3): LionLayer(\r\n      (layer): Linear(in_features=40, out_features=10, bias=True)\r\n    )\r\n    (4): Softmax(dim=None)\r\n  )\r\n)\r\n\r\n\r\nfor name, param in mlp.named_parameters():\r\n    print(f\"Layer: {name} | Size: {param.size()} | Values: {param[:2]} \\n\")\r\nLayer: linear_1.layer.weight | Size: torch.Size([40, 784]) | Values: tensor([[-0.0320, -0.0161,  0.0018,  ...,  0.0278,  0.0135,  0.0355],\r\n        [-0.0096,  0.0356, -0.0273,  ..., -0.0196,  0.0305, -0.0003]],\r\n       grad_fn=<SliceBackward0>) \r\n\r\nLayer: linear_1.layer.bias | Size: torch.Size([40]) | Values: tensor([0.0305, 0.0006], grad_fn=<SliceBackward0>) \r\n\r\nLayer: linear_2.layer.weight | Size: torch.Size([10, 40]) | Values: tensor([[ 0.1549,  0.1293, -0.1401, -0.0738,  0.0004,  0.0726,  0.0289,  0.0976,\r\n         -0.0857, -0.1468, -0.0863, -0.0898,  0.0983, -0.0270, -0.0402,  0.0588,\r\n         -0.0031, -0.1369,  0.1563,  0.0316, -0.0642,  0.0575,  0.0104,  0.1135,\r\n          0.1001, -0.0564,  0.1311,  0.0601,  0.1249,  0.1569, -0.1053,  0.0030,\r\n         -0.0938,  0.1343, -0.1414, -0.0034,  0.1165, -0.1074, -0.0518, -0.1441],\r\n        [ 0.0381, -0.1287,  0.1479,  0.1138, -0.1021,  0.1123, -0.0741,  0.0752,\r\n         -0.1153, -0.0909,  0.1182, -0.1152, -0.1060,  0.1236,  0.0429, -0.1258,\r\n         -0.0227, -0.1014,  0.1515, -0.0474, -0.0308,  0.0486,  0.0399, -0.0094,\r\n          0.1450,  0.0633, -0.0504,  0.1421,  0.0489,  0.0882, -0.1149, -0.1266,\r\n         -0.1398,  0.0022,  0.0165,  0.0197, -0.0585, -0.0427, -0.1185, -0.0144]],\r\n       grad_fn=<SliceBackward0>) \r\n\r\nLayer: linear_2.layer.bias | Size: torch.Size([10]) | Values: tensor([-0.1406, -0.0314], grad_fn=<SliceBackward0>) \r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-21T11:49:59+09:00",
    "input_file": "pytorch-tutorial.knit.md"
  },
  {
    "path": "til/2022-05-19-cotepart1/",
    "title": "[백준 문제풀이] 기본기",
    "description": "코딩테스트를 위한 백준 문제 풀어보기",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-19",
    "categories": [
      "coding test"
    ],
    "contents": "\r\n\r\nContents\r\n약수 구하기\r\n이진수\r\nbin() 직접 구현하기(양의\r\n정수일때만)\r\n\r\n최대, 최소\r\n\r\n약수 구하기\r\n두 개의 자연수 N과 K가 주어졌을 때, N의 약수들 중 K번째로 작은 수를\r\n출력하는 프로그램을 작성하시오.\r\n입력\r\n첫째 줄에 N과 K가 빈칸을 사이에 두고 주어진다. N은 1 이상 10,000\r\n이하이다. K는 1 이상 N 이하이다.\r\n출력\r\n첫째 줄에 N의 약수들 중 K번째로 작은 수를 출력한다. 만일 N의 약수의\r\n개수가 K개보다 적어서 K번째 약수가 존재하지 않을 경우에는 0을\r\n출력하시오.\r\n\r\nN, K = map(int, input().split())\r\n\r\nresult = 0\r\n\r\nfor i in range(1, N + 1):\r\n    if N % i == 0:\r\n        K -= 1\r\n        if K == 0:\r\n            result = i\r\n            break\r\n\r\nprint(result)\r\n\r\n이진수\r\n양의 정수 n이 주어졌을 때, 이를 이진수로 나타냈을 때 1의 위치를 모두\r\n찾는 프로그램을 작성하시오. 최하위 비트(least significant bit, lsb)의\r\n위치는 0이다.\r\n입력\r\n첫째 줄에 테스트 케이스의 개수 T가 주어진다. 각 테스트 케이스는 한\r\n줄로 이루어져 있고, n이 주어진다. \\((1 ≤ T ≤\r\n10, 1 ≤ n ≤ 10^6)\\)\r\n출력\r\n각 테스트 케이스에 대해서, 1의 위치를 공백으로 구분해서 줄 하나에\r\n출력한다. 위치가 낮은 것부터 출력한다.\r\n\r\nt = int(input())\r\n\r\nfor _ in range(t):\r\n    n = int(input())\r\n    b = bin(n)[2:]\r\n    \r\n    for i in range(len(b)):\r\n        if b[::-1][i] == '1':\r\n            print(i, end=' ')\r\n\r\nbin() 직접 구현하기(양의\r\n정수일때만)\r\n\r\ndef my_bin(num):\r\n    a = num//2\r\n    b = num%2\r\n    if a == 0:\r\n        return '1'\r\n    else:\r\n        return my_bin(a)+str(b)\r\n\r\n최대, 최소\r\nN개의 정수가 주어진다. 이때, 최솟값과 최댓값을 구하는 프로그램을\r\n작성하시오.\r\n입력\r\n첫째 줄에 정수의 개수 N (1 ≤ N ≤ 1,000,000)이 주어진다. 둘째 줄에는\r\nN개의 정수를 공백으로 구분해서 주어진다. 모든 정수는 -1,000,000보다\r\n크거나 같고, 1,000,000보다 작거나 같은 정수이다.\r\n출력\r\n첫째 줄에 주어진 정수 N개의 최솟값과 최댓값을 공백으로 구분해\r\n출력한다.\r\n\r\ninput()\r\nl = list(map(int, input().split()))\r\n\r\nhigh = low = l[0]\r\n\r\nfor i in l[1:]:\r\n    if high < i:\r\n        high = i\r\n    if low > i:\r\n        low = i\r\n        \r\nprint(low, high)\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-20T19:04:09+09:00",
    "input_file": {}
  },
  {
    "path": "til/2022-05-18-dlfromscratchch2/",
    "title": "퍼셉트론이란",
    "description": "밑바닥부터 시작하는 딥러닝을 읽고 딥러닝의 기원이 되는 알고리즘인 퍼셉트론에 대해 정리해봤습니다.",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-18",
    "categories": [
      "밑바닥부터 시작하는 딥러닝"
    ],
    "contents": "\r\n\r\nContents\r\n1️⃣ 퍼셉트론이란\r\n2️⃣ 퍼셉트론의 한계\r\n3️⃣ 다층 퍼셉트론\r\n✅ 요약\r\n\r\n1️⃣ 퍼셉트론이란\r\n퍼셉트론은 다수의 신호를 입력(input)으로 받아 하나의 신호를\r\n출력(output)하는 알고리즘입니다.\r\n2개의 입력을 받아 하나의 신호를 출력하는 퍼셉트론을 수식으로\r\n나타내면,\r\n\\[y = \\begin{cases}0,(w_{1}x_{1} +\r\nw_{2}x_{2}\\leq\\theta)\\\\1,(w_{1}x_{1} +\r\nw_{2}x_{2}>\\theta)\\\\\\end{cases}\\] 또는 \\(\\theta\\)를 \\(-b\\)로 치환하여,\r\n\\[y = \\begin{cases}0,(b+w_{1}x_{1} +\r\nw_{2}x_{2}\\leq0)\\\\1,(b+w_{1}x_{1} +\r\nw_{2}x_{2}>0)\\\\\\end{cases}\\]\r\n\\(w\\) : weight, \\(b\\): bias\r\n2️⃣ 퍼셉트론의 한계\r\n퍼셉트론이 풀 수 있는 (예측할 수 있는) 문제는 AND, NAND,\r\nOR입니다.\r\nAND\r\n\r\nx1x2y000100010111\r\n\r\nNAND\r\n\r\nx1x2y001101011110\r\n\r\nOR\r\n\r\nx1x2y000101011111\r\n\r\n위 세가지 문제는 \\((w_1, w_2,\r\ntheta)\\)에 각각 \\((0.5, 0.5, 0.7),\r\n(-0.5, -0.5, -0.7), (1.5, 1.5, 0.7)\\) 등을 대입하면 풀 수\r\n있습니다.\r\n이를 파이썬 함수로 구현하면,\r\n\r\ndef AND(x1, x2):\r\n    w1, w2, theta = 0.5, 0.5, 0.5\r\n    tmp = x1*w1+x2*w2\r\n    if tmp <= theta:\r\n        return 0\r\n    else:\r\n        return 1\r\n\r\ndef NAND(x1, x2):\r\n    w1, w2, theta = -0.5, -0.5, -0.7\r\n    tmp = x1*w1+x2*w2\r\n    if tmp <= theta:\r\n        return 0\r\n    else:\r\n        return 1\r\n    \r\ndef OR(x1, x2):\r\n    w1, w2, theta = 1.5, 1.5, 0.7\r\n    tmp = x1*w1+x2*w2\r\n    if tmp <= theta:\r\n        return 0\r\n    else:\r\n        return 1\r\n\r\nprint('x1=0, x2=0 : ', AND(0,0))\r\nx1=0, x2=0 :  0\r\nprint('x1=0, x2=1 : ', AND(0,1))\r\nx1=0, x2=1 :  0\r\nprint('x1=1, x2=0 : ', AND(1,0))\r\nx1=1, x2=0 :  0\r\nprint('x1=1, x2=1 : ', AND(1,1))\r\nx1=1, x2=1 :  1\r\nprint('x1=0, x2=0 : ', NAND(0,0))\r\nx1=0, x2=0 :  1\r\nprint('x1=0, x2=1 : ', NAND(0,1))\r\nx1=0, x2=1 :  1\r\nprint('x1=1, x2=0 : ', NAND(1,0))\r\nx1=1, x2=0 :  1\r\nprint('x1=1, x2=1 : ', NAND(1,1))\r\nx1=1, x2=1 :  0\r\nprint('x1=0, x2=0 : ', OR(0,0))\r\nx1=0, x2=0 :  0\r\nprint('x1=0, x2=1 : ', OR(0,1))\r\nx1=0, x2=1 :  1\r\nprint('x1=1, x2=0 : ', OR(1,0))\r\nx1=1, x2=0 :  1\r\nprint('x1=1, x2=1 : ', OR(1,1))\r\nx1=1, x2=1 :  1\r\n\r\n하지만, 퍼셉트론으로 XOR은 해결할 수 없습니다.\r\nXOR\r\n\r\nx1x2y000101011110\r\n\r\n위 문제를 그림으로 나타내면 다음과 같이 빨간점과 검은점을 구분할 수\r\n있는 선을 그을 수 있어야 합니다.\r\n\r\n\r\n\r\n곡선으로 선을 그린다면 구분할 수 있겠지만 퍼셉트론으로는 직선밖에\r\n그릴 수 없으므로 해결할 수 없습니다. 즉, 퍼셉트론은 비선형의 영역을\r\n표현할 수 없습니다.\r\n3️⃣ 다층 퍼셉트론\r\n이를 해결하기 위해 나온 것이 다층 퍼셉트론입니다. 말 그대로 퍼셉트론\r\n층을 여러 개 갖는 알고리즘이며 앞에서 언급했던 퍼셉트론은 정확히 말하면\r\n단층 퍼셉트론입니다.\r\nXOR문제를 해결하려면 위에서 AND, NAND, OR 문제를 해결하기 위해\r\n만들었던 퍼셉트론을 적절히 쌓아 만들면 됩니다.\r\n\r\ndef XOR(x1, x2):\r\n    s1 = NAND(x1, x2)\r\n    s2 = OR(x1, x2)\r\n    y = AND(s1, s2)\r\n    return y\r\n\r\nprint('x1=0, x2=0 : ', XOR(0,0))\r\nx1=0, x2=0 :  0\r\nprint('x1=0, x2=1 : ', XOR(0,1))\r\nx1=0, x2=1 :  1\r\nprint('x1=1, x2=0 : ', XOR(1,0))\r\nx1=1, x2=0 :  1\r\nprint('x1=1, x2=1 : ', XOR(1,1))\r\nx1=1, x2=1 :  0\r\n\r\n첫번째 층에 NAND, OR를 풀기 위해 만들었던 퍼셉트론을 쌓고 두번째 층에\r\nAND를 풀기 위해 만들었던 퍼셉트론을 쌓아 XOR문제를 해결했습니다.\r\n이처럼 다층 퍼셉트론은 비선형의 영역까지도 표현할 수 있음을 알 수\r\n있습니다.\r\n✅ 요약\r\n퍼셉트론은 입출력을 갖춘 알고리즘\r\n단층 퍼셉트론은 선형 영역만 표현할 수 있고, 다층 퍼셉트론은\r\n비선형 영역도 표현 가능\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-19T00:59:30+09:00",
    "input_file": {}
  },
  {
    "path": "til/2022-05-13-first-til/",
    "title": "TIL(Today I Learn) 페이지 개설!!",
    "description": "매일 공부하는 것을 기록하기",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-13",
    "categories": [],
    "contents": "\r\n매일매일 공부한 것을 기록하는 페이지를 만들었다. 일기처럼 나중에 봐도\r\n뿌듯한 페이지가 되길..\r\n😄화이팅!!\r\n\r\n\r\n\r\n",
    "preview": "til/2022-05-13-first-til/images/owl.png",
    "last_modified": "2022-05-16T00:44:05+09:00",
    "input_file": {},
    "preview_width": 640,
    "preview_height": 514
  }
]

[
  {
    "path": "til/2022-05-23-dlfromscratchch3/",
    "title": "신경망과 활성화 함수",
    "description": "밑바닥부터 시작하는 딥러닝을 읽고 신경망과 활성화 함수에 대해 정리해봤습니다.",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-23",
    "categories": [
      "밑바닥부터 시작하는 딥러닝"
    ],
    "contents": "\r\n\r\nContents\r\n1️⃣신경망이란\r\n2️⃣활성화 함수의 종류\r\n계단함수\r\n시그모이드 함수\r\nReLU 함수\r\n항등 함수\r\n소프트맥스 함수\r\n\r\n3️⃣비선형 함수\r\n4️⃣numpy로 신경망 구현\r\n5️⃣소프트맥스 함수의 특징\r\n6️⃣분류 신경망 구현\r\n7️⃣배치 처리\r\n\r\n1️⃣신경망이란\r\n단층 퍼셉트론은 다음과 같은 수식으로 표현됩니다.\r\n\\[y = \\begin{cases}0,(b+w_{1}x_{1} +\r\nw_{2}x_{2}\\leq0)\\\\1,(b+w_{1}x_{1} +\r\nw_{2}x_{2}>0)\\\\\\end{cases}\\]\r\n\\(z = b+w_{1}x_{1} + w_{2}x_{2}\\)로\r\n나타낸다면 다음과 같이 나타낼 수 있습니다.\r\n\\[y =\r\n\\begin{cases}0,z\\leq0)\\\\1,z>0\\\\\\end{cases}\\]\r\n즉, 입력신호의 가중합이 0을 넘으면 1을 반환하고 그렇지 않다면 0을\r\n반환합니다. 이처럼 입력신호들의 값을 출력신호로 변환하는 함수를 활성화\r\n함수라고 하며 단층 퍼셉트론에서 사용한 함수를 계단 함수라고 합니다.\r\n단층 퍼셉트론은 활성화 함수로 계단 함수 사용하지만 신경망은 다른\r\n활성화 함수를 사용할 수 있고 다층 퍼셉트론처럼 여러 층을 쌓아 만든\r\n알고리즘이라 할 수 있습니다\r\n신경망의 예위 그림처럼 신경망은 크게 출력층(input layer), 은닉층(hidden layer),\r\n출력층(output layer)로 이뤄져 있으며 각 원을 노드(node)라고\r\n부릅니다.\r\n2️⃣활성화 함수의 종류\r\n계단함수\r\n퍼셉트론에서 사용하는 함수. 출력는 0 또는 1이다.\r\n\\[h(z) =\r\n\\begin{cases}0,z\\leq0)\\\\1,z>0\\\\\\end{cases}\\]\r\n시그모이드 함수\r\n출력값의 범위가 0과 1사이에 존재한다.\r\n\\[h(z) = 1/(1+exp(-z))\\]\r\nReLU 함수\r\n\\[h(z) =\r\n\\begin{cases}0,z\\leq0)\\\\z,z>0\\\\\\end{cases}\\]\r\n항등 함수\r\n\\[ h(z) = z\\]\r\n소프트맥스 함수\r\n\\(h(z_k)\\)는 k번째 출력값을 의미\r\n\\[h(z_k) = exp(z_k)/\\sum\r\nexp(z_i)\\]\r\n3️⃣비선형 함수\r\n신경망에서 층을 쌓을 때 활성화 함수는 비선형 함수이어야 합니다. 만약\r\n활성화 함수로 다음과 같은 함수를 사용한다면,\r\n\\[ h(z) = cz\\]\r\n층을 아무리 쌓아도 다음과 같은 형태로 단순화 시킬 수 있고 결국 층을\r\n안쌓은 것과 동일한 효과를 갖게 됩니다.\r\n\\[ h(h(h(z))) = c^3z\\]\r\n그러므로 층을 쌓는 혜택을 얻기 위해서는 활성화 함수로 비선형 함수를\r\n사용해야 합니다.\r\n4️⃣numpy로 신경망 구현\r\n이 글 처음에 등장한 그림은 3개의 입력을 받아 4개의 노드가 존재하는\r\n은닉층 하나를 거쳐 2개를 출력하는 신경망을 도식화한 것입니다. numpy를\r\n활용해 이를 구현해보겠습니다.\r\n각 노드의 가중치는 임의로 넣고 은닉층의 활성화 함수는 시그모이드,\r\n출력층의 활성화 함수는 항등함수를 사용했습니다.\r\n\r\nimport numpy as np\r\n\r\ndef init_network():\r\n    network = {}\r\n    network[\"W1\"] = np.array([[0.1,0.3,0.1,0.5],[0.2,0.4,0.3,0.4],[0.1,0.2,0.3,0.5]]) # 은닉층\r\n    network[\"W2\"] = np.array([[0.2,0.4],[0.1,0.2],[0.1,0.3],[0.5,0.5]]) # 출력층\r\n    \r\n    return network\r\n\r\ndef sigmoid(x):\r\n    return 1/(1+np.exp(-x))\r\n\r\ndef identity_function(x):\r\n    return x\r\n\r\ndef forward(network, x):\r\n    W1, W2 = network[\"W1\"], network[\"W2\"]\r\n    x = np.dot(x,W1)\r\n    x = sigmoid(x)\r\n    x = np.dot(x,W2)\r\n    x = identity_function(x)\r\n    \r\n    return x\r\n\r\n\r\nnetwork = init_network()\r\nforward(network, np.array([0.1, 0.5, 0.7]))\r\narray([0.55009573, 0.8365092 ])\r\n\r\n5️⃣소프트맥스 함수의 특징\r\n위 신경망에서 출력층은 항등함수로 구현된 것을 볼 수 있습니다.\r\n일반적으로 회귀에는 항등함수를, 분류에는 소프트맥스 함수를 사용합니다.\r\n그 이유는 소프트맥스의 출력값들의 합은 1이어서 각 출력값들을 분류확률로\r\n해석할 수 있기 때문입니다. 그러므로 소프트맥스의 출력값으로 cost를\r\n계산할 수 있게 됩니다. 하지만 학습된 모델을 적용할 때는 출력층에\r\n소프트맥스를 사용할 필요가 없습니다. 소프트맥스 함수는 단조증가 함수이고\r\ncost를 계산할 필요가 없기 때문에 가장 큰 값을 그대로 분류하면 됩니다.\r\n지수함수 계산에 드는 비용도 줄일 수 있습니다.\r\n소프트맥스를 코드로 구현 시 오버플로 문제가 존재합니다. 이 문제를\r\n해결하기 위해 다음과 같은 수식으로 소프트맥스 함수를 개선할 수\r\n있습니다.\r\n\\[h(z_k) = exp(z_k)/\\sum exp(z_i) \\\\ =\r\nCexp(z_k)/C\\sum exp(z_i) \\\\ = exp(z_k+log(C))/\\sum exp(z_i+log(C)), \\\\ C\r\n= -max(z_i)\\] 즉, 소프트맥스의 입력값에 상수를 더해도 그 값의\r\n차이가 없으므로 입력값들 중 최대값을 각 입력값에 빼주면 오버플로 문제를\r\n해결할 수 있습니다.\r\n6️⃣분류 신경망 구현\r\niris 데이터를 활용하여 Species를 분류하는 모델을 만들어\r\n보겠습니다.\r\ndata load\r\n\r\n\r\nlibrary(reticulate)\r\npy$iris <- iris\r\n\r\n\r\n\r\n\r\nimport pandas as pd\r\niris = pd.DataFrame(iris)\r\n\r\n\r\niris\r\n     Sepal.Length  Sepal.Width  Petal.Length  Petal.Width    Species\r\n0             5.1          3.5           1.4          0.2     setosa\r\n1             4.9          3.0           1.4          0.2     setosa\r\n2             4.7          3.2           1.3          0.2     setosa\r\n3             4.6          3.1           1.5          0.2     setosa\r\n4             5.0          3.6           1.4          0.2     setosa\r\n..            ...          ...           ...          ...        ...\r\n145           6.7          3.0           5.2          2.3  virginica\r\n146           6.3          2.5           5.0          1.9  virginica\r\n147           6.5          3.0           5.2          2.0  virginica\r\n148           6.2          3.4           5.4          2.3  virginica\r\n149           5.9          3.0           5.1          1.8  virginica\r\n\r\n[150 rows x 5 columns]\r\n\r\none-hot_encoding\r\n\r\nx_data = iris.drop([\"Species\"], axis=1)\r\nx_data = x_data.values\r\ny_data = pd.get_dummies(iris['Species'])\r\ny_data\r\n     setosa  versicolor  virginica\r\n0         1           0          0\r\n1         1           0          0\r\n2         1           0          0\r\n3         1           0          0\r\n4         1           0          0\r\n..      ...         ...        ...\r\n145       0           0          1\r\n146       0           0          1\r\n147       0           0          1\r\n148       0           0          1\r\n149       0           0          1\r\n\r\n[150 rows x 3 columns]\r\ny_data = y_data.values\r\n\r\nmodeling\r\n임의로 가중치를 부여하여 모델을 만듭니다. 이전 모델과 큰 차이는\r\n없지만 입력 노드와 출력 노드가 하나씩 더 들어가있고 출력층의 활성화\r\n함수가 소프트맥스 함수로 바뀌었습니다.\r\n\r\ndef init_network():\r\n    network = {}\r\n    network[\"W1\"] = np.array([[0.1,0.3,0.1,0.5],[0.2,0.4,0.3,0.4],[0.5,0.2,0.3,0.5],[0.7,0.5,0.2,0.5]]) # 은닉층\r\n    network[\"W2\"] = np.array([[0.5,0.9,0.1],[0.1,0.2,0.7],[0.2,0.5,0.5],[0.5,0.5,0.5]]) # 출력층\r\n    \r\n    return network\r\n\r\ndef sigmoid(x):\r\n    return 1/(1+np.exp(-x))\r\n\r\ndef softmax(x):\r\n    c = np.max(x)\r\n    exp_x = np.exp(x-c)\r\n    sum_exp_x = np.sum(exp_x)\r\n    \r\n    return exp_x/sum_exp_x\r\n\r\ndef forward(network, x):\r\n    W1, W2 = network[\"W1\"], network[\"W2\"]\r\n    x = np.dot(x,W1)\r\n    x = sigmoid(x)\r\n    x = np.dot(x,W2)\r\n    x = softmax(x)\r\n    \r\n    return x\r\n\r\n첫번째 입력값만 넣어 출력을 확인해보면 2번째 값의 분류확률이 가장\r\n높은 것을 확인할 수 있습니다.\r\n\r\nnetwork = init_network()\r\nforward(network, x_data[0])\r\narray([0.21381633, 0.43739548, 0.34878819])\r\n\r\nAccuacy를 확인해보면 33%인 것을 확인할 수 있습니다. 아직 학습과정을\r\n통해 가중치를 최적화 하지 않았기 때문입니다.\r\n\r\nnetwork = init_network()\r\n\r\ncount = 0\r\nfor i in range(len(x_data)):\r\n    pred_p = forward(network, x_data[i])\r\n    index_max = np.argmax(pred_p)\r\n    \r\n    if index_max == np.argmax(y_data[i]):\r\n        count += 1\r\n\r\nprint(\"Accuacy: \", count/len(x_data))\r\nAccuacy:  0.3333333333333333\r\n\r\n7️⃣배치 처리\r\n아까 구현했던 모델에서 입력값의 shape, 각 가중치의 shape, 출력값의\r\nshape을 확인해보겠습니다.\r\n\r\ndef forward(network, x):\r\n    W1, W2 = network[\"W1\"], network[\"W2\"]\r\n    print(\"input : \", x.shape)\r\n    print(\"W1 : \", W1.shape)\r\n    print(\"W2 : \", W2.shape)\r\n    x = np.dot(x,W1)\r\n    x = sigmoid(x)\r\n    x = np.dot(x,W2)\r\n    x = softmax(x)\r\n    print(\"output : \", x.shape)\r\n    return x\r\n\r\nforward(network, x_data[0])\r\ninput :  (4,)\r\nW1 :  (4, 4)\r\nW2 :  (4, 3)\r\noutput :  (3,)\r\narray([0.21381633, 0.43739548, 0.34878819])\r\n\r\n행렬의 곱 원리에 따라 (1x4)(4x4)(4x3)=(1x3)의 array가 출력되는 것을\r\n알 수 있습니다. 여기서 만약 input을 (3x4)로 주게 된다면 output은\r\n(3x3)array로 출력될 것입니다.\r\n이처럼 입력데이터를 묶어서 계산할 수 있으며 입력 데이터 묶음을\r\n배치라고 하고 배치 안에 데이터의 수를 배치 크기(batch size)라고 합니다.\r\n그럼 배치 처리를 구현해보겠습니다.\r\n이전 코드와 다른 점은 소프트맥스 함수가 배치처리를 할 수 있도록\r\n수정했고 예측을 수행할 때도 배치 별로 예측할 수 있도록 했습니다.\r\n\r\ndef init_network():\r\n    network = {}\r\n    network[\"W1\"] = np.array([[0.1,0.3,0.1,0.5],[0.2,0.4,0.3,0.4],[0.5,0.2,0.3,0.5],[0.7,0.5,0.2,0.5]]) # 은닉층\r\n    network[\"W2\"] = np.array([[0.5,0.9,0.1],[0.1,0.2,0.7],[0.2,0.5,0.5],[0.5,0.5,0.5]]) # 출력층\r\n    \r\n    return network\r\n\r\ndef sigmoid(x):\r\n    return 1/(1+np.exp(-x))\r\n\r\ndef softmax(x):\r\n    c = np.max(x)\r\n    exp_x = np.exp(x-c)\r\n    \r\n    return np.multiply(exp_x.T, 1/np.sum(exp_x, axis=1)).T\r\n\r\ndef forward(network, x):\r\n    W1, W2 = network[\"W1\"], network[\"W2\"]\r\n    x = np.dot(x,W1)\r\n    x = sigmoid(x)\r\n    x = np.dot(x,W2)\r\n    x = softmax(x)\r\n    \r\n    return x\r\n\r\nnetwork = init_network()\r\nbatch_size = 10\r\n\r\ncount = 0\r\nfor i in range(0, len(x_data), batch_size):\r\n    x_batch = x_data[i:i+batch_size]\r\n    pred_p = forward(network, x_batch)\r\n    count += (np.argmax(pred_p, axis=1) == np.argmax(y_data[i:i+batch_size], axis=1)).sum()\r\n    \r\nprint(\"Accuacy: \", count/len(x_data))\r\nAccuacy:  0.3333333333333333\r\n\r\n배치 처리는 컴퓨터로 계산할 때 큰 이점을 줍니다. 배치처리를 하지\r\n않았을 때는 for문 루프가 데이터 수만큼 돌았지만 배치크기를 10개로 했을\r\n때에는 10분의 1만큼 감소했습니다. 즉, 더 큰 배열로 한번에 계산하는 것이\r\n컴퓨터는 더 빠릅니다.\r\n다음장은 신경망 학습과정을 통해 가중치를 최적화하는 방법입니다. 위\r\n모델도 학습과정을 통해 Accuacy를 높일 수 있습니다!\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-26T00:42:26+09:00",
    "input_file": {}
  },
  {
    "path": "til/2022-05-20-pytorch-tutorial/",
    "title": "Pytorch tutorial",
    "description": "딥러닝 프레임워크 파이토치에 대해 알아보자",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-20",
    "categories": [
      "pytorch"
    ],
    "contents": "\r\n\r\nContents\r\ntorch tensor\r\ntensor의 parameter\r\ntensor의 생성\r\ndevice 지정\r\ntensor 다루기\r\ntonsor의 연산\r\n\r\nDataset\r\nDataLoader\r\ntorch.nn.Module\r\nreference\r\n\r\nsocar 부트캠프의 강의 중 일부를 정리한 내용입니다.\r\n\r\nimport numpy as np\r\n\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.utils.data import Dataset, DataLoader\r\n\r\nfrom torchvision import datasets, transforms\r\n\r\nimport matplotlib.pyplot as plt\r\n\r\ntorch tensor\r\ntensor는 numpy의 ndarray처럼 다차원 데이터 배열입니다. tensor를\r\n생성할 때는 list를 사용할 수도 있지만 ndarray를 사용할 수도\r\n있습니다.\r\nndarray와 tensor의 차이점은 tensor에는 back propagation(역전파)를\r\n다루기 위해 forward pass(순전파)에서 전달된 값과 연산의 종류를 기억할 수\r\n있습니다.(gradient 값 저장)\r\ntensor의 parameter\r\ndata : list나 ndarray 등 array데이터\r\ndtype : 데이터 타입\r\ndevice : tensor가 위치해있는 device 지정\r\nrequires_grad : gradient 값 저장 유무\r\ntensor의 생성\r\nndarray와 비슷한 생성 방법입니다.\r\n\r\na = torch.tensor([[1.0, 4.0], [4.0, 3.0]])\r\nb = torch.tensor([[4, 3], [1, 4], [1, 2]])\r\nprint(a)\r\ntensor([[1., 4.],\r\n        [4., 3.]])\r\nprint(b)\r\ntensor([[4, 3],\r\n        [1, 4],\r\n        [1, 2]])\r\n\r\ntensor.dtype : tensor의 자료형 확인\r\ntensor.shape : tensor의 size 확인\r\n\r\nprint(a.dtype, a.shape)\r\ntorch.float32 torch.Size([2, 2])\r\nprint(b.dtype, b.shape)\r\ntorch.int64 torch.Size([3, 2])\r\n\r\ntorch.ones(*size) : 모든 원소가 1인 tensor\r\ntorch.zeros(*size) : 모든 원소가 0인 tensor\r\ntorch.eye(n, m) : 대각 원소가 1이고 나머지가 0인 \\(n*m\\) tensor를 생성. m = None이면 \\(n*n\\)tensor를 생성\r\ntorch.rand(*size) : 모든 원소를 랜덤한 값으로 채워진 tensor.\r\ndtype을 int로 지정시 에러가 발생\r\n\r\na = torch.ones([2, 3])\r\nb = torch.zeros([3, 2], dtype=torch.int64)\r\nc = torch.eye(4,3)\r\nd = torch.rand([2, 4, 3], dtype=torch.float)\r\n\r\nprint(a)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.]])\r\nprint(b)\r\ntensor([[0, 0],\r\n        [0, 0],\r\n        [0, 0]])\r\nprint(c)\r\ntensor([[1., 0., 0.],\r\n        [0., 1., 0.],\r\n        [0., 0., 1.],\r\n        [0., 0., 0.]])\r\nprint(d)\r\ntensor([[[0.1721, 0.5773, 0.8588],\r\n         [0.8192, 0.3488, 0.5716],\r\n         [0.3033, 0.4843, 0.1258],\r\n         [0.6734, 0.4262, 0.9605]],\r\n\r\n        [[0.0436, 0.7960, 0.0340],\r\n         [0.6968, 0.8373, 0.5634],\r\n         [0.0413, 0.0030, 0.7226],\r\n         [0.0329, 0.8479, 0.7469]]])\r\n\r\n앞서 언급했듯이 ndarray로도 tensor를 생성할 수 있습니다. tensor()는\r\n원본의 값을 복사하는 반면 as_tensor()와 from_numpy()는 원본의 값을\r\n참조하기 때문에 원본의 값을 바꾸면 같이 변하는 것을 알 수 있습니다.\r\n\r\nimport numpy as np\r\n\r\nd = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])\r\n\r\na = torch.tensor(d)\r\nb = torch.as_tensor(d)\r\nc = torch.from_numpy(d)\r\n\r\nprint(a, id(a))\r\ntensor([[1, 1, 1],\r\n        [2, 2, 2],\r\n        [3, 3, 3]], dtype=torch.int32) 883303344\r\nprint(b, id(b))\r\ntensor([[1, 1, 1],\r\n        [2, 2, 2],\r\n        [3, 3, 3]], dtype=torch.int32) 883288720\r\nprint(c, id(c))\r\ntensor([[1, 1, 1],\r\n        [2, 2, 2],\r\n        [3, 3, 3]], dtype=torch.int32) 883073808\r\nprint(d, id(d))\r\n[[1 1 1]\r\n [2 2 2]\r\n [3 3 3]] 883007184\r\nd[0,0] = 0\r\n\r\nprint(a)\r\ntensor([[1, 1, 1],\r\n        [2, 2, 2],\r\n        [3, 3, 3]], dtype=torch.int32)\r\nprint(b)\r\ntensor([[0, 1, 1],\r\n        [2, 2, 2],\r\n        [3, 3, 3]], dtype=torch.int32)\r\nprint(c)\r\ntensor([[0, 1, 1],\r\n        [2, 2, 2],\r\n        [3, 3, 3]], dtype=torch.int32)\r\nprint(d)\r\n[[0 1 1]\r\n [2 2 2]\r\n [3 3 3]]\r\n\r\ntensor.shape을 통해 동일한 size의 tensor를 만들 수도 있지만\r\n_like(tensor) 방법으로도 동일한 size의 tensor를 만들 수 있습니다.\r\n\r\na = torch.ones(a.shape)\r\nprint(a)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.],\r\n        [1., 1., 1.]])\r\nb = torch.ones_like(a)\r\nc = torch.zeros_like(a, dtype=torch.float)\r\nd = torch.rand_like(a, dtype=torch.float)\r\n\r\nprint(b)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.],\r\n        [1., 1., 1.]])\r\nprint(c)\r\ntensor([[0., 0., 0.],\r\n        [0., 0., 0.],\r\n        [0., 0., 0.]])\r\nprint(d)\r\ntensor([[0.0823, 0.9254, 0.6186],\r\n        [0.4919, 0.9770, 0.6451],\r\n        [0.4601, 0.9505, 0.9724]])\r\n\r\ndevice 지정\r\ngpu연산을 위해서는 tensor의 device를 cuda로 지정해줘야합니다.\r\ntorch.cuda.is_available()를 통해 gpu가 사용 가능한지 확인\r\n후 tensor.to(‘cuda’)를 통해 device를 지정할 수 있습니다.\r\n\r\ntorch.cuda.is_available() # 저는 gpu가 없어요 ㅠㅜ\r\nFalse\r\nif torch.cuda.is_available():\r\n    device = \"cuda\"\r\nelse:\r\n    device = \"cpu\"\r\n    \r\na.to(device)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.],\r\n        [1., 1., 1.]])\r\na.device\r\ndevice(type='cpu')\r\n\r\ntensor 다루기\r\n\r\na = torch.ones([2, 3])\r\nb = torch.zeros([2, 3])\r\n\r\ntorch.cat() : 텐서 합치기\r\n\r\nc = torch.cat([a, b], dim=0) # 열방향으로 합치기\r\nd = torch.cat([a, b], dim=1) # 행방향으로 합치기\r\nprint(c)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.],\r\n        [0., 0., 0.],\r\n        [0., 0., 0.]])\r\nprint(c.shape)\r\ntorch.Size([4, 3])\r\nprint(d)\r\ntensor([[1., 1., 1., 0., 0., 0.],\r\n        [1., 1., 1., 0., 0., 0.]])\r\nprint(d.shape)\r\ntorch.Size([2, 6])\r\n\r\ntorch.stack : 텐서 쌓기\r\n새로운 차원(dim)에 따라 tensor들 쌓아줍니다. dim=0일때에는 tensor\r\n전체를 기준으로 쌓고, dim=1일때는 tensor의 다음 차원을 기준으로\r\n쌓는식입니다. 그러므로 dim은 0부터 tensor의 차원의 수를 넘을 수 없고,\r\nstack 안의 tensor들은 서로 size가 같아야 합니다.\r\n\r\nc = torch.stack([a, b], dim=0)\r\nd = torch.stack([a, b], dim=1)\r\ne = torch.stack([a, b], dim=2)\r\nprint(c)\r\ntensor([[[1., 1., 1.],\r\n         [1., 1., 1.]],\r\n\r\n        [[0., 0., 0.],\r\n         [0., 0., 0.]]])\r\nprint(c.shape)\r\ntorch.Size([2, 2, 3])\r\nprint(d)\r\ntensor([[[1., 1., 1.],\r\n         [0., 0., 0.]],\r\n\r\n        [[1., 1., 1.],\r\n         [0., 0., 0.]]])\r\nprint(d.shape)\r\ntorch.Size([2, 2, 3])\r\nprint(e)\r\ntensor([[[1., 0.],\r\n         [1., 0.],\r\n         [1., 0.]],\r\n\r\n        [[1., 0.],\r\n         [1., 0.],\r\n         [1., 0.]]])\r\nprint(e.shape)\r\ntorch.Size([2, 3, 2])\r\n\r\nhstack : torch.cat([a, b], dim=0)\r\nvstack : torch.cat([a, b], dim=1)\r\n\r\nc = torch.hstack([a, b])\r\nprint(c)\r\ntensor([[1., 1., 1., 0., 0., 0.],\r\n        [1., 1., 1., 0., 0., 0.]])\r\nprint(c.shape)\r\ntorch.Size([2, 6])\r\nd = torch.vstack([a, b])\r\nprint(d)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.],\r\n        [0., 0., 0.],\r\n        [0., 0., 0.]])\r\nprint(d.shape)\r\ntorch.Size([4, 3])\r\n\r\ntorch.unsqueeze() : 더미차원 삭제\r\ntorch.squeeze() : 더미차원 추가\r\n\r\n# unsqueeze and squeeze\r\nprint(a)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.]])\r\na = torch.unsqueeze(a, dim=1)\r\nprint(a)\r\ntensor([[[1., 1., 1.]],\r\n\r\n        [[1., 1., 1.]]])\r\nprint(a.shape)\r\ntorch.Size([2, 1, 3])\r\nprint(torch.squeeze(a))\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.]])\r\nprint(a.shape)\r\ntorch.Size([2, 1, 3])\r\n\r\ntonsor의 연산\r\n행렬 곱\r\n\r\na = torch.tensor(np.array(list(range(12)))).reshape(3, 4)\r\nprint(a)\r\ntensor([[ 0,  1,  2,  3],\r\n        [ 4,  5,  6,  7],\r\n        [ 8,  9, 10, 11]], dtype=torch.int32)\r\nb = torch.tensor(np.array(list(range(8)))).reshape(4, 2)\r\nprint(b)\r\ntensor([[0, 1],\r\n        [2, 3],\r\n        [4, 5],\r\n        [6, 7]], dtype=torch.int32)\r\nc = a @ b\r\nprint(c)\r\ntensor([[ 28,  34],\r\n        [ 76,  98],\r\n        [124, 162]], dtype=torch.int32)\r\nd = torch.matmul(a, b)\r\nprint(d)\r\ntensor([[ 28,  34],\r\n        [ 76,  98],\r\n        [124, 162]], dtype=torch.int32)\r\n\r\nelement-wise product\r\n\r\na = torch.tensor(np.array(list(range(6)))).reshape(2, 3)\r\nprint(a)\r\ntensor([[0, 1, 2],\r\n        [3, 4, 5]], dtype=torch.int32)\r\nb = torch.tensor(list(range(10, 13))).reshape(1, 3)\r\nprint(b)\r\ntensor([[10, 11, 12]])\r\nc = a * b\r\nprint(c)\r\ntensor([[ 0, 11, 24],\r\n        [30, 44, 60]])\r\nd = a.mul(b)\r\nprint(d)\r\ntensor([[ 0, 11, 24],\r\n        [30, 44, 60]])\r\n\r\nitem() : tensor의 원소를 반환합니다. tensor 안에 원소가 1개만 있어야\r\n합니다.\r\n\r\n# item\r\nagg = d.sum()\r\nv = agg.item()\r\nprint(v, type(v))\r\n169 <class 'int'>\r\n\r\ninplace operations\r\n\r\nprint(a)\r\ntensor([[0, 1, 2],\r\n        [3, 4, 5]], dtype=torch.int32)\r\na.add(5) \r\ntensor([[ 5,  6,  7],\r\n        [ 8,  9, 10]], dtype=torch.int32)\r\nprint(a)\r\ntensor([[0, 1, 2],\r\n        [3, 4, 5]], dtype=torch.int32)\r\na.add_(5) # inplace operations\r\ntensor([[ 5,  6,  7],\r\n        [ 8,  9, 10]], dtype=torch.int32)\r\nprint(a)\r\ntensor([[ 5,  6,  7],\r\n        [ 8,  9, 10]], dtype=torch.int32)\r\n\r\nDataset\r\n파이토치에서 Dataset은 전체 데이터를 sample 단위로 처리해주는 역할을\r\n합니다. Dataset을 상속받아 오버라이딩을 통해 커스텀 Dataset을\r\n만들어보겠습니다.\r\n커스텀 Dataset 구조\r\n\r\nclass CustomDataset(torch.utils.data.Dataset): \r\n    def __init__(self): \r\n        # 데이터셋의 전처리를 해주는 부분\r\n        \r\n    def __len__(self):\r\n        # 데이터셋의 길이를 반환해주는 부분\r\n        \r\n    def __getitem__(self, idx):\r\n        # 데이터셋에서 샘플을 추출해주는 부분\r\n\r\n다음 커스텀 데이터 셋을 확인해보면 __init__에서\r\nfeatures와 target 데이터, 전처리 함수를 저장하도록 정의 되있고,\r\n__len__에서 data의 길이를 반환해주도록 정의되있습니다.\r\n마지막으로 __getitem__에서 저장된 데이터를 인덱싱 후 정의된\r\n전처리 함수를 거쳐 반환되도록 구현되있습니다.\r\n\r\nclass LionDataset(Dataset):\r\n    def __init__(self, data, target, transform=None, target_transform=None):\r\n        self.data = data # feature data\r\n        self.target = target # target data\r\n        self.transform = transform # featrue\r\n        self.target_transform = target_transform\r\n        pass\r\n    \r\n    def __len__(self):\r\n        return len(self.data)\r\n    def __getitem__(self, idx):\r\n        x = self.data[idx]\r\n        y = self.target[idx]\r\n        \r\n        if self.transform:\r\n          x = self.transform(x)\r\n        if self.target_transform:\r\n          y = self.target_transform(y)\r\n        \r\n        return x, y\r\n\r\n\r\ndata = np.array(list(range(100))).reshape(-1, 2)\r\ntarget = np.array([[i] * 5 for i in range(10)]).reshape(-1)\r\n\r\nprint(data)\r\n[[ 0  1]\r\n [ 2  3]\r\n [ 4  5]\r\n [ 6  7]\r\n [ 8  9]\r\n [10 11]\r\n [12 13]\r\n [14 15]\r\n [16 17]\r\n [18 19]\r\n [20 21]\r\n [22 23]\r\n [24 25]\r\n [26 27]\r\n [28 29]\r\n [30 31]\r\n [32 33]\r\n [34 35]\r\n [36 37]\r\n [38 39]\r\n [40 41]\r\n [42 43]\r\n [44 45]\r\n [46 47]\r\n [48 49]\r\n [50 51]\r\n [52 53]\r\n [54 55]\r\n [56 57]\r\n [58 59]\r\n [60 61]\r\n [62 63]\r\n [64 65]\r\n [66 67]\r\n [68 69]\r\n [70 71]\r\n [72 73]\r\n [74 75]\r\n [76 77]\r\n [78 79]\r\n [80 81]\r\n [82 83]\r\n [84 85]\r\n [86 87]\r\n [88 89]\r\n [90 91]\r\n [92 93]\r\n [94 95]\r\n [96 97]\r\n [98 99]]\r\nprint(target)\r\n[0 0 0 0 0 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 4 4 4 4 4 5 5 5 5 5 6 6 6 6 6 7 7\r\n 7 7 7 8 8 8 8 8 9 9 9 9 9]\r\n\r\n\r\nlion = LionDataset(data=data, target=target)\r\nprint(lion[0:4])\r\n(array([[0, 1],\r\n       [2, 3],\r\n       [4, 5],\r\n       [6, 7]]), array([0, 0, 0, 0]))\r\nprint(len(lion))\r\n50\r\n\r\nDataLoader\r\nDataLoader는 dataset을 batch 단위로 묶어주는 역할을 합니다.\r\nbatch_size : batch_size\r\nshuffle : True시 epoch마다 데이터가 학습되는 순서가 섞임\r\n\r\nloader = DataLoader(dataset=lion, batch_size=10, shuffle=True)\r\n\r\nfor i, batch in enumerate(loader):\r\n    x, y = batch\r\n    if i == 0:\r\n        print(x)\r\n        print(y)\r\n    print(x.shape)\r\ntensor([[38, 39],\r\n        [ 4,  5],\r\n        [88, 89],\r\n        [62, 63],\r\n        [84, 85],\r\n        [44, 45],\r\n        [40, 41],\r\n        [20, 21],\r\n        [66, 67],\r\n        [68, 69]], dtype=torch.int32)\r\ntensor([3, 0, 8, 6, 8, 4, 4, 2, 6, 6], dtype=torch.int32)\r\ntorch.Size([10, 2])\r\ntorch.Size([10, 2])\r\ntorch.Size([10, 2])\r\ntorch.Size([10, 2])\r\ntorch.Size([10, 2])\r\n\r\ntorch.nn.Module\r\npytorch 모델은 parameters를 추적하며 forward pass를 진행한 뒤 back\r\npropagation을 통해 학습을 진행합니다. torch.nn.Module은 여러 층의\r\nlayer로 이뤄진 모델을 쉽게 관리할 수 있는 class입니다.\r\npytorch 모델의 기본구조\r\n\r\nclass Model_Name(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        \"\"\"\r\n        모델에 사용될 Layer(nn.Linear, nn.Conv2d)와 \r\n        activation function(nn.functional.relu, nn.functional.sigmoid)등을 정의\r\n        \"\"\"\r\n\r\n    def forward(self, x):\r\n        \"\"\"\r\n        모델에서 실행되어야하는 계산을 정의\r\n        \"\"\"\r\n        return x\r\n\r\nexample\r\n\r\nclass LionLinear(nn.Module):\r\n    def __init__(self, input_dim, output_dim):\r\n        super().__init__()\r\n        self.input_dim = input_dim   # 입력차원 \r\n        self.output_dim = output_dim # 출력차원\r\n        \r\n        self.flatten = nn.Flatten()  # tensor 평탄화 정의\r\n        self.classifier = nn.Linear(input_dim, output_dim) # Linear layer 정의\r\n        self.act = nn.ReLU() # activation function(ReLU) 정의\r\n        \r\n    def forward(self, x):\r\n        x = self.flatten(x)    # data를 linear layers에 맞게 평탄화 후\r\n        x = self.classifier(x) # linear layer를 통과,\r\n        x = self.act(x)        # activation function을 통해 출력\r\n        \r\n        return x\r\n\r\npytorch 모델은 모델의 구조를 쉽게 파악할 수 있습니다.\r\n\r\nlinear_model = LionLinear(28*28, 10).to(device)\r\nprint(linear_model)\r\nLionLinear(\r\n  (flatten): Flatten(start_dim=1, end_dim=-1)\r\n  (classifier): Linear(in_features=784, out_features=10, bias=True)\r\n  (act): ReLU()\r\n)\r\nprint(linear_model.classifier)\r\nLinear(in_features=784, out_features=10, bias=True)\r\n\r\n다음은 MLP를 구현하는 module입니다.\r\n코드를 더 간결하게 하기 위해 다음과 같이 일부 layer 등을 따로 모듈로\r\n구현 후 전체 모듈에 합쳐서 구현할 수 있습니다.\r\n\r\nclass LionLayer(nn.Module):\r\n    def __init__(self, input_dim, output_dim):\r\n        super().__init__()\r\n        self.input_dim = input_dim\r\n        self.output_dim = output_dim\r\n        self.layer = nn.Linear(self.input_dim, self.output_dim)\r\n        pass\r\n\r\n    def forward(self, x):\r\n        assert x.shape[-1] == self.input_dim, \"Input dimension mismatch\"\r\n        return self.layer(x)\r\n\r\n\r\nclass LionMLP(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim):\r\n        super().__init__()\r\n        self.input_dim = input_dim\r\n        self.hidden_dim = hidden_dim\r\n        self.output_dim = output_dim\r\n        self.flatten = nn.Flatten()\r\n        self.linear_1 = LionLayer(self.input_dim, self.hidden_dim)\r\n        self.linear_2 = LionLayer(self.hidden_dim, self.output_dim)\r\n        self.act_1 = nn.ReLU()\r\n        self.act_2 = nn.Softmax()\r\n        pass\r\n    \r\n    def forward(self, x):\r\n        x = self.flatten(x)\r\n        x = self.act_1(self.linear_1(x))\r\n        x = self.act_2(self.linear_2(x))\r\n        return x\r\n\r\n\r\nmlp = LionMLP(28*28, 50, 10)\r\nprint(mlp)\r\nLionMLP(\r\n  (flatten): Flatten(start_dim=1, end_dim=-1)\r\n  (linear_1): LionLayer(\r\n    (layer): Linear(in_features=784, out_features=50, bias=True)\r\n  )\r\n  (linear_2): LionLayer(\r\n    (layer): Linear(in_features=50, out_features=10, bias=True)\r\n  )\r\n  (act_1): ReLU()\r\n  (act_2): Softmax(dim=None)\r\n)\r\n\r\nnn.Sequential()을 통해 forward() 부분을 짧게 작성할 수 있습니다.\r\n\r\nclass LionMLP(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim):\r\n        super().__init__()\r\n        self.input_dim = input_dim\r\n        self.hidden_dim = hidden_dim\r\n        self.output_dim = output_dim\r\n        self.flatten = nn.Flatten()\r\n        self.linear_1 = LionLayer(self.input_dim, self.hidden_dim)\r\n        self.linear_2 = LionLayer(self.hidden_dim, self.output_dim)\r\n        self.act_1 = nn.ReLU()\r\n        self.act_2 = nn.Softmax()\r\n        self.model = nn.Sequential(self.flatten, self.linear_1, self.act_1, self.linear_2, self.act_2)\r\n        pass\r\n    def forward(self, x):\r\n        return self.model(x)\r\n\r\n\r\nmlp = LionMLP(28 * 28, 40, 10)\r\nprint(mlp)\r\nLionMLP(\r\n  (flatten): Flatten(start_dim=1, end_dim=-1)\r\n  (linear_1): LionLayer(\r\n    (layer): Linear(in_features=784, out_features=40, bias=True)\r\n  )\r\n  (linear_2): LionLayer(\r\n    (layer): Linear(in_features=40, out_features=10, bias=True)\r\n  )\r\n  (act_1): ReLU()\r\n  (act_2): Softmax(dim=None)\r\n  (model): Sequential(\r\n    (0): Flatten(start_dim=1, end_dim=-1)\r\n    (1): LionLayer(\r\n      (layer): Linear(in_features=784, out_features=40, bias=True)\r\n    )\r\n    (2): ReLU()\r\n    (3): LionLayer(\r\n      (layer): Linear(in_features=40, out_features=10, bias=True)\r\n    )\r\n    (4): Softmax(dim=None)\r\n  )\r\n)\r\n\r\n모델 파라미터 확인\r\n\r\nfor name, param in mlp.named_parameters():\r\n    print(f\"Layer: {name} | Size: {param.size()} | Values: {param[:2]} \\n\")\r\nLayer: linear_1.layer.weight | Size: torch.Size([40, 784]) | Values: tensor([[ 0.0178, -0.0108, -0.0017,  ...,  0.0248, -0.0011,  0.0218],\r\n        [-0.0343,  0.0235, -0.0201,  ...,  0.0321, -0.0141,  0.0180]],\r\n       grad_fn=<SliceBackward0>) \r\n\r\nLayer: linear_1.layer.bias | Size: torch.Size([40]) | Values: tensor([ 0.0207, -0.0041], grad_fn=<SliceBackward0>) \r\n\r\nLayer: linear_2.layer.weight | Size: torch.Size([10, 40]) | Values: tensor([[ 0.0301, -0.0820, -0.1123, -0.0777, -0.0678, -0.1305, -0.0293,  0.0596,\r\n         -0.0557, -0.1315, -0.1003,  0.0239,  0.1351,  0.0894, -0.0559, -0.0034,\r\n          0.0682,  0.1417,  0.0025, -0.0303, -0.0004, -0.0029,  0.0281,  0.0950,\r\n          0.0631, -0.0958, -0.0276, -0.1566,  0.1130, -0.0361,  0.0906, -0.0657,\r\n          0.1251,  0.0872, -0.1033,  0.0821,  0.0856, -0.1505,  0.1350,  0.1250],\r\n        [ 0.0562,  0.0557,  0.0730, -0.1076,  0.0850, -0.1426,  0.1005,  0.1108,\r\n          0.0231,  0.1560,  0.1185, -0.0472,  0.0049,  0.0836,  0.0008, -0.1055,\r\n          0.1363,  0.1266, -0.0864,  0.1325,  0.1444,  0.1412,  0.1253, -0.0832,\r\n          0.0536,  0.0351, -0.1228, -0.0855, -0.0909,  0.0840, -0.0335,  0.0978,\r\n         -0.0824,  0.1048,  0.0254, -0.0287,  0.0238,  0.1337, -0.1085,  0.1532]],\r\n       grad_fn=<SliceBackward0>) \r\n\r\nLayer: linear_2.layer.bias | Size: torch.Size([10]) | Values: tensor([ 0.0235, -0.0153], grad_fn=<SliceBackward0>) \r\n\r\nreference\r\nPyTorch로 시작하는 딥 러닝 입문 : https://wikidocs.net/57165\r\nhttps://anweh.tistory.com/21\r\n\r\n\r\n\r\n",
    "preview": "til/2022-05-20-pytorch-tutorial/images/pytorch.jpg",
    "last_modified": "2022-05-22T15:10:31+09:00",
    "input_file": {}
  },
  {
    "path": "til/2022-05-19-cotepart1/",
    "title": "[백준 문제풀이] 기본기",
    "description": "코딩테스트를 위한 백준 문제 풀어보기",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-19",
    "categories": [
      "coding test"
    ],
    "contents": "\r\n\r\nContents\r\n약수 구하기\r\n이진수\r\nbin() 직접 구현하기(양의\r\n정수일때만)\r\n\r\n최대, 최소\r\n\r\n약수 구하기\r\n두 개의 자연수 N과 K가 주어졌을 때, N의 약수들 중 K번째로 작은 수를\r\n출력하는 프로그램을 작성하시오.\r\n입력\r\n첫째 줄에 N과 K가 빈칸을 사이에 두고 주어진다. N은 1 이상 10,000\r\n이하이다. K는 1 이상 N 이하이다.\r\n출력\r\n첫째 줄에 N의 약수들 중 K번째로 작은 수를 출력한다. 만일 N의 약수의\r\n개수가 K개보다 적어서 K번째 약수가 존재하지 않을 경우에는 0을\r\n출력하시오.\r\n\r\nN, K = map(int, input().split())\r\n\r\nresult = 0\r\n\r\nfor i in range(1, N + 1):\r\n    if N % i == 0:\r\n        K -= 1\r\n        if K == 0:\r\n            result = i\r\n            break\r\n\r\nprint(result)\r\n\r\n이진수\r\n양의 정수 n이 주어졌을 때, 이를 이진수로 나타냈을 때 1의 위치를 모두\r\n찾는 프로그램을 작성하시오. 최하위 비트(least significant bit, lsb)의\r\n위치는 0이다.\r\n입력\r\n첫째 줄에 테스트 케이스의 개수 T가 주어진다. 각 테스트 케이스는 한\r\n줄로 이루어져 있고, n이 주어진다. \\((1 ≤ T ≤\r\n10, 1 ≤ n ≤ 10^6)\\)\r\n출력\r\n각 테스트 케이스에 대해서, 1의 위치를 공백으로 구분해서 줄 하나에\r\n출력한다. 위치가 낮은 것부터 출력한다.\r\n\r\nt = int(input())\r\n\r\nfor _ in range(t):\r\n    n = int(input())\r\n    b = bin(n)[2:]\r\n    \r\n    for i in range(len(b)):\r\n        if b[::-1][i] == '1':\r\n            print(i, end=' ')\r\n\r\nbin() 직접 구현하기(양의\r\n정수일때만)\r\n\r\ndef my_bin(num):\r\n    a = num//2\r\n    b = num%2\r\n    if a == 0:\r\n        return '1'\r\n    else:\r\n        return my_bin(a)+str(b)\r\n\r\n최대, 최소\r\nN개의 정수가 주어진다. 이때, 최솟값과 최댓값을 구하는 프로그램을\r\n작성하시오.\r\n입력\r\n첫째 줄에 정수의 개수 N (1 ≤ N ≤ 1,000,000)이 주어진다. 둘째 줄에는\r\nN개의 정수를 공백으로 구분해서 주어진다. 모든 정수는 -1,000,000보다\r\n크거나 같고, 1,000,000보다 작거나 같은 정수이다.\r\n출력\r\n첫째 줄에 주어진 정수 N개의 최솟값과 최댓값을 공백으로 구분해\r\n출력한다.\r\n\r\ninput()\r\nl = list(map(int, input().split()))\r\n\r\nhigh = low = l[0]\r\n\r\nfor i in l[1:]:\r\n    if high < i:\r\n        high = i\r\n    if low > i:\r\n        low = i\r\n        \r\nprint(low, high)\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-20T19:04:09+09:00",
    "input_file": {}
  },
  {
    "path": "til/2022-05-18-dlfromscratchch2/",
    "title": "퍼셉트론이란",
    "description": "밑바닥부터 시작하는 딥러닝을 읽고 딥러닝의 기원이 되는 알고리즘인 퍼셉트론에 대해 정리해봤습니다.",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-18",
    "categories": [
      "밑바닥부터 시작하는 딥러닝"
    ],
    "contents": "\r\n\r\nContents\r\n1️⃣ 퍼셉트론이란\r\n2️⃣ 퍼셉트론의 한계\r\n3️⃣ 다층 퍼셉트론\r\n✅ 요약\r\n\r\n1️⃣ 퍼셉트론이란\r\n퍼셉트론은 다수의 신호를 입력(input)으로 받아 하나의 신호를\r\n출력(output)하는 알고리즘입니다.\r\n2개의 입력을 받아 하나의 신호를 출력하는 퍼셉트론을 수식으로\r\n나타내면,\r\n\\[y = \\begin{cases}0,(w_{1}x_{1} +\r\nw_{2}x_{2}\\leq\\theta)\\\\1,(w_{1}x_{1} +\r\nw_{2}x_{2}>\\theta)\\\\\\end{cases}\\] 또는 \\(\\theta\\)를 \\(-b\\)로 치환하여,\r\n\\[y = \\begin{cases}0,(b+w_{1}x_{1} +\r\nw_{2}x_{2}\\leq0)\\\\1,(b+w_{1}x_{1} +\r\nw_{2}x_{2}>0)\\\\\\end{cases}\\]\r\n\\(w\\) : weight, \\(b\\): bias\r\n2️⃣ 퍼셉트론의 한계\r\n퍼셉트론이 풀 수 있는 (예측할 수 있는) 문제는 AND, NAND,\r\nOR입니다.\r\nAND\r\n\r\nx1x2y000100010111\r\n\r\nNAND\r\n\r\nx1x2y001101011110\r\n\r\nOR\r\n\r\nx1x2y000101011111\r\n\r\n위 세가지 문제는 \\((w_1, w_2,\r\ntheta)\\)에 각각 \\((0.5, 0.5, 0.7),\r\n(-0.5, -0.5, -0.7), (1.5, 1.5, 0.7)\\) 등을 대입하면 풀 수\r\n있습니다.\r\n이를 파이썬 함수로 구현하면,\r\n\r\ndef AND(x1, x2):\r\n    w1, w2, theta = 0.5, 0.5, 0.5\r\n    tmp = x1*w1+x2*w2\r\n    if tmp <= theta:\r\n        return 0\r\n    else:\r\n        return 1\r\n\r\ndef NAND(x1, x2):\r\n    w1, w2, theta = -0.5, -0.5, -0.7\r\n    tmp = x1*w1+x2*w2\r\n    if tmp <= theta:\r\n        return 0\r\n    else:\r\n        return 1\r\n    \r\ndef OR(x1, x2):\r\n    w1, w2, theta = 1.5, 1.5, 0.7\r\n    tmp = x1*w1+x2*w2\r\n    if tmp <= theta:\r\n        return 0\r\n    else:\r\n        return 1\r\n\r\nprint('x1=0, x2=0 : ', AND(0,0))\r\nx1=0, x2=0 :  0\r\nprint('x1=0, x2=1 : ', AND(0,1))\r\nx1=0, x2=1 :  0\r\nprint('x1=1, x2=0 : ', AND(1,0))\r\nx1=1, x2=0 :  0\r\nprint('x1=1, x2=1 : ', AND(1,1))\r\nx1=1, x2=1 :  1\r\nprint('x1=0, x2=0 : ', NAND(0,0))\r\nx1=0, x2=0 :  1\r\nprint('x1=0, x2=1 : ', NAND(0,1))\r\nx1=0, x2=1 :  1\r\nprint('x1=1, x2=0 : ', NAND(1,0))\r\nx1=1, x2=0 :  1\r\nprint('x1=1, x2=1 : ', NAND(1,1))\r\nx1=1, x2=1 :  0\r\nprint('x1=0, x2=0 : ', OR(0,0))\r\nx1=0, x2=0 :  0\r\nprint('x1=0, x2=1 : ', OR(0,1))\r\nx1=0, x2=1 :  1\r\nprint('x1=1, x2=0 : ', OR(1,0))\r\nx1=1, x2=0 :  1\r\nprint('x1=1, x2=1 : ', OR(1,1))\r\nx1=1, x2=1 :  1\r\n\r\n하지만, 퍼셉트론으로 XOR은 해결할 수 없습니다.\r\nXOR\r\n\r\nx1x2y000101011110\r\n\r\n위 문제를 그림으로 나타내면 다음과 같이 빨간점과 검은점을 구분할 수\r\n있는 선을 그을 수 있어야 합니다.\r\n\r\n\r\n\r\n곡선으로 선을 그린다면 구분할 수 있겠지만 퍼셉트론으로는 직선밖에\r\n그릴 수 없으므로 해결할 수 없습니다. 즉, 퍼셉트론은 비선형의 영역을\r\n표현할 수 없습니다.\r\n3️⃣ 다층 퍼셉트론\r\n이를 해결하기 위해 나온 것이 다층 퍼셉트론입니다. 말 그대로 퍼셉트론\r\n층을 여러 개 갖는 알고리즘이며 앞에서 언급했던 퍼셉트론은 정확히 말하면\r\n단층 퍼셉트론입니다.\r\nXOR문제를 해결하려면 위에서 AND, NAND, OR 문제를 해결하기 위해\r\n만들었던 퍼셉트론을 적절히 쌓아 만들면 됩니다.\r\n\r\ndef XOR(x1, x2):\r\n    s1 = NAND(x1, x2)\r\n    s2 = OR(x1, x2)\r\n    y = AND(s1, s2)\r\n    return y\r\n\r\nprint('x1=0, x2=0 : ', XOR(0,0))\r\nx1=0, x2=0 :  0\r\nprint('x1=0, x2=1 : ', XOR(0,1))\r\nx1=0, x2=1 :  1\r\nprint('x1=1, x2=0 : ', XOR(1,0))\r\nx1=1, x2=0 :  1\r\nprint('x1=1, x2=1 : ', XOR(1,1))\r\nx1=1, x2=1 :  0\r\n\r\n첫번째 층에 NAND, OR를 풀기 위해 만들었던 퍼셉트론을 쌓고 두번째 층에\r\nAND를 풀기 위해 만들었던 퍼셉트론을 쌓아 XOR문제를 해결했습니다.\r\n이처럼 다층 퍼셉트론은 비선형의 영역까지도 표현할 수 있음을 알 수\r\n있습니다.\r\n✅ 요약\r\n퍼셉트론은 입출력을 갖춘 알고리즘\r\n단층 퍼셉트론은 선형 영역만 표현할 수 있고, 다층 퍼셉트론은\r\n비선형 영역도 표현 가능\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-19T00:59:30+09:00",
    "input_file": {}
  },
  {
    "path": "til/2022-05-13-first-til/",
    "title": "멋쟁이사자처럼! TIL(Today I Learn) 페이지 개설!!",
    "description": "매일 공부하는 것을 기록하기",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-13",
    "categories": [],
    "contents": "\r\n매일매일 공부한 것을 기록하는 페이지를 만들었다. 일기처럼 나중에 봐도\r\n뿌듯한 페이지가 되길..\r\n😄화이팅!!\r\n\r\n\r\n\r\n",
    "preview": "til/2022-05-13-first-til/images/owl.png",
    "last_modified": "2022-05-26T18:50:34+09:00",
    "input_file": {},
    "preview_width": 640,
    "preview_height": 514
  }
]

[
  {
    "path": "til/2022-05-20-pytorch-tutorial/",
    "title": "Pytorch tutorial",
    "description": "딥러닝 프레임워크 파이토치에 대해 알아보자",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-20",
    "categories": [
      "pytorch"
    ],
    "contents": "\r\n\r\nContents\r\ntorch tensor\r\ntensor의 parameter\r\ntensor의 생성\r\ndevice 지정\r\ntensor 다루기\r\ntonsor의 연산\r\n\r\nDataset\r\nDataLoader\r\ntorch.nn.Module\r\nreference\r\n\r\nsocar 부트캠프의 강의 중 일부를 정리한 내용입니다.\r\n\r\nimport numpy as np\r\n\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.utils.data import Dataset, DataLoader\r\n\r\nfrom torchvision import datasets, transforms\r\n\r\nimport matplotlib.pyplot as plt\r\n\r\ntorch tensor\r\ntensor는 numpy의 ndarray처럼 다차원 데이터 배열입니다. tensor를\r\n생성할 때는 list를 사용할 수도 있지만 ndarray를 사용할 수도\r\n있습니다.\r\nndarray와 tensor의 차이점은 tensor에는 back propagation(역전파)를\r\n다루기 위해 forward pass(순전파)에서 전달된 값과 연산의 종류를 기억할 수\r\n있습니다.(gradient 값 저장)\r\ntensor의 parameter\r\ndata : list나 ndarray 등 array데이터\r\ndtype : 데이터 타입\r\ndevice : tensor가 위치해있는 device 지정\r\nrequires_grad : gradient 값 저장 유무\r\ntensor의 생성\r\nndarray와 비슷한 생성 방법입니다.\r\n\r\na = torch.tensor([[1.0, 4.0], [4.0, 3.0]])\r\nb = torch.tensor([[4, 3], [1, 4], [1, 2]])\r\nprint(a)\r\ntensor([[1., 4.],\r\n        [4., 3.]])\r\nprint(b)\r\ntensor([[4, 3],\r\n        [1, 4],\r\n        [1, 2]])\r\n\r\ntensor.dtype : tensor의 자료형 확인\r\ntensor.shape : tensor의 size 확인\r\n\r\nprint(a.dtype, a.shape)\r\ntorch.float32 torch.Size([2, 2])\r\nprint(b.dtype, b.shape)\r\ntorch.int64 torch.Size([3, 2])\r\n\r\ntorch.ones(*size) : 모든 원소가 1인 tensor\r\ntorch.zeros(*size) : 모든 원소가 0인 tensor\r\ntorch.eye(n, m) : 대각 원소가 1이고 나머지가 0인 \\(n*m\\) tensor를 생성. m = None이면 \\(n*n\\)tensor를 생성\r\ntorch.rand(*size) : 모든 원소를 랜덤한 값으로 채워진 tensor.\r\ndtype을 int로 지정시 에러가 발생\r\n\r\na = torch.ones([2, 3])\r\nb = torch.zeros([3, 2], dtype=torch.int64)\r\nc = torch.eye(4,3)\r\nd = torch.rand([2, 4, 3], dtype=torch.float)\r\n\r\nprint(a)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.]])\r\nprint(b)\r\ntensor([[0, 0],\r\n        [0, 0],\r\n        [0, 0]])\r\nprint(c)\r\ntensor([[1., 0., 0.],\r\n        [0., 1., 0.],\r\n        [0., 0., 1.],\r\n        [0., 0., 0.]])\r\nprint(d)\r\ntensor([[[0.4267, 0.7558, 0.7581],\r\n         [0.9764, 0.5643, 0.0557],\r\n         [0.3452, 0.6512, 0.9713],\r\n         [0.0638, 0.0089, 0.6160]],\r\n\r\n        [[0.1207, 0.4361, 0.2639],\r\n         [0.5522, 0.8477, 0.4962],\r\n         [0.3894, 0.3661, 0.4973],\r\n         [0.0413, 0.8170, 0.7664]]])\r\n\r\n앞서 언급했듯이 ndarray로도 tensor를 생성할 수 있습니다. tensor()는\r\n원본의 값을 복사하는 반면 as_tensor()와 from_numpy()는 원본의 값을\r\n참조하기 때문에 원본의 값을 바꾸면 같이 변하는 것을 알 수 있습니다.\r\n\r\nimport numpy as np\r\n\r\nd = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])\r\n\r\na = torch.tensor(d)\r\nb = torch.as_tensor(d)\r\nc = torch.from_numpy(d)\r\n\r\nprint(a, id(a))\r\ntensor([[1, 1, 1],\r\n        [2, 2, 2],\r\n        [3, 3, 3]], dtype=torch.int32) 883304064\r\nprint(b, id(b))\r\ntensor([[1, 1, 1],\r\n        [2, 2, 2],\r\n        [3, 3, 3]], dtype=torch.int32) 883289280\r\nprint(c, id(c))\r\ntensor([[1, 1, 1],\r\n        [2, 2, 2],\r\n        [3, 3, 3]], dtype=torch.int32) 883069472\r\nprint(d, id(d))\r\n[[1 1 1]\r\n [2 2 2]\r\n [3 3 3]] 883007184\r\nd[0,0] = 0\r\n\r\nprint(a)\r\ntensor([[1, 1, 1],\r\n        [2, 2, 2],\r\n        [3, 3, 3]], dtype=torch.int32)\r\nprint(b)\r\ntensor([[0, 1, 1],\r\n        [2, 2, 2],\r\n        [3, 3, 3]], dtype=torch.int32)\r\nprint(c)\r\ntensor([[0, 1, 1],\r\n        [2, 2, 2],\r\n        [3, 3, 3]], dtype=torch.int32)\r\nprint(d)\r\n[[0 1 1]\r\n [2 2 2]\r\n [3 3 3]]\r\n\r\ntensor.shape을 통해 동일한 size의 tensor를 만들 수도 있지만\r\n_like(tensor) 방법으로도 동일한 size의 tensor를 만들 수 있습니다.\r\n\r\na = torch.ones(a.shape)\r\nprint(a)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.],\r\n        [1., 1., 1.]])\r\nb = torch.ones_like(a)\r\nc = torch.zeros_like(a, dtype=torch.float)\r\nd = torch.rand_like(a, dtype=torch.float)\r\n\r\nprint(b)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.],\r\n        [1., 1., 1.]])\r\nprint(c)\r\ntensor([[0., 0., 0.],\r\n        [0., 0., 0.],\r\n        [0., 0., 0.]])\r\nprint(d)\r\ntensor([[0.1732, 0.2645, 0.7020],\r\n        [0.5683, 0.8747, 0.1726],\r\n        [0.5885, 0.1139, 0.1493]])\r\n\r\ndevice 지정\r\ngpu연산을 위해서는 tensor의 device를 cuda로 지정해줘야합니다.\r\ntorch.cuda.is_available()를 통해 gpu가 사용 가능한지 확인\r\n후 tensor.to(‘cuda’)를 통해 device를 지정할 수 있습니다.\r\n\r\ntorch.cuda.is_available() # 저는 gpu가 없어요 ㅠㅜ\r\nFalse\r\nif torch.cuda.is_available():\r\n    device = \"cuda\"\r\nelse:\r\n    device = \"cpu\"\r\n    \r\na.to(device)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.],\r\n        [1., 1., 1.]])\r\na.device\r\ndevice(type='cpu')\r\n\r\ntensor 다루기\r\n\r\na = torch.ones([2, 3])\r\nb = torch.zeros([2, 3])\r\n\r\ntorch.cat() : 텐서 합치기\r\n\r\nc = torch.cat([a, b], dim=0) # 열방향으로 합치기\r\nd = torch.cat([a, b], dim=1) # 행방향으로 합치기\r\nprint(c)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.],\r\n        [0., 0., 0.],\r\n        [0., 0., 0.]])\r\nprint(c.shape)\r\ntorch.Size([4, 3])\r\nprint(d)\r\ntensor([[1., 1., 1., 0., 0., 0.],\r\n        [1., 1., 1., 0., 0., 0.]])\r\nprint(d.shape)\r\ntorch.Size([2, 6])\r\n\r\ntorch.stack : 텐서 쌓기\r\n새로운 차원(dim)에 따라 tensor들 쌓아줍니다. dim=0일때에는 tensor\r\n전체를 기준으로 쌓고, dim=1일때는 tensor의 다음 차원을 기준으로\r\n쌓는식입니다. 그러므로 dim은 0부터 tensor의 차원의 수를 넘을 수 없고,\r\nstack 안의 tensor들은 서로 size가 같아야 합니다.\r\n\r\nc = torch.stack([a, b], dim=0)\r\nd = torch.stack([a, b], dim=1)\r\ne = torch.stack([a, b], dim=2)\r\nprint(c)\r\ntensor([[[1., 1., 1.],\r\n         [1., 1., 1.]],\r\n\r\n        [[0., 0., 0.],\r\n         [0., 0., 0.]]])\r\nprint(c.shape)\r\ntorch.Size([2, 2, 3])\r\nprint(d)\r\ntensor([[[1., 1., 1.],\r\n         [0., 0., 0.]],\r\n\r\n        [[1., 1., 1.],\r\n         [0., 0., 0.]]])\r\nprint(d.shape)\r\ntorch.Size([2, 2, 3])\r\nprint(e)\r\ntensor([[[1., 0.],\r\n         [1., 0.],\r\n         [1., 0.]],\r\n\r\n        [[1., 0.],\r\n         [1., 0.],\r\n         [1., 0.]]])\r\nprint(e.shape)\r\ntorch.Size([2, 3, 2])\r\n\r\nhstack : torch.cat([a, b], dim=0)\r\nvstack : torch.cat([a, b], dim=1)\r\n\r\nc = torch.hstack([a, b])\r\nprint(c)\r\ntensor([[1., 1., 1., 0., 0., 0.],\r\n        [1., 1., 1., 0., 0., 0.]])\r\nprint(c.shape)\r\ntorch.Size([2, 6])\r\nd = torch.vstack([a, b])\r\nprint(d)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.],\r\n        [0., 0., 0.],\r\n        [0., 0., 0.]])\r\nprint(d.shape)\r\ntorch.Size([4, 3])\r\n\r\ntorch.unsqueeze() : 더미차원 삭제\r\ntorch.squeeze() : 더미차원 추가\r\n\r\n# unsqueeze and squeeze\r\nprint(a)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.]])\r\na = torch.unsqueeze(a, dim=1)\r\nprint(a)\r\ntensor([[[1., 1., 1.]],\r\n\r\n        [[1., 1., 1.]]])\r\nprint(a.shape)\r\ntorch.Size([2, 1, 3])\r\nprint(torch.squeeze(a))\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.]])\r\nprint(a.shape)\r\ntorch.Size([2, 1, 3])\r\n\r\ntonsor의 연산\r\n행렬 곱\r\n\r\na = torch.tensor(np.array(list(range(12)))).reshape(3, 4)\r\nprint(a)\r\ntensor([[ 0,  1,  2,  3],\r\n        [ 4,  5,  6,  7],\r\n        [ 8,  9, 10, 11]], dtype=torch.int32)\r\nb = torch.tensor(np.array(list(range(8)))).reshape(4, 2)\r\nprint(b)\r\ntensor([[0, 1],\r\n        [2, 3],\r\n        [4, 5],\r\n        [6, 7]], dtype=torch.int32)\r\nc = a @ b\r\nprint(c)\r\ntensor([[ 28,  34],\r\n        [ 76,  98],\r\n        [124, 162]], dtype=torch.int32)\r\nd = torch.matmul(a, b)\r\nprint(d)\r\ntensor([[ 28,  34],\r\n        [ 76,  98],\r\n        [124, 162]], dtype=torch.int32)\r\n\r\nelement-wise product\r\n\r\na = torch.tensor(np.array(list(range(6)))).reshape(2, 3)\r\nprint(a)\r\ntensor([[0, 1, 2],\r\n        [3, 4, 5]], dtype=torch.int32)\r\nb = torch.tensor(list(range(10, 13))).reshape(1, 3)\r\nprint(b)\r\ntensor([[10, 11, 12]])\r\nc = a * b\r\nprint(c)\r\ntensor([[ 0, 11, 24],\r\n        [30, 44, 60]])\r\nd = a.mul(b)\r\nprint(d)\r\ntensor([[ 0, 11, 24],\r\n        [30, 44, 60]])\r\n\r\nitem() : tensor의 원소를 반환합니다. tensor 안에 원소가 1개만 있어야\r\n합니다.\r\n\r\n# item\r\nagg = d.sum()\r\nv = agg.item()\r\nprint(v, type(v))\r\n169 <class 'int'>\r\n\r\ninplace operations\r\n\r\nprint(a)\r\ntensor([[0, 1, 2],\r\n        [3, 4, 5]], dtype=torch.int32)\r\na.add(5) \r\ntensor([[ 5,  6,  7],\r\n        [ 8,  9, 10]], dtype=torch.int32)\r\nprint(a)\r\ntensor([[0, 1, 2],\r\n        [3, 4, 5]], dtype=torch.int32)\r\na.add_(5) # inplace operations\r\ntensor([[ 5,  6,  7],\r\n        [ 8,  9, 10]], dtype=torch.int32)\r\nprint(a)\r\ntensor([[ 5,  6,  7],\r\n        [ 8,  9, 10]], dtype=torch.int32)\r\n\r\nDataset\r\n파이토치에서 Dataset은 전체 데이터를 sample 단위로 처리해주는 역할을\r\n합니다. Dataset을 상속받아 오버라이딩을 통해 커스텀 Dataset을\r\n만들어보겠습니다.\r\n커스텀 Dataset 구조\r\n\r\nclass CustomDataset(torch.utils.data.Dataset): \r\n    def __init__(self): \r\n        # 데이터셋의 전처리를 해주는 부분\r\n        \r\n    def __len__(self):\r\n        # 데이터셋의 길이를 반환해주는 부분\r\n        \r\n    def __getitem__(self, idx):\r\n        # 데이터셋에서 샘플을 추출해주는 부분\r\n\r\n다음 커스텀 데이터 셋을 확인해보면 __init__에서\r\nfeatures와 target 데이터, 전처리 함수를 저장하도록 정의 되있고,\r\n__len__에서 data의 길이를 반환해주도록 정의되있습니다.\r\n마지막으로 __getitem__에서 저장된 데이터를 인덱싱 후 정의된\r\n전처리 함수를 거쳐 반환되도록 구현되있습니다.\r\n\r\nclass LionDataset(Dataset):\r\n    def __init__(self, data, target, transform=None, target_transform=None):\r\n        self.data = data # feature data\r\n        self.target = target # target data\r\n        self.transform = transform # featrue\r\n        self.target_transform = target_transform\r\n        pass\r\n    \r\n    def __len__(self):\r\n        return len(self.data)\r\n    def __getitem__(self, idx):\r\n        x = self.data[idx]\r\n        y = self.target[idx]\r\n        \r\n        if self.transform:\r\n          x = self.transform(x)\r\n        if self.target_transform:\r\n          y = self.target_transform(y)\r\n        \r\n        return x, y\r\n\r\n\r\ndata = np.array(list(range(100))).reshape(-1, 2)\r\ntarget = np.array([[i] * 5 for i in range(10)]).reshape(-1)\r\n\r\nprint(data)\r\n[[ 0  1]\r\n [ 2  3]\r\n [ 4  5]\r\n [ 6  7]\r\n [ 8  9]\r\n [10 11]\r\n [12 13]\r\n [14 15]\r\n [16 17]\r\n [18 19]\r\n [20 21]\r\n [22 23]\r\n [24 25]\r\n [26 27]\r\n [28 29]\r\n [30 31]\r\n [32 33]\r\n [34 35]\r\n [36 37]\r\n [38 39]\r\n [40 41]\r\n [42 43]\r\n [44 45]\r\n [46 47]\r\n [48 49]\r\n [50 51]\r\n [52 53]\r\n [54 55]\r\n [56 57]\r\n [58 59]\r\n [60 61]\r\n [62 63]\r\n [64 65]\r\n [66 67]\r\n [68 69]\r\n [70 71]\r\n [72 73]\r\n [74 75]\r\n [76 77]\r\n [78 79]\r\n [80 81]\r\n [82 83]\r\n [84 85]\r\n [86 87]\r\n [88 89]\r\n [90 91]\r\n [92 93]\r\n [94 95]\r\n [96 97]\r\n [98 99]]\r\nprint(target)\r\n[0 0 0 0 0 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 4 4 4 4 4 5 5 5 5 5 6 6 6 6 6 7 7\r\n 7 7 7 8 8 8 8 8 9 9 9 9 9]\r\n\r\n\r\nlion = LionDataset(data=data, target=target)\r\nprint(lion[0:4])\r\n(array([[0, 1],\r\n       [2, 3],\r\n       [4, 5],\r\n       [6, 7]]), array([0, 0, 0, 0]))\r\nprint(len(lion))\r\n50\r\n\r\nDataLoader\r\nDataLoader는 dataset을 batch 단위로 묶어주는 역할을 합니다.\r\nbatch_size : batch_size\r\nshuffle : True시 epoch마다 데이터가 학습되는 순서가 섞임\r\n\r\nloader = DataLoader(dataset=lion, batch_size=10, shuffle=True)\r\n\r\nfor i, batch in enumerate(loader):\r\n    x, y = batch\r\n    if i == 0:\r\n        print(x)\r\n        print(y)\r\n    print(x.shape)\r\ntensor([[76, 77],\r\n        [ 0,  1],\r\n        [16, 17],\r\n        [64, 65],\r\n        [42, 43],\r\n        [82, 83],\r\n        [86, 87],\r\n        [ 2,  3],\r\n        [36, 37],\r\n        [70, 71]], dtype=torch.int32)\r\ntensor([7, 0, 1, 6, 4, 8, 8, 0, 3, 7], dtype=torch.int32)\r\ntorch.Size([10, 2])\r\ntorch.Size([10, 2])\r\ntorch.Size([10, 2])\r\ntorch.Size([10, 2])\r\ntorch.Size([10, 2])\r\n\r\ntorch.nn.Module\r\npytorch 모델은 parameters를 추적하며 forward pass를 진행한 뒤 back\r\npropagation을 통해 학습을 진행합니다. torch.nn.Module은 여러 층의\r\nlayer로 이뤄진 모델을 쉽게 관리할 수 있는 class입니다.\r\npytorch 모델의 기본구조\r\n\r\nclass Model_Name(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        \"\"\"\r\n        모델에 사용될 Layer(nn.Linear, nn.Conv2d)와 \r\n        activation function(nn.functional.relu, nn.functional.sigmoid)등을 정의\r\n        \"\"\"\r\n\r\n    def forward(self, x):\r\n        \"\"\"\r\n        모델에서 실행되어야하는 계산을 정의\r\n        \"\"\"\r\n        return x\r\n\r\nexample\r\n\r\nclass LionLinear(nn.Module):\r\n    def __init__(self, input_dim, output_dim):\r\n        super().__init__()\r\n        self.input_dim = input_dim   # 입력차원 \r\n        self.output_dim = output_dim # 출력차원\r\n        \r\n        self.flatten = nn.Flatten()  # tensor 평탄화 정의\r\n        self.classifier = nn.Linear(input_dim, output_dim) # Linear layer 정의\r\n        self.act = nn.ReLU() # activation function(ReLU) 정의\r\n        \r\n    def forward(self, x):\r\n        x = self.flatten(x)    # data를 linear layers에 맞게 평탄화 후\r\n        x = self.classifier(x) # linear layer를 통과,\r\n        x = self.act(x)        # activation function을 통해 출력\r\n        \r\n        return x\r\n\r\npytorch 모델은 모델의 구조를 쉽게 파악할 수 있습니다.\r\n\r\nlinear_model = LionLinear(28*28, 10).to(device)\r\nprint(linear_model)\r\nLionLinear(\r\n  (flatten): Flatten(start_dim=1, end_dim=-1)\r\n  (classifier): Linear(in_features=784, out_features=10, bias=True)\r\n  (act): ReLU()\r\n)\r\nprint(linear_model.classifier)\r\nLinear(in_features=784, out_features=10, bias=True)\r\n\r\n다음은 MLP를 구현하는 module입니다.\r\n코드를 더 간결하게 하기 위해 다음과 같이 일부 layer 등을 따로 모듈로\r\n구현 후 전체 모듈에 합쳐서 구현할 수 있습니다.\r\n\r\nclass LionLayer(nn.Module):\r\n    def __init__(self, input_dim, output_dim):\r\n        super().__init__()\r\n        self.input_dim = input_dim\r\n        self.output_dim = output_dim\r\n        self.layer = nn.Linear(self.input_dim, self.output_dim)\r\n        pass\r\n\r\n    def forward(self, x):\r\n        assert x.shape[-1] == self.input_dim, \"Input dimension mismatch\"\r\n        return self.layer(x)\r\n\r\n\r\nclass LionMLP(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim):\r\n        super().__init__()\r\n        self.input_dim = input_dim\r\n        self.hidden_dim = hidden_dim\r\n        self.output_dim = output_dim\r\n        self.flatten = nn.Flatten()\r\n        self.linear_1 = LionLayer(self.input_dim, self.hidden_dim)\r\n        self.linear_2 = LionLayer(self.hidden_dim, self.output_dim)\r\n        self.act_1 = nn.ReLU()\r\n        self.act_2 = nn.Softmax()\r\n        pass\r\n    \r\n    def forward(self, x):\r\n        x = self.flatten(x)\r\n        x = self.act_1(self.linear_1(x))\r\n        x = self.act_2(self.linear_2(x))\r\n        return x\r\n\r\n\r\nmlp = LionMLP(28*28, 50, 10)\r\nprint(mlp)\r\nLionMLP(\r\n  (flatten): Flatten(start_dim=1, end_dim=-1)\r\n  (linear_1): LionLayer(\r\n    (layer): Linear(in_features=784, out_features=50, bias=True)\r\n  )\r\n  (linear_2): LionLayer(\r\n    (layer): Linear(in_features=50, out_features=10, bias=True)\r\n  )\r\n  (act_1): ReLU()\r\n  (act_2): Softmax(dim=None)\r\n)\r\n\r\nnn.Sequential()을 통해 forward() 부분을 짧게 작성할 수 있습니다.\r\n\r\nclass LionMLP(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim):\r\n        super().__init__()\r\n        self.input_dim = input_dim\r\n        self.hidden_dim = hidden_dim\r\n        self.output_dim = output_dim\r\n        self.flatten = nn.Flatten()\r\n        self.linear_1 = LionLayer(self.input_dim, self.hidden_dim)\r\n        self.linear_2 = LionLayer(self.hidden_dim, self.output_dim)\r\n        self.act_1 = nn.ReLU()\r\n        self.act_2 = nn.Softmax()\r\n        self.model = nn.Sequential(self.flatten, self.linear_1, self.act_1, self.linear_2, self.act_2)\r\n        pass\r\n    def forward(self, x):\r\n        return self.model(x)\r\n\r\n\r\nmlp = LionMLP(28 * 28, 40, 10)\r\nprint(mlp)\r\nLionMLP(\r\n  (flatten): Flatten(start_dim=1, end_dim=-1)\r\n  (linear_1): LionLayer(\r\n    (layer): Linear(in_features=784, out_features=40, bias=True)\r\n  )\r\n  (linear_2): LionLayer(\r\n    (layer): Linear(in_features=40, out_features=10, bias=True)\r\n  )\r\n  (act_1): ReLU()\r\n  (act_2): Softmax(dim=None)\r\n  (model): Sequential(\r\n    (0): Flatten(start_dim=1, end_dim=-1)\r\n    (1): LionLayer(\r\n      (layer): Linear(in_features=784, out_features=40, bias=True)\r\n    )\r\n    (2): ReLU()\r\n    (3): LionLayer(\r\n      (layer): Linear(in_features=40, out_features=10, bias=True)\r\n    )\r\n    (4): Softmax(dim=None)\r\n  )\r\n)\r\n\r\n모델 파라미터 확인\r\n\r\nfor name, param in mlp.named_parameters():\r\n    print(f\"Layer: {name} | Size: {param.size()} | Values: {param[:2]} \\n\")\r\nLayer: linear_1.layer.weight | Size: torch.Size([40, 784]) | Values: tensor([[ 0.0223,  0.0277,  0.0059,  ...,  0.0214, -0.0128,  0.0094],\r\n        [ 0.0003, -0.0235, -0.0014,  ...,  0.0134,  0.0252, -0.0157]],\r\n       grad_fn=<SliceBackward0>) \r\n\r\nLayer: linear_1.layer.bias | Size: torch.Size([40]) | Values: tensor([-0.0290,  0.0290], grad_fn=<SliceBackward0>) \r\n\r\nLayer: linear_2.layer.weight | Size: torch.Size([10, 40]) | Values: tensor([[-1.3287e-01,  1.5117e-01,  1.0537e-01,  7.8630e-02,  1.0438e-01,\r\n         -5.3689e-02, -1.2888e-01,  8.2394e-02, -1.3274e-01, -1.0242e-03,\r\n          9.0983e-03, -4.1699e-03, -1.1986e-02,  1.0621e-01,  1.3446e-01,\r\n         -1.3404e-01,  2.3344e-02,  4.0762e-02, -1.4608e-01, -9.6388e-02,\r\n          7.6862e-02, -1.4166e-01, -1.2884e-01, -1.0316e-01, -1.8399e-02,\r\n          7.4728e-02, -1.3642e-01, -8.7340e-02,  1.3541e-01, -1.5621e-01,\r\n         -3.3360e-02,  1.3945e-01, -1.2610e-01, -1.7838e-02,  8.0318e-02,\r\n          1.0165e-01, -2.3599e-02, -1.8348e-02,  1.2724e-04, -9.8836e-03],\r\n        [ 2.4786e-02,  7.4154e-02,  2.7105e-02, -1.1759e-01, -1.5434e-01,\r\n         -8.9445e-02, -1.6053e-02, -1.3646e-01,  9.9987e-02, -4.5860e-02,\r\n         -1.0728e-01,  1.4374e-01,  6.7146e-02, -7.9271e-03,  1.3654e-01,\r\n          3.3813e-02, -1.3032e-01, -3.9476e-02,  1.4638e-01, -1.4502e-01,\r\n         -9.8445e-02, -7.6872e-02, -1.3876e-01,  4.7213e-02, -6.8424e-02,\r\n         -7.1604e-02, -6.8757e-02,  1.0555e-01, -2.9994e-02,  1.4658e-01,\r\n         -4.7815e-02, -1.0488e-02, -7.3219e-02,  3.6112e-02, -6.6246e-02,\r\n         -1.9281e-02, -2.6451e-02,  6.8400e-02,  1.4304e-01,  1.3780e-01]],\r\n       grad_fn=<SliceBackward0>) \r\n\r\nLayer: linear_2.layer.bias | Size: torch.Size([10]) | Values: tensor([ 0.1439, -0.0609], grad_fn=<SliceBackward0>) \r\n\r\nreference\r\nPyTorch로 시작하는 딥 러닝 입문 : https://wikidocs.net/57165\r\nhttps://anweh.tistory.com/21\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-22T14:56:20+09:00",
    "input_file": "pytorch-tutorial.knit.md"
  },
  {
    "path": "til/2022-05-19-cotepart1/",
    "title": "[백준 문제풀이] 기본기",
    "description": "코딩테스트를 위한 백준 문제 풀어보기",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-19",
    "categories": [
      "coding test"
    ],
    "contents": "\r\n\r\nContents\r\n약수 구하기\r\n이진수\r\nbin() 직접 구현하기(양의\r\n정수일때만)\r\n\r\n최대, 최소\r\n\r\n약수 구하기\r\n두 개의 자연수 N과 K가 주어졌을 때, N의 약수들 중 K번째로 작은 수를\r\n출력하는 프로그램을 작성하시오.\r\n입력\r\n첫째 줄에 N과 K가 빈칸을 사이에 두고 주어진다. N은 1 이상 10,000\r\n이하이다. K는 1 이상 N 이하이다.\r\n출력\r\n첫째 줄에 N의 약수들 중 K번째로 작은 수를 출력한다. 만일 N의 약수의\r\n개수가 K개보다 적어서 K번째 약수가 존재하지 않을 경우에는 0을\r\n출력하시오.\r\n\r\nN, K = map(int, input().split())\r\n\r\nresult = 0\r\n\r\nfor i in range(1, N + 1):\r\n    if N % i == 0:\r\n        K -= 1\r\n        if K == 0:\r\n            result = i\r\n            break\r\n\r\nprint(result)\r\n\r\n이진수\r\n양의 정수 n이 주어졌을 때, 이를 이진수로 나타냈을 때 1의 위치를 모두\r\n찾는 프로그램을 작성하시오. 최하위 비트(least significant bit, lsb)의\r\n위치는 0이다.\r\n입력\r\n첫째 줄에 테스트 케이스의 개수 T가 주어진다. 각 테스트 케이스는 한\r\n줄로 이루어져 있고, n이 주어진다. \\((1 ≤ T ≤\r\n10, 1 ≤ n ≤ 10^6)\\)\r\n출력\r\n각 테스트 케이스에 대해서, 1의 위치를 공백으로 구분해서 줄 하나에\r\n출력한다. 위치가 낮은 것부터 출력한다.\r\n\r\nt = int(input())\r\n\r\nfor _ in range(t):\r\n    n = int(input())\r\n    b = bin(n)[2:]\r\n    \r\n    for i in range(len(b)):\r\n        if b[::-1][i] == '1':\r\n            print(i, end=' ')\r\n\r\nbin() 직접 구현하기(양의\r\n정수일때만)\r\n\r\ndef my_bin(num):\r\n    a = num//2\r\n    b = num%2\r\n    if a == 0:\r\n        return '1'\r\n    else:\r\n        return my_bin(a)+str(b)\r\n\r\n최대, 최소\r\nN개의 정수가 주어진다. 이때, 최솟값과 최댓값을 구하는 프로그램을\r\n작성하시오.\r\n입력\r\n첫째 줄에 정수의 개수 N (1 ≤ N ≤ 1,000,000)이 주어진다. 둘째 줄에는\r\nN개의 정수를 공백으로 구분해서 주어진다. 모든 정수는 -1,000,000보다\r\n크거나 같고, 1,000,000보다 작거나 같은 정수이다.\r\n출력\r\n첫째 줄에 주어진 정수 N개의 최솟값과 최댓값을 공백으로 구분해\r\n출력한다.\r\n\r\ninput()\r\nl = list(map(int, input().split()))\r\n\r\nhigh = low = l[0]\r\n\r\nfor i in l[1:]:\r\n    if high < i:\r\n        high = i\r\n    if low > i:\r\n        low = i\r\n        \r\nprint(low, high)\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-20T19:04:09+09:00",
    "input_file": {}
  },
  {
    "path": "til/2022-05-18-dlfromscratchch2/",
    "title": "퍼셉트론이란",
    "description": "밑바닥부터 시작하는 딥러닝을 읽고 딥러닝의 기원이 되는 알고리즘인 퍼셉트론에 대해 정리해봤습니다.",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-18",
    "categories": [
      "밑바닥부터 시작하는 딥러닝"
    ],
    "contents": "\r\n\r\nContents\r\n1️⃣ 퍼셉트론이란\r\n2️⃣ 퍼셉트론의 한계\r\n3️⃣ 다층 퍼셉트론\r\n✅ 요약\r\n\r\n1️⃣ 퍼셉트론이란\r\n퍼셉트론은 다수의 신호를 입력(input)으로 받아 하나의 신호를\r\n출력(output)하는 알고리즘입니다.\r\n2개의 입력을 받아 하나의 신호를 출력하는 퍼셉트론을 수식으로\r\n나타내면,\r\n\\[y = \\begin{cases}0,(w_{1}x_{1} +\r\nw_{2}x_{2}\\leq\\theta)\\\\1,(w_{1}x_{1} +\r\nw_{2}x_{2}>\\theta)\\\\\\end{cases}\\] 또는 \\(\\theta\\)를 \\(-b\\)로 치환하여,\r\n\\[y = \\begin{cases}0,(b+w_{1}x_{1} +\r\nw_{2}x_{2}\\leq0)\\\\1,(b+w_{1}x_{1} +\r\nw_{2}x_{2}>0)\\\\\\end{cases}\\]\r\n\\(w\\) : weight, \\(b\\): bias\r\n2️⃣ 퍼셉트론의 한계\r\n퍼셉트론이 풀 수 있는 (예측할 수 있는) 문제는 AND, NAND,\r\nOR입니다.\r\nAND\r\n\r\nx1x2y000100010111\r\n\r\nNAND\r\n\r\nx1x2y001101011110\r\n\r\nOR\r\n\r\nx1x2y000101011111\r\n\r\n위 세가지 문제는 \\((w_1, w_2,\r\ntheta)\\)에 각각 \\((0.5, 0.5, 0.7),\r\n(-0.5, -0.5, -0.7), (1.5, 1.5, 0.7)\\) 등을 대입하면 풀 수\r\n있습니다.\r\n이를 파이썬 함수로 구현하면,\r\n\r\ndef AND(x1, x2):\r\n    w1, w2, theta = 0.5, 0.5, 0.5\r\n    tmp = x1*w1+x2*w2\r\n    if tmp <= theta:\r\n        return 0\r\n    else:\r\n        return 1\r\n\r\ndef NAND(x1, x2):\r\n    w1, w2, theta = -0.5, -0.5, -0.7\r\n    tmp = x1*w1+x2*w2\r\n    if tmp <= theta:\r\n        return 0\r\n    else:\r\n        return 1\r\n    \r\ndef OR(x1, x2):\r\n    w1, w2, theta = 1.5, 1.5, 0.7\r\n    tmp = x1*w1+x2*w2\r\n    if tmp <= theta:\r\n        return 0\r\n    else:\r\n        return 1\r\n\r\nprint('x1=0, x2=0 : ', AND(0,0))\r\nx1=0, x2=0 :  0\r\nprint('x1=0, x2=1 : ', AND(0,1))\r\nx1=0, x2=1 :  0\r\nprint('x1=1, x2=0 : ', AND(1,0))\r\nx1=1, x2=0 :  0\r\nprint('x1=1, x2=1 : ', AND(1,1))\r\nx1=1, x2=1 :  1\r\nprint('x1=0, x2=0 : ', NAND(0,0))\r\nx1=0, x2=0 :  1\r\nprint('x1=0, x2=1 : ', NAND(0,1))\r\nx1=0, x2=1 :  1\r\nprint('x1=1, x2=0 : ', NAND(1,0))\r\nx1=1, x2=0 :  1\r\nprint('x1=1, x2=1 : ', NAND(1,1))\r\nx1=1, x2=1 :  0\r\nprint('x1=0, x2=0 : ', OR(0,0))\r\nx1=0, x2=0 :  0\r\nprint('x1=0, x2=1 : ', OR(0,1))\r\nx1=0, x2=1 :  1\r\nprint('x1=1, x2=0 : ', OR(1,0))\r\nx1=1, x2=0 :  1\r\nprint('x1=1, x2=1 : ', OR(1,1))\r\nx1=1, x2=1 :  1\r\n\r\n하지만, 퍼셉트론으로 XOR은 해결할 수 없습니다.\r\nXOR\r\n\r\nx1x2y000101011110\r\n\r\n위 문제를 그림으로 나타내면 다음과 같이 빨간점과 검은점을 구분할 수\r\n있는 선을 그을 수 있어야 합니다.\r\n\r\n\r\n\r\n곡선으로 선을 그린다면 구분할 수 있겠지만 퍼셉트론으로는 직선밖에\r\n그릴 수 없으므로 해결할 수 없습니다. 즉, 퍼셉트론은 비선형의 영역을\r\n표현할 수 없습니다.\r\n3️⃣ 다층 퍼셉트론\r\n이를 해결하기 위해 나온 것이 다층 퍼셉트론입니다. 말 그대로 퍼셉트론\r\n층을 여러 개 갖는 알고리즘이며 앞에서 언급했던 퍼셉트론은 정확히 말하면\r\n단층 퍼셉트론입니다.\r\nXOR문제를 해결하려면 위에서 AND, NAND, OR 문제를 해결하기 위해\r\n만들었던 퍼셉트론을 적절히 쌓아 만들면 됩니다.\r\n\r\ndef XOR(x1, x2):\r\n    s1 = NAND(x1, x2)\r\n    s2 = OR(x1, x2)\r\n    y = AND(s1, s2)\r\n    return y\r\n\r\nprint('x1=0, x2=0 : ', XOR(0,0))\r\nx1=0, x2=0 :  0\r\nprint('x1=0, x2=1 : ', XOR(0,1))\r\nx1=0, x2=1 :  1\r\nprint('x1=1, x2=0 : ', XOR(1,0))\r\nx1=1, x2=0 :  1\r\nprint('x1=1, x2=1 : ', XOR(1,1))\r\nx1=1, x2=1 :  0\r\n\r\n첫번째 층에 NAND, OR를 풀기 위해 만들었던 퍼셉트론을 쌓고 두번째 층에\r\nAND를 풀기 위해 만들었던 퍼셉트론을 쌓아 XOR문제를 해결했습니다.\r\n이처럼 다층 퍼셉트론은 비선형의 영역까지도 표현할 수 있음을 알 수\r\n있습니다.\r\n✅ 요약\r\n퍼셉트론은 입출력을 갖춘 알고리즘\r\n단층 퍼셉트론은 선형 영역만 표현할 수 있고, 다층 퍼셉트론은\r\n비선형 영역도 표현 가능\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-19T00:59:30+09:00",
    "input_file": {}
  },
  {
    "path": "til/2022-05-13-first-til/",
    "title": "TIL(Today I Learn) 페이지 개설!!",
    "description": "매일 공부하는 것을 기록하기",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-13",
    "categories": [],
    "contents": "\r\n매일매일 공부한 것을 기록하는 페이지를 만들었다. 일기처럼 나중에 봐도\r\n뿌듯한 페이지가 되길..\r\n😄화이팅!!\r\n\r\n\r\n\r\n",
    "preview": "til/2022-05-13-first-til/images/owl.png",
    "last_modified": "2022-05-16T00:44:05+09:00",
    "input_file": {},
    "preview_width": 640,
    "preview_height": 514
  }
]

[
  {
    "path": "til/2022-05-27-mnistclassification/",
    "title": "pytorch로 mnist 분류하기",
    "description": "pytorch로 mnist 분류 MLP모델을 구현하고 optimizer, lr scheduler, regularization, dropout, batch normalization의 적용 전 후 성능차이를 비교해봤습니다.",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-27",
    "categories": [
      "pytorch"
    ],
    "contents": "\r\n\r\nContents\r\nMnist 데이터 로드\r\n모델 구현\r\nTrain\r\nTest\r\nsplit train vaild\r\nTrainer class 구현\r\n_ _ init _ _ ()\r\n_get_optimizer()\r\n_get_scheduler()\r\ntrain()\r\n\r\nopimizer 비교 (sgd, adam)\r\nSGD\r\nadam\r\n\r\nlearning rate\r\nscheduler 사용 유무 비교\r\ndropout, batch\r\nnormalization\r\n\r\nsocar 부트캠프의 강의 중 일부를 정리한 내용입니다.\r\ntorch를 이용하여 MLP 모델을 구현했습니다. 또한 여러 종류의 optimizer,\r\nlr scheduler, regularization, dropout, batch normalization을 사용해보고\r\n어떤 효과가 있는지 실험을 하며 비교해봤습니다.\r\nimport os \r\n \r\nimport torch \r\nfrom torch import nn \r\nfrom torch.utils.data import Dataset, DataLoader \r\nfrom torchvision import datasets, transforms \r\n \r\nimport matplotlib.pyplot as plt \r\ngpu가 없지만 있다고 가정하고 구현했습니다.\r\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\" \r\ndevice \r\n'cpu' \r\nMnist 데이터 로드\r\ndatasets.MNIST()을 이용하여 mnist 데이터를 저장 후 로드합니다. root에\r\n이미 저장이 되있다면 로드만 됩니다.\r\n# 저장될 데이터 위치 지정 \r\nroot = os.path.join(os.getcwd(), \"data\") \r\nroot \r\n'/home/nackta/python_project/socar_bootcamp/data' \r\nmnist_train = datasets.MNIST( \r\n    root=root,  \r\n    train=True,  \r\n    download=True,  \r\n    transform=transforms.ToTensor() \r\n) \r\nmnist_test = datasets.MNIST( \r\n    root=root,  \r\n    train=False,  \r\n    download=True,  \r\n    transform=transforms.ToTensor() \r\n)  \r\nmnist train 데이터는 60000개, test 데이터는 10000개임을 알 수\r\n있습니다.\r\nprint(mnist_train) \r\nprint(mnist_test) \r\nDataset MNIST \r\n    Number of datapoints: 60000 \r\n    Root location: /home/nackta/python_project/socar_bootcamp/data \r\n    Split: Train \r\n    StandardTransform \r\nTransform: ToTensor() \r\nDataset MNIST \r\n    Number of datapoints: 10000 \r\n    Root location: /home/nackta/python_project/socar_bootcamp/data \r\n    Split: Test \r\n    StandardTransform \r\nTransform: ToTensor() \r\nmnist data에는 feature data와 label data가 tuple로 한 쌍으로 묶여져\r\n있습니다.\r\nfeature의 데이터 하나당 size는 (1x28x28)임을 알 수 있으며 매칭되는\r\nlabel data로 정수가 있습니다.\r\nsample_x, sample_y = mnist_train[0] \r\nprint(sample_x.shape) \r\nprint(sample_y) \r\ntorch.Size([1, 28, 28]) \r\n5 \r\n라벨 데이터의 범위는 0~9까지의 정수입니다.\r\nmnist_train.targets.unique() \r\ntensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) \r\ndef plot_mnist(data, figsize=(20, 10)): \r\n    fig = plt.figure(figsize=figsize) \r\n    for i in range(18): \r\n        img = data[i][0] \r\n        ax = fig.add_subplot(3, 6, i+1) # 3x6, i+1번째 자리에 plot \r\n        ax.imshow(img.reshape(28, 28), cmap=\"gray\") \r\n        ax.set_title(f\"Label: {data[i][1]}\") \r\n    fig.show() \r\n    pass \r\nplot_mnist(data=mnist_train) \r\npng이처럼 mnist 데이터는 feature data로 손글씨 이미지가 있고 그에 맞게\r\nlabel data가 있습니다.\r\n그럼 DataLoader를 통해 batch size를 64로 지정하겠습니다.\r\ndrop_last=True이면 전체 데이터를 배치단위로 묶은 뒤 남은 데이터를 버리게\r\n됩니다.\r\ntrain_loader = DataLoader(dataset=mnist_train, batch_size=64, shuffle=True, drop_last=True) \r\ntest_loader = DataLoader(dataset=mnist_test, batch_size=64, shuffle=False, drop_last=True) \r\n각 배치의 shape은 64x1x28x28인 feature data와 64개의 label data로\r\n이뤄져 있고 총 937의 배치가 만들어졌습니다.\r\nfor i, batch in enumerate(train_loader): \r\n    x, y = batch \r\n    if i == 0: \r\n        print(x.shape) \r\n        print(y) \r\nprint(i+1) \r\ntorch.Size([64, 1, 28, 28]) \r\ntensor([0, 2, 0, 0, 1, 7, 3, 2, 4, 5, 8, 8, 4, 0, 9, 8, 3, 1, 3, 3, 5, 1, 4, 7, \r\n        0, 4, 8, 4, 9, 4, 3, 7, 9, 7, 6, 1, 3, 8, 7, 0, 5, 1, 1, 3, 3, 0, 6, 3, \r\n        2, 2, 2, 5, 4, 5, 5, 7, 3, 7, 8, 3, 0, 7, 9, 9]) \r\n937 \r\nlen(train_loader) \r\n937 \r\n모델 구현\r\n기본적인 mlp 모델을 구현해보겠습니다. 2차원 input데이터를 Flatten()을\r\n통해 1차원 행렬로 변환 후 2개의 은닉층을 거쳐 10개의 값을 출력하는\r\n모델을 만듭니다.\r\nclass LionMNISTClassifier(nn.Module): \r\n    def __init__(self, n_class=10): \r\n        super().__init__() \r\n \r\n        self.model = nn.Sequential( \r\n            nn.Flatten(), \r\n            nn.Linear(28 * 28, 256), \r\n            nn.ReLU(), \r\n            nn.Linear(256, 128), \r\n            nn.ReLU(), \r\n            nn.Linear(128, n_class), \r\n        ) \r\n \r\n    def forward(self, x): \r\n        return self.model(x) \r\nmlp = LionMNISTClassifier().to(device) # 모델을 gpu에서 계산할 수 있도록 지정 \r\nmlp \r\nLionMNISTClassifier( \r\n  (model): Sequential( \r\n    (0): Flatten(start_dim=1, end_dim=-1) \r\n    (1): Linear(in_features=784, out_features=256, bias=True) \r\n    (2): ReLU() \r\n    (3): Linear(in_features=256, out_features=128, bias=True) \r\n    (4): ReLU() \r\n    (5): Linear(in_features=128, out_features=10, bias=True) \r\n  ) \r\n) \r\nloss 함수는 CrossEntropy, optimizer는 SGD, learning rate는 0.001을\r\n사용합니다\r\nloss_fn = nn.CrossEntropyLoss() \r\noptim = torch.optim.SGD(mlp.parameters(), lr=0.001) \r\nTrain\r\ntrain 과정에서 가장 핵심인 부분은 loss.backward(), optim.step(),\r\noptim.zero_grad()로 이어지는 3단계입니다.\r\nloss.backward()\r\n계산된 loss 텐서에 .backward()를 실행하면 역전파가 실행됩니다. loss\r\n안에 pred의 각 parameters에 대한 변화도를 계산하고 저장합니다.\r\noptim.step()\r\n저장된 변화도에 따라 각 parameters를 조정합니다.\r\noptim.zero_grad()\r\n각 parameters에 저장된 변화도를 0으로 초기화합니다. 초기화하지 않으면\r\n변화도가 누적되어 계산됩니다.\r\nnum_epochs = 20 \r\ntrain_history = {\"loss\": [], \"acc\": []} \r\nfor e in range(num_epochs): \r\n    print(f\"Epoch: {e} start.\") \r\n    epoch_loss, epoch_acc = 0, 0 \r\n    size = len(train_loader.dataset) \r\n    for b, (x, y) in enumerate(train_loader): \r\n        x.to(device) \r\n        y.to(device) \r\n \r\n        pred = mlp(x) \r\n        loss = loss_fn(pred, y) \r\n \r\n        optim.zero_grad() # parameters의 변화량을 0으로 초기화 \r\n        loss.backward() # 에러의 변화도를 계산하여 저장 \r\n        optim.step() # loss를 최소화 할 수 있도록 parameters 수정 \r\n \r\n        loss = loss.to(\"cpu\").item() \r\n        acc = (pred.argmax(1) == y).type(torch.float).to(\"cpu\").mean().item() \r\n \r\n        if b % 200 == 0: \r\n            print(f\"loss: {loss:>7f}, acc: {acc:>4f} [{b * len(x)}/{size}]\") \r\n \r\n        epoch_loss += loss \r\n        epoch_acc += acc \r\n \r\n    train_history[\"loss\"].append(epoch_loss / len(train_loader)) \r\n    train_history[\"acc\"].append(epoch_acc / len(train_loader)) \r\nEpoch: 0 start. \r\nloss: 2.306067, acc: 0.093750 [0/60000] \r\nloss: 2.307073, acc: 0.046875 [12800/60000] \r\nloss: 2.293016, acc: 0.109375 [25600/60000] \r\nloss: 2.285884, acc: 0.140625 [38400/60000] \r\nloss: 2.294895, acc: 0.156250 [51200/60000] \r\nEpoch: 1 start. \r\nloss: 2.277667, acc: 0.218750 [0/60000] \r\nloss: 2.266313, acc: 0.250000 [12800/60000] \r\nloss: 2.267338, acc: 0.250000 [25600/60000] \r\nloss: 2.254096, acc: 0.296875 [38400/60000] \r\nloss: 2.242105, acc: 0.453125 [51200/60000] \r\nEpoch: 2 start. \r\nloss: 2.255541, acc: 0.390625 [0/60000] \r\nloss: 2.236494, acc: 0.484375 [12800/60000] \r\nloss: 2.213965, acc: 0.500000 [25600/60000] \r\nloss: 2.202532, acc: 0.593750 [38400/60000] \r\nloss: 2.200587, acc: 0.437500 [51200/60000] \r\nEpoch: 3 start. \r\nloss: 2.181767, acc: 0.578125 [0/60000] \r\nloss: 2.161530, acc: 0.640625 [12800/60000] \r\nloss: 2.147703, acc: 0.625000 [25600/60000] \r\nloss: 2.144805, acc: 0.593750 [38400/60000] \r\nloss: 2.088173, acc: 0.656250 [51200/60000] \r\nEpoch: 4 start. \r\nloss: 2.067614, acc: 0.671875 [0/60000] \r\nloss: 2.047233, acc: 0.671875 [12800/60000] \r\nloss: 2.004169, acc: 0.718750 [25600/60000] \r\nloss: 1.925291, acc: 0.750000 [38400/60000] \r\nloss: 1.996897, acc: 0.593750 [51200/60000] \r\nEpoch: 5 start. \r\nloss: 1.886086, acc: 0.703125 [0/60000] \r\nloss: 1.824186, acc: 0.718750 [12800/60000] \r\nloss: 1.785833, acc: 0.750000 [25600/60000] \r\nloss: 1.820181, acc: 0.593750 [38400/60000] \r\nloss: 1.707675, acc: 0.734375 [51200/60000] \r\nEpoch: 6 start. \r\nloss: 1.637798, acc: 0.703125 [0/60000] \r\nloss: 1.547363, acc: 0.656250 [12800/60000] \r\nloss: 1.555369, acc: 0.734375 [25600/60000] \r\nloss: 1.479918, acc: 0.781250 [38400/60000] \r\nloss: 1.312469, acc: 0.796875 [51200/60000] \r\nEpoch: 7 start. \r\nloss: 1.329046, acc: 0.718750 [0/60000] \r\nloss: 1.342677, acc: 0.765625 [12800/60000] \r\nloss: 1.278799, acc: 0.765625 [25600/60000] \r\nloss: 1.217570, acc: 0.734375 [38400/60000] \r\nloss: 1.073273, acc: 0.781250 [51200/60000] \r\nEpoch: 8 start. \r\nloss: 1.050970, acc: 0.765625 [0/60000] \r\nloss: 0.996948, acc: 0.718750 [12800/60000] \r\nloss: 0.857504, acc: 0.843750 [25600/60000] \r\nloss: 0.988426, acc: 0.750000 [38400/60000] \r\nloss: 0.861934, acc: 0.812500 [51200/60000] \r\nEpoch: 9 start. \r\nloss: 0.766343, acc: 0.875000 [0/60000] \r\nloss: 0.836710, acc: 0.812500 [12800/60000] \r\nloss: 0.742993, acc: 0.828125 [25600/60000] \r\nloss: 0.921689, acc: 0.765625 [38400/60000] \r\nloss: 0.722942, acc: 0.843750 [51200/60000] \r\nEpoch: 10 start. \r\nloss: 0.701428, acc: 0.843750 [0/60000] \r\nloss: 0.671582, acc: 0.875000 [12800/60000] \r\nloss: 0.853458, acc: 0.765625 [25600/60000] \r\nloss: 0.622920, acc: 0.875000 [38400/60000] \r\nloss: 0.490978, acc: 0.937500 [51200/60000] \r\nEpoch: 11 start. \r\nloss: 0.700725, acc: 0.750000 [0/60000] \r\nloss: 0.636329, acc: 0.875000 [12800/60000] \r\nloss: 0.546823, acc: 0.875000 [25600/60000] \r\nloss: 0.531841, acc: 0.875000 [38400/60000] \r\nloss: 0.744622, acc: 0.828125 [51200/60000] \r\nEpoch: 12 start. \r\nloss: 0.895801, acc: 0.718750 [0/60000] \r\nloss: 0.559750, acc: 0.828125 [12800/60000] \r\nloss: 0.458211, acc: 0.921875 [25600/60000] \r\nloss: 0.624367, acc: 0.859375 [38400/60000] \r\nloss: 0.529751, acc: 0.890625 [51200/60000] \r\nEpoch: 13 start. \r\nloss: 0.457774, acc: 0.828125 [0/60000] \r\nloss: 0.428744, acc: 0.890625 [12800/60000] \r\nloss: 0.594997, acc: 0.843750 [25600/60000] \r\nloss: 0.552424, acc: 0.859375 [38400/60000] \r\nloss: 0.562785, acc: 0.890625 [51200/60000] \r\nEpoch: 14 start. \r\nloss: 0.538646, acc: 0.843750 [0/60000] \r\nloss: 0.445669, acc: 0.875000 [12800/60000] \r\nloss: 0.538240, acc: 0.890625 [25600/60000] \r\nloss: 0.518024, acc: 0.890625 [38400/60000] \r\nloss: 0.471751, acc: 0.875000 [51200/60000] \r\nEpoch: 15 start. \r\nloss: 0.483013, acc: 0.828125 [0/60000] \r\nloss: 0.470244, acc: 0.890625 [12800/60000] \r\nloss: 0.593319, acc: 0.828125 [25600/60000] \r\nloss: 0.383731, acc: 0.937500 [38400/60000] \r\nloss: 0.705848, acc: 0.812500 [51200/60000] \r\nEpoch: 16 start. \r\nloss: 0.380595, acc: 0.906250 [0/60000] \r\nloss: 0.301426, acc: 0.921875 [12800/60000] \r\nloss: 0.450054, acc: 0.921875 [25600/60000] \r\nloss: 0.438868, acc: 0.875000 [38400/60000] \r\nloss: 0.573046, acc: 0.843750 [51200/60000] \r\nEpoch: 17 start. \r\nloss: 0.500203, acc: 0.875000 [0/60000] \r\nloss: 0.595378, acc: 0.812500 [12800/60000] \r\nloss: 0.625601, acc: 0.843750 [25600/60000] \r\nloss: 0.523467, acc: 0.828125 [38400/60000] \r\nloss: 0.544994, acc: 0.859375 [51200/60000] \r\nEpoch: 18 start. \r\nloss: 0.590928, acc: 0.875000 [0/60000] \r\nloss: 0.597640, acc: 0.890625 [12800/60000] \r\nloss: 0.659121, acc: 0.781250 [25600/60000] \r\nloss: 0.361116, acc: 0.906250 [38400/60000] \r\nloss: 0.741920, acc: 0.843750 [51200/60000] \r\nEpoch: 19 start. \r\nloss: 0.407344, acc: 0.875000 [0/60000] \r\nloss: 0.564885, acc: 0.843750 [12800/60000] \r\nloss: 0.557660, acc: 0.796875 [25600/60000] \r\nloss: 0.335187, acc: 0.921875 [38400/60000] \r\nloss: 0.486878, acc: 0.812500 [51200/60000] \r\n훈련이 진행될수록 loss는 감소하고 정확도는 증가했습니다.\r\nfig = plt.figure(figsize=(10, 10)) \r\nax = fig.add_subplot(1, 1, 1) \r\nax.plot(train_history[\"acc\"], color=\"red\", label=\"acc\") \r\nax.plot(train_history[\"loss\"], color=\"blue\", label=\"loss\") \r\nfig.show() \r\npngTest\r\ntrain을 통해 최적화된 model로 test data를 예측합니다. 여기서 with\r\ntorch.no_grad()문 안에서 실행되는 pytorch는 autograd engine 기능을\r\n중단합니다. 이를 통해 모델의 parameters가 변화하는 것을 방지 할 수 있고\r\n메모리 사용량을 줄이고 연산속도을 높일 수 있습니다.\r\nprint(\"Test start.\") \r\ntest_loss, test_acc = 0, 0 \r\nsize = len(test_loader.dataset) \r\n \r\nwith torch.no_grad(): \r\n    for b, (x, y) in enumerate(test_loader): \r\n        x.to(device) \r\n        y.to(device) \r\n \r\n        pred = mlp(x) \r\n        test_loss += loss_fn(pred, y).to(\"cpu\").item() \r\n        test_acc += (pred.argmax(1) == y).type(torch.float).to(\"cpu\").mean().item() \r\n \r\n    test_loss /= len(test_loader) \r\n    test_acc /= len(test_loader) \r\n \r\nprint(f\"test loss: {test_loss:>7f}, test_acc: {test_acc:>4f}.\") \r\nTest start. \r\ntest loss: 0.409632, test_acc: 0.886719. \r\n지금까지는 각 노드의 parameters를 최적화하여 모델의 성능을\r\n높였습니다. 하지만 모델의 성능을 높이기 위해 optimizer, lr scheduler,\r\nregularization, dropout과 batch normalization 등을 적용할 수\r\n있습니다.\r\nsplit train vaild\r\nclass mnist_dataset(Dataset): \r\n    def __init__(self, data, targets, transform=None, target_transform=None): \r\n        super().__init__() \r\n \r\n        self.data = data \r\n        self.targets = targets \r\n        self.transform = transform \r\n        self.target_transform = target_transform \r\n        pass \r\n \r\n    def __len__(self): \r\n        return len(self.data) \r\n \r\n    def __getitem__(self, idx): \r\n        x, y = self.data[idx], self.targets[idx] \r\n \r\n        if self.transform: \r\n            x = self.transform(x) \r\n        if self.target_transform: \r\n            y = self.target_transform(y) \r\n \r\n        return x, y \r\n \r\ndef split_train_valid(dataset, valid_ratio=0.1): \r\n    n_valid = int(len(dataset) * valid_ratio) \r\n     \r\n    train_data = dataset.data[:-n_valid].numpy() \r\n    valid_data = dataset.data[-n_valid:].numpy() \r\n    train_targets = dataset.targets[:-n_valid] \r\n    valid_targets = dataset.targets[-n_valid:] \r\n \r\n    train = mnist_dataset(data=train_data, targets=train_targets, transform=dataset.transform, target_transform=dataset.target_transform) \r\n    valid = mnist_dataset(data=valid_data, targets=valid_targets, transform=dataset.transform, target_transform=dataset.target_transform) \r\n \r\n    return train, valid \r\nmnist_train, mnist_valid = split_train_valid(dataset=mnist_train) \r\ntrain_loader = DataLoader(dataset=mnist_train, batch_size= 64, shuffle = True, drop_last=True) \r\nvalid_loader = DataLoader(dataset=mnist_valid, batch_size= 64, shuffle = False, drop_last=True) \r\ntest_loader = DataLoader(dataset=mnist_test, batch_size= 64, shuffle = False, drop_last=True) \r\n \r\nloaders = { \r\n    'train' : train_loader, \r\n    'valid' : valid_loader, \r\n    'test' : test_loader \r\n} \r\nTrainer class 구현\r\n_ _ init _ _ ()\r\nmodel, train/valid/test로 나눠진 dataloder, optimizer 함수, learning\r\nrate, learning rate scheduler 사용 여부를 받아서 정의합니다.\r\n_get_optimizer()\r\noptimizer 함수를 정의합니다.\r\n_get_scheduler()\r\nlearning rate scheduler를 정의합니다. StepLR()는 일정한 Step 마다\r\nlearning rate에 gamma를 곱해주는 방식입니다.\r\ntrain()\r\nepoch 마다 훈련과 검증을 실시하고 loss와 accuracy를 저장합니다.\r\nclass Trainer(nn.Module): \r\n    def __init__(self, model_class, loaders, n_class=10, opt=\"sgd\", lr=0.001, has_scheduler=False, device=\"cpu\"): \r\n        super().__init__() \r\n \r\n        self.model = model_class(n_class=n_class) \r\n        self.loss = nn.CrossEntropyLoss() \r\n \r\n        self.train_loader = loaders[\"train\"] \r\n        self.valid_loader = loaders[\"valid\"] \r\n        self.test_loader = loaders[\"test\"] \r\n \r\n        self._get_optimizer(opt=opt.lower(), lr=lr) \r\n        self.has_scheduler = has_scheduler \r\n        if self.has_scheduler: \r\n            self._get_scheduler() \r\n \r\n        self.device = device \r\n        pass \r\n \r\n    def _get_optimizer(self, opt, lr=0.001): \r\n        if opt == \"sgd\": \r\n            self.optimizer = torch.optim.SGD( \r\n                params=self.model.parameters(), lr=lr) \r\n        elif opt == \"adam\": \r\n            self.optimizer = torch.optim.Adam( \r\n                params=self.model.parameters(), lr=lr) \r\n        else: \r\n            raise ValueError(f\"optimizer {opt} is not supproted\") \r\n \r\n    def _get_scheduler(self): \r\n        self.scheduler = torch.optim.lr_scheduler.StepLR( \r\n            optimizer=self.optimizer, step_size=5, gamma=0.5, verbose=True) \r\n \r\n    def train(self, max_epochs=10): \r\n        print(\"===== Train Start =====\") \r\n        history = {\"train_loss\": [], \"train_acc\": [], \r\n                   \"valid_loss\": [], \"valid_acc\": []} \r\n        for e in range(max_epochs): \r\n            train_loss, train_acc = self._train_epoch() \r\n            valid_loss, valid_acc = self._valid_epoch() \r\n \r\n            history[\"train_loss\"].append(train_loss) \r\n            history[\"train_acc\"].append(train_acc) \r\n            history[\"valid_loss\"].append(valid_loss) \r\n            history[\"valid_acc\"].append(valid_acc) \r\n \r\n            if self.has_scheduler: \r\n                self.scheduler.step() \r\n \r\n            if e % 5 == 0: \r\n                print( \r\n                    f\"Epoch: {e}, train loss: {train_loss:>6f}, train acc: {train_acc:>3f}, valid loss: {valid_loss:>6f}, valid acc: {valid_acc:>3f}\") \r\n \r\n        self.plot_history(history) \r\n \r\n    def _train_epoch(self): \r\n        epoch_loss, epoch_acc = 0, 0 \r\n        self.model.train() \r\n        for (x, y) in self.train_loader: \r\n            x = x.to(self.device) \r\n            y = y.to(self.device) \r\n \r\n            y_hat = self.model(x) \r\n            loss = self.loss(y_hat, y) \r\n \r\n            self.optimizer.zero_grad() \r\n            loss.backward() \r\n            self.optimizer.step() \r\n \r\n            epoch_loss += loss.to(\"cpu\").item() \r\n            epoch_acc += (y_hat.argmax(1) == \r\n                          y).type(torch.float).to(\"cpu\").mean().item() \r\n \r\n        epoch_loss /= len(self.train_loader) \r\n        epoch_acc /= len(self.train_loader) \r\n \r\n        return epoch_loss, epoch_acc \r\n \r\n    def _valid_epoch(self): \r\n        epoch_loss, epoch_acc = 0, 0 \r\n        self.model.eval() \r\n        with torch.no_grad(): \r\n            for (x, y) in self.valid_loader: \r\n                x = x.to(self.device) \r\n                y = y.to(self.device) \r\n \r\n                y_hat = self.model(x) \r\n                loss = self.loss(y_hat, y) \r\n \r\n                epoch_loss += loss.to(\"cpu\").item() \r\n                epoch_acc += (y_hat.argmax(1) == \r\n                              y).type(torch.float).to(\"cpu\").mean().item() \r\n \r\n        epoch_loss /= len(self.valid_loader) \r\n        epoch_acc /= len(self.valid_loader) \r\n \r\n        return epoch_loss, epoch_acc \r\n \r\n    def plot_history(self, history): \r\n        fig = plt.figure(figsize=(20, 10)) \r\n \r\n        ax = fig.add_subplot(1, 2, 1) \r\n        ax.plot(history[\"train_loss\"], color=\"red\", label=\"train loss\") \r\n        ax.plot(history[\"valid_loss\"], color=\"blue\", label=\"valid loss\") \r\n        ax.set_title(\"Loss\") \r\n        ax.legend() \r\n \r\n        ax = fig.add_subplot(1, 2, 2) \r\n        ax.plot(history[\"train_acc\"], color=\"red\", label=\"train acc\") \r\n        ax.plot(history[\"valid_acc\"], color=\"blue\", label=\"valid acc\") \r\n        ax.set_title(\"Acc\") \r\n        ax.legend() \r\n \r\n        fig.show() \r\n \r\n    def test(self): \r\n        print(\"===== Test Start =====\") \r\n        epoch_loss, epoch_acc = 0, 0 \r\n        self.model.eval() \r\n        with torch.no_grad(): \r\n            for (x, y) in self.test_loader: \r\n                x = x.to(self.device) \r\n                y = y.to(self.device) \r\n \r\n                y_hat = self.model(x) \r\n                loss = self.loss(y_hat, y) \r\n \r\n                epoch_loss += loss.to(\"cpu\").item() \r\n                epoch_acc += (y_hat.argmax(1) == \r\n                              y).type(torch.float).to(\"cpu\").mean().item() \r\n \r\n        epoch_loss /= len(self.test_loader) \r\n        epoch_acc /= len(self.test_loader) \r\n \r\n        print(f\"Test loss: {epoch_loss:>6f}, Test acc: {epoch_acc:>3f}\") \r\n \r\nopimizer 비교 (sgd, adam)\r\nSGD\r\ntrainer = Trainer(model_class=LionMNISTClassifier, loaders=loaders, n_class=10, opt=\"sgd\", lr=0.001, device=device).to(device) \r\ntrainer.train(max_epochs=50) \r\ntrainer.test() \r\n===== Train Start ===== \r\nEpoch: 0, train loss: 2.293853, train acc: 0.140866, valid loss: 2.280568, valid acc: 0.206149 \r\nEpoch: 5, train loss: 1.959832, train acc: 0.643127, valid loss: 1.855423, valid acc: 0.681788 \r\nEpoch: 10, train loss: 0.902655, train acc: 0.794799, valid loss: 0.778864, valid acc: 0.839550 \r\nEpoch: 15, train loss: 0.577160, train acc: 0.848291, valid loss: 0.483942, valid acc: 0.883401 \r\nEpoch: 20, train loss: 0.465347, train acc: 0.872461, valid loss: 0.383265, valid acc: 0.903058 \r\nEpoch: 25, train loss: 0.410083, train acc: 0.886343, valid loss: 0.335912, valid acc: 0.910282 \r\nEpoch: 30, train loss: 0.377565, train acc: 0.894276, valid loss: 0.308982, valid acc: 0.916667 \r\nEpoch: 35, train loss: 0.354993, train acc: 0.899726, valid loss: 0.289612, valid acc: 0.919691 \r\nEpoch: 40, train loss: 0.337928, train acc: 0.903321, valid loss: 0.276297, valid acc: 0.920867 \r\nEpoch: 45, train loss: 0.323637, train acc: 0.906880, valid loss: 0.265034, valid acc: 0.923723 \r\n===== Test Start ===== \r\nTest loss: 0.295112, Test acc: 0.914764 \r\npngadam\r\ntrainer = Trainer(model_class=LionMNISTClassifier, loaders=loaders, n_class=10, opt=\"adam\", lr=0.001, device=device).to(device) \r\ntrainer.train(max_epochs=50) \r\ntrainer.test() \r\n===== Train Start ===== \r\nEpoch: 0, train loss: 0.302497, train acc: 0.911477, valid loss: 0.114387, valid acc: 0.966566 \r\nEpoch: 5, train loss: 0.033073, train acc: 0.989417, valid loss: 0.066883, valid acc: 0.982527 \r\nEpoch: 10, train loss: 0.015452, train acc: 0.994921, valid loss: 0.093913, valid acc: 0.980679 \r\nEpoch: 15, train loss: 0.010835, train acc: 0.996664, valid loss: 0.107089, valid acc: 0.981183 \r\nEpoch: 20, train loss: 0.006893, train acc: 0.997924, valid loss: 0.132105, valid acc: 0.980175 \r\nEpoch: 25, train loss: 0.008649, train acc: 0.997294, valid loss: 0.121791, valid acc: 0.979839 \r\nEpoch: 30, train loss: 0.003595, train acc: 0.998869, valid loss: 0.136822, valid acc: 0.983031 \r\nEpoch: 35, train loss: 0.000891, train acc: 0.999778, valid loss: 0.126824, valid acc: 0.984039 \r\nEpoch: 40, train loss: 0.003926, train acc: 0.998647, valid loss: 0.164973, valid acc: 0.981351 \r\nEpoch: 45, train loss: 0.003763, train acc: 0.998869, valid loss: 0.165165, valid acc: 0.979335 \r\n===== Test Start ===== \r\nTest loss: 0.142515, Test acc: 0.980869 \r\npng그래프를 비교해보면 adam이 sgd보다 더 빠르게 accuracy=1에 가까워지고\r\n있음을 알 수 있습니다. 그래프 모양 또한 sgd는 부드러운 곡선을 그리지만\r\nadam은 삐뚤삐뚤한 형태로 수렴하고 있습니다. adam이 더 높은 accuracy를\r\n갖는 모델이지만 훈련시간 또한 긴 것을 확인할 수 있습니다.\r\nlearning rate scheduler\r\n사용 유무 비교\r\ntrainer = Trainer(model_class=LionMNISTClassifier, loaders=loaders, n_class=10, opt=\"adam\", lr=0.001, has_scheduler=True, device=device).to(device) \r\ntrainer.train(max_epochs=50) \r\nAdjusting learning rate of group 0 to 1.0000e-03. \r\n===== Train Start ===== \r\nAdjusting learning rate of group 0 to 1.0000e-03. \r\nEpoch: 0, train loss: 0.296774, train acc: 0.914591, valid loss: 0.112701, valid acc: 0.966230 \r\nAdjusting learning rate of group 0 to 1.0000e-03. \r\nAdjusting learning rate of group 0 to 1.0000e-03. \r\nAdjusting learning rate of group 0 to 1.0000e-03. \r\nAdjusting learning rate of group 0 to 5.0000e-04. \r\nAdjusting learning rate of group 0 to 5.0000e-04. \r\nEpoch: 5, train loss: 0.018088, train acc: 0.994940, valid loss: 0.077383, valid acc: 0.978663 \r\nAdjusting learning rate of group 0 to 5.0000e-04. \r\nAdjusting learning rate of group 0 to 5.0000e-04. \r\nAdjusting learning rate of group 0 to 5.0000e-04. \r\nAdjusting learning rate of group 0 to 2.5000e-04. \r\nAdjusting learning rate of group 0 to 2.5000e-04. \r\nEpoch: 10, train loss: 0.002736, train acc: 0.999462, valid loss: 0.073415, valid acc: 0.985047 \r\nAdjusting learning rate of group 0 to 2.5000e-04. \r\nAdjusting learning rate of group 0 to 2.5000e-04. \r\nAdjusting learning rate of group 0 to 2.5000e-04. \r\nAdjusting learning rate of group 0 to 1.2500e-04. \r\nAdjusting learning rate of group 0 to 1.2500e-04. \r\nEpoch: 15, train loss: 0.000520, train acc: 0.999944, valid loss: 0.085636, valid acc: 0.984207 \r\nAdjusting learning rate of group 0 to 1.2500e-04. \r\nAdjusting learning rate of group 0 to 1.2500e-04. \r\nAdjusting learning rate of group 0 to 1.2500e-04. \r\nAdjusting learning rate of group 0 to 6.2500e-05. \r\nAdjusting learning rate of group 0 to 6.2500e-05. \r\nEpoch: 20, train loss: 0.000101, train acc: 1.000000, valid loss: 0.096484, valid acc: 0.984207 \r\nAdjusting learning rate of group 0 to 6.2500e-05. \r\nAdjusting learning rate of group 0 to 6.2500e-05. \r\nAdjusting learning rate of group 0 to 6.2500e-05. \r\nAdjusting learning rate of group 0 to 3.1250e-05. \r\nAdjusting learning rate of group 0 to 3.1250e-05. \r\nEpoch: 25, train loss: 0.000039, train acc: 1.000000, valid loss: 0.103663, valid acc: 0.983703 \r\nAdjusting learning rate of group 0 to 3.1250e-05. \r\nAdjusting learning rate of group 0 to 3.1250e-05. \r\nAdjusting learning rate of group 0 to 3.1250e-05. \r\nAdjusting learning rate of group 0 to 1.5625e-05. \r\nAdjusting learning rate of group 0 to 1.5625e-05. \r\nEpoch: 30, train loss: 0.000020, train acc: 1.000000, valid loss: 0.110385, valid acc: 0.983535 \r\nAdjusting learning rate of group 0 to 1.5625e-05. \r\nAdjusting learning rate of group 0 to 1.5625e-05. \r\nAdjusting learning rate of group 0 to 1.5625e-05. \r\nAdjusting learning rate of group 0 to 7.8125e-06. \r\nAdjusting learning rate of group 0 to 7.8125e-06. \r\nEpoch: 35, train loss: 0.000013, train acc: 1.000000, valid loss: 0.113576, valid acc: 0.983535 \r\nAdjusting learning rate of group 0 to 7.8125e-06. \r\nAdjusting learning rate of group 0 to 7.8125e-06. \r\nAdjusting learning rate of group 0 to 7.8125e-06. \r\nAdjusting learning rate of group 0 to 3.9063e-06. \r\nAdjusting learning rate of group 0 to 3.9063e-06. \r\nEpoch: 40, train loss: 0.000010, train acc: 1.000000, valid loss: 0.115529, valid acc: 0.983535 \r\nAdjusting learning rate of group 0 to 3.9063e-06. \r\nAdjusting learning rate of group 0 to 3.9063e-06. \r\nAdjusting learning rate of group 0 to 3.9063e-06. \r\nAdjusting learning rate of group 0 to 1.9531e-06. \r\nAdjusting learning rate of group 0 to 1.9531e-06. \r\nEpoch: 45, train loss: 0.000009, train acc: 1.000000, valid loss: 0.116594, valid acc: 0.983535 \r\nAdjusting learning rate of group 0 to 1.9531e-06. \r\nAdjusting learning rate of group 0 to 1.9531e-06. \r\nAdjusting learning rate of group 0 to 1.9531e-06. \r\nAdjusting learning rate of group 0 to 9.7656e-07. \r\npngtrainer.test() \r\n===== Test Start ===== \r\nTest loss: 0.102079, Test acc: 0.983574 \r\nlearning rate scheduler를 사용한 결과 accuracy의 소폭 향상이\r\n있었습니다. 훈련 과정을 보면 점점 learning rate가 줄어들면서 학습이\r\n진행되는 것을 볼 수 있습니다.\r\ndropout, batch normalization\r\ndropout과 batch normalization을 적용하는 방법은 각 layer에\r\nnn.Dropout(), nn.BatchNorm1d()을 넣어주면 됩니다. 모두 accuracy가 소폭\r\n향상한 것을 알 수 있습니다.\r\nclass LionMNISTDropoutClassifier(nn.Module): \r\n    def __init__(self, n_class=10): \r\n        super().__init__() \r\n \r\n        self.model = nn.Sequential( \r\n            nn.Flatten(), \r\n            nn.Linear(28 * 28, 256), \r\n            nn.Dropout(p=0.5), \r\n            nn.ReLU(), \r\n            nn.Linear(256, 128), \r\n            nn.Dropout(p=0.5), \r\n            nn.ReLU(), \r\n            nn.Linear(128, n_class), \r\n        ) \r\n \r\n    def forward(self, x): \r\n        return self.model(x) \r\n \r\n \r\nclass LionMNISTBNClassifier(nn.Module): \r\n    def __init__(self, n_class=10): \r\n        super().__init__() \r\n \r\n        self.model = nn.Sequential( \r\n            nn.Flatten(), \r\n            nn.Linear(28 * 28, 256), \r\n            nn.BatchNorm1d(256), \r\n            nn.ReLU(), \r\n            nn.Linear(256, 128), \r\n            nn.BatchNorm1d(128), \r\n            nn.ReLU(), \r\n            nn.Linear(128, n_class), \r\n        ) \r\n \r\n    def forward(self, x): \r\n        return self.model(x) \r\n \r\n \r\nclass LionMNISTDropoutBNClassifier(nn.Module): \r\n    def __init__(self, n_class=10): \r\n        super().__init__() \r\n \r\n        self.model = nn.Sequential( \r\n            nn.Flatten(), \r\n            nn.Linear(28 * 28, 256), \r\n            nn.BatchNorm1d(256), \r\n            nn.Dropout(p=0.5), \r\n            nn.ReLU(), \r\n            nn.Linear(256, 128), \r\n            nn.BatchNorm1d(128), \r\n            nn.Dropout(p=0.5), \r\n            nn.ReLU(), \r\n            nn.Linear(128, n_class), \r\n        ) \r\n \r\n    def forward(self, x): \r\n        return self.model(x) \r\n \r\ntrainer = Trainer(model_class=LionMNISTDropoutClassifier, loaders=loaders, n_class=10, opt=\"adam\", lr=0.001, device=device).to(device) \r\ntrainer.train(max_epochs=30) \r\ntrainer.test() \r\n===== Train Start ===== \r\nEpoch: 0, train loss: 0.471969, train acc: 0.858708, valid loss: 0.140879, valid acc: 0.960853 \r\nEpoch: 5, train loss: 0.131920, train acc: 0.961410, valid loss: 0.073890, valid acc: 0.977991 \r\nEpoch: 10, train loss: 0.099638, train acc: 0.969195, valid loss: 0.063448, valid acc: 0.980847 \r\nEpoch: 15, train loss: 0.086451, train acc: 0.974032, valid loss: 0.061495, valid acc: 0.982359 \r\nEpoch: 20, train loss: 0.076410, train acc: 0.976479, valid loss: 0.066648, valid acc: 0.982023 \r\nEpoch: 25, train loss: 0.070171, train acc: 0.977832, valid loss: 0.065918, valid acc: 0.983031 \r\n===== Test Start ===== \r\nTest loss: 0.073770, Test acc: 0.981671 \r\npngtrainer = Trainer(model_class=LionMNISTBNClassifier, loaders=loaders, n_class=10, opt=\"adam\", lr=0.001, device=device).to(device) \r\ntrainer.train(max_epochs=30) \r\ntrainer.test() \r\n===== Train Start ===== \r\nEpoch: 0, train loss: 0.215938, train acc: 0.940410, valid loss: 0.088783, valid acc: 0.974630 \r\nEpoch: 5, train loss: 0.031933, train acc: 0.989824, valid loss: 0.077726, valid acc: 0.977991 \r\nEpoch: 10, train loss: 0.017131, train acc: 0.993624, valid loss: 0.066482, valid acc: 0.983703 \r\nEpoch: 15, train loss: 0.008775, train acc: 0.997220, valid loss: 0.077114, valid acc: 0.982527 \r\nEpoch: 20, train loss: 0.008320, train acc: 0.997146, valid loss: 0.074517, valid acc: 0.983031 \r\nEpoch: 25, train loss: 0.008707, train acc: 0.996942, valid loss: 0.072421, valid acc: 0.985719 \r\n===== Test Start ===== \r\nTest loss: 0.072791, Test acc: 0.983774 \r\npngtrainer = Trainer(model_class=LionMNISTDropoutBNClassifier, loaders=loaders, n_class=10, opt=\"adam\", lr=0.001, device=device).to(device) \r\ntrainer.train(max_epochs=30) \r\ntrainer.test() \r\n===== Train Start ===== \r\nEpoch: 0, train loss: 0.440807, train acc: 0.872961, valid loss: 0.126382, valid acc: 0.964214 \r\nEpoch: 5, train loss: 0.157185, train acc: 0.952365, valid loss: 0.067443, valid acc: 0.980511 \r\nEpoch: 10, train loss: 0.116907, train acc: 0.963820, valid loss: 0.059610, valid acc: 0.982863 \r\nEpoch: 15, train loss: 0.099826, train acc: 0.968546, valid loss: 0.058426, valid acc: 0.984711 \r\nEpoch: 20, train loss: 0.090204, train acc: 0.971716, valid loss: 0.060830, valid acc: 0.983199 \r\nEpoch: 25, train loss: 0.080905, train acc: 0.974385, valid loss: 0.059024, valid acc: 0.985887 \r\n===== Test Start ===== \r\nTest loss: 0.057088, Test acc: 0.983073 \r\npng\r\n\r\n\r\n",
    "preview": "til/2022-05-27-mnistclassification/output_15_0.png",
    "last_modified": "2022-06-01T16:48:09+09:00",
    "input_file": {},
    "preview_width": 1150,
    "preview_height": 590
  },
  {
    "path": "til/2022-05-23-dlfromscratchch3/",
    "title": "신경망과 활성화 함수",
    "description": "밑바닥부터 시작하는 딥러닝을 읽고 신경망과 활성화 함수에 대해 정리해봤습니다.",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-23",
    "categories": [
      "밑바닥부터 시작하는 딥러닝"
    ],
    "contents": "\r\n\r\nContents\r\n1️⃣신경망이란\r\n2️⃣활성화 함수의 종류\r\n계단함수\r\n시그모이드 함수\r\nReLU 함수\r\n항등 함수\r\n소프트맥스 함수\r\n\r\n3️⃣비선형 함수\r\n4️⃣numpy로 신경망 구현\r\n5️⃣소프트맥스 함수의 특징\r\n6️⃣분류 신경망 구현\r\n7️⃣배치 처리\r\n\r\n1️⃣신경망이란\r\n단층 퍼셉트론은 다음과 같은 수식으로 표현됩니다.\r\n\\[y = \\begin{cases}0,(b+w_{1}x_{1} +\r\nw_{2}x_{2}\\leq0)\\\\1,(b+w_{1}x_{1} +\r\nw_{2}x_{2}>0)\\\\\\end{cases}\\]\r\n\\(z = b+w_{1}x_{1} + w_{2}x_{2}\\)로\r\n나타낸다면 다음과 같이 나타낼 수 있습니다.\r\n\\[y =\r\n\\begin{cases}0,z\\leq0)\\\\1,z>0\\\\\\end{cases}\\]\r\n즉, 입력신호의 가중합이 0을 넘으면 1을 반환하고 그렇지 않다면 0을\r\n반환합니다. 이처럼 입력신호들의 값을 출력신호로 변환하는 함수를 활성화\r\n함수라고 하며 단층 퍼셉트론에서 사용한 함수를 계단 함수라고 합니다.\r\n단층 퍼셉트론은 활성화 함수로 계단 함수 사용하지만 신경망은 다른\r\n활성화 함수를 사용할 수 있고 다층 퍼셉트론처럼 여러 층을 쌓아 만든\r\n알고리즘이라 할 수 있습니다\r\n신경망의 예위 그림처럼 신경망은 크게 출력층(input layer), 은닉층(hidden layer),\r\n출력층(output layer)로 이뤄져 있으며 각 원을 노드(node)라고\r\n부릅니다.\r\n2️⃣활성화 함수의 종류\r\n계단함수\r\n퍼셉트론에서 사용하는 함수. 출력는 0 또는 1이다.\r\n\\[h(z) =\r\n\\begin{cases}0,z\\leq0)\\\\1,z>0\\\\\\end{cases}\\]\r\n시그모이드 함수\r\n출력값의 범위가 0과 1사이에 존재한다.\r\n\\[h(z) = 1/(1+exp(-z))\\]\r\nReLU 함수\r\n\\[h(z) =\r\n\\begin{cases}0,z\\leq0)\\\\z,z>0\\\\\\end{cases}\\]\r\n항등 함수\r\n\\[ h(z) = z\\]\r\n소프트맥스 함수\r\n\\(h(z_k)\\)는 k번째 출력값을 의미\r\n\\[h(z_k) = exp(z_k)/\\sum\r\nexp(z_i)\\]\r\n3️⃣비선형 함수\r\n신경망에서 층을 쌓을 때 활성화 함수는 비선형 함수이어야 합니다. 만약\r\n활성화 함수로 다음과 같은 함수를 사용한다면,\r\n\\[ h(z) = cz\\]\r\n층을 아무리 쌓아도 다음과 같은 형태로 단순화 시킬 수 있고 결국 층을\r\n안쌓은 것과 동일한 효과를 갖게 됩니다.\r\n\\[ h(h(h(z))) = c^3z\\]\r\n그러므로 층을 쌓는 혜택을 얻기 위해서는 활성화 함수로 비선형 함수를\r\n사용해야 합니다.\r\n4️⃣numpy로 신경망 구현\r\n이 글 처음에 등장한 그림은 3개의 입력을 받아 4개의 노드가 존재하는\r\n은닉층 하나를 거쳐 2개를 출력하는 신경망을 도식화한 것입니다. numpy를\r\n활용해 이를 구현해보겠습니다.\r\n각 노드의 가중치는 임의로 넣고 은닉층의 활성화 함수는 시그모이드,\r\n출력층의 활성화 함수는 항등함수를 사용했습니다.\r\n\r\nimport numpy as np\r\n\r\ndef init_network():\r\n    network = {}\r\n    network[\"W1\"] = np.array([[0.1,0.3,0.1,0.5],[0.2,0.4,0.3,0.4],[0.1,0.2,0.3,0.5]]) # 은닉층\r\n    network[\"W2\"] = np.array([[0.2,0.4],[0.1,0.2],[0.1,0.3],[0.5,0.5]]) # 출력층\r\n    \r\n    return network\r\n\r\ndef sigmoid(x):\r\n    return 1/(1+np.exp(-x))\r\n\r\ndef identity_function(x):\r\n    return x\r\n\r\ndef forward(network, x):\r\n    W1, W2 = network[\"W1\"], network[\"W2\"]\r\n    x = np.dot(x,W1)\r\n    x = sigmoid(x)\r\n    x = np.dot(x,W2)\r\n    x = identity_function(x)\r\n    \r\n    return x\r\n\r\n\r\nnetwork = init_network()\r\nforward(network, np.array([0.1, 0.5, 0.7]))\r\narray([0.55009573, 0.8365092 ])\r\n\r\n5️⃣소프트맥스 함수의 특징\r\n위 신경망에서 출력층은 항등함수로 구현된 것을 볼 수 있습니다.\r\n일반적으로 회귀에는 항등함수를, 분류에는 소프트맥스 함수를 사용합니다.\r\n그 이유는 소프트맥스의 출력값들의 합은 1이어서 각 출력값들을 분류확률로\r\n해석할 수 있기 때문입니다. 그러므로 소프트맥스의 출력값으로 cost를\r\n계산할 수 있게 됩니다. 하지만 학습된 모델을 적용할 때는 출력층에\r\n소프트맥스를 사용할 필요가 없습니다. 소프트맥스 함수는 단조증가 함수이고\r\ncost를 계산할 필요가 없기 때문에 가장 큰 값을 그대로 분류하면 됩니다.\r\n지수함수 계산에 드는 비용도 줄일 수 있습니다.\r\n소프트맥스를 코드로 구현 시 오버플로 문제가 존재합니다. 이 문제를\r\n해결하기 위해 다음과 같은 수식으로 소프트맥스 함수를 개선할 수\r\n있습니다.\r\n\\[h(z_k) = exp(z_k)/\\sum exp(z_i) \\\\ =\r\nCexp(z_k)/C\\sum exp(z_i) \\\\ = exp(z_k+log(C))/\\sum exp(z_i+log(C)), \\\\ C\r\n= -max(z_i)\\] 즉, 소프트맥스의 입력값에 상수를 더해도 그 값의\r\n차이가 없으므로 입력값들 중 최대값을 각 입력값에 빼주면 오버플로 문제를\r\n해결할 수 있습니다.\r\n6️⃣분류 신경망 구현\r\niris 데이터를 활용하여 Species를 분류하는 모델을 만들어\r\n보겠습니다.\r\ndata load\r\n\r\n\r\nlibrary(reticulate)\r\npy$iris <- iris\r\n\r\n\r\n\r\n\r\nimport pandas as pd\r\niris = pd.DataFrame(iris)\r\n\r\n\r\niris\r\n     Sepal.Length  Sepal.Width  Petal.Length  Petal.Width    Species\r\n0             5.1          3.5           1.4          0.2     setosa\r\n1             4.9          3.0           1.4          0.2     setosa\r\n2             4.7          3.2           1.3          0.2     setosa\r\n3             4.6          3.1           1.5          0.2     setosa\r\n4             5.0          3.6           1.4          0.2     setosa\r\n..            ...          ...           ...          ...        ...\r\n145           6.7          3.0           5.2          2.3  virginica\r\n146           6.3          2.5           5.0          1.9  virginica\r\n147           6.5          3.0           5.2          2.0  virginica\r\n148           6.2          3.4           5.4          2.3  virginica\r\n149           5.9          3.0           5.1          1.8  virginica\r\n\r\n[150 rows x 5 columns]\r\n\r\none-hot_encoding\r\n\r\nx_data = iris.drop([\"Species\"], axis=1)\r\nx_data = x_data.values\r\ny_data = pd.get_dummies(iris['Species'])\r\ny_data\r\n     setosa  versicolor  virginica\r\n0         1           0          0\r\n1         1           0          0\r\n2         1           0          0\r\n3         1           0          0\r\n4         1           0          0\r\n..      ...         ...        ...\r\n145       0           0          1\r\n146       0           0          1\r\n147       0           0          1\r\n148       0           0          1\r\n149       0           0          1\r\n\r\n[150 rows x 3 columns]\r\ny_data = y_data.values\r\n\r\nmodeling\r\n임의로 가중치를 부여하여 모델을 만듭니다. 이전 모델과 큰 차이는\r\n없지만 입력 노드와 출력 노드가 하나씩 더 들어가있고 출력층의 활성화\r\n함수가 소프트맥스 함수로 바뀌었습니다.\r\n\r\ndef init_network():\r\n    network = {}\r\n    network[\"W1\"] = np.array([[0.1,0.3,0.1,0.5],[0.2,0.4,0.3,0.4],[0.5,0.2,0.3,0.5],[0.7,0.5,0.2,0.5]]) # 은닉층\r\n    network[\"W2\"] = np.array([[0.5,0.9,0.1],[0.1,0.2,0.7],[0.2,0.5,0.5],[0.5,0.5,0.5]]) # 출력층\r\n    \r\n    return network\r\n\r\ndef sigmoid(x):\r\n    return 1/(1+np.exp(-x))\r\n\r\ndef softmax(x):\r\n    c = np.max(x)\r\n    exp_x = np.exp(x-c)\r\n    sum_exp_x = np.sum(exp_x)\r\n    \r\n    return exp_x/sum_exp_x\r\n\r\ndef forward(network, x):\r\n    W1, W2 = network[\"W1\"], network[\"W2\"]\r\n    x = np.dot(x,W1)\r\n    x = sigmoid(x)\r\n    x = np.dot(x,W2)\r\n    x = softmax(x)\r\n    \r\n    return x\r\n\r\n첫번째 입력값만 넣어 출력을 확인해보면 2번째 값의 분류확률이 가장\r\n높은 것을 확인할 수 있습니다.\r\n\r\nnetwork = init_network()\r\nforward(network, x_data[0])\r\narray([0.21381633, 0.43739548, 0.34878819])\r\n\r\nAccuacy를 확인해보면 33%인 것을 확인할 수 있습니다. 아직 학습과정을\r\n통해 가중치를 최적화 하지 않았기 때문입니다.\r\n\r\nnetwork = init_network()\r\n\r\ncount = 0\r\nfor i in range(len(x_data)):\r\n    pred_p = forward(network, x_data[i])\r\n    index_max = np.argmax(pred_p)\r\n    \r\n    if index_max == np.argmax(y_data[i]):\r\n        count += 1\r\n\r\nprint(\"Accuacy: \", count/len(x_data))\r\nAccuacy:  0.3333333333333333\r\n\r\n7️⃣배치 처리\r\n아까 구현했던 모델에서 입력값의 shape, 각 가중치의 shape, 출력값의\r\nshape을 확인해보겠습니다.\r\n\r\ndef forward(network, x):\r\n    W1, W2 = network[\"W1\"], network[\"W2\"]\r\n    print(\"input : \", x.shape)\r\n    print(\"W1 : \", W1.shape)\r\n    print(\"W2 : \", W2.shape)\r\n    x = np.dot(x,W1)\r\n    x = sigmoid(x)\r\n    x = np.dot(x,W2)\r\n    x = softmax(x)\r\n    print(\"output : \", x.shape)\r\n    return x\r\n\r\nforward(network, x_data[0])\r\ninput :  (4,)\r\nW1 :  (4, 4)\r\nW2 :  (4, 3)\r\noutput :  (3,)\r\narray([0.21381633, 0.43739548, 0.34878819])\r\n\r\n행렬의 곱 원리에 따라 (1x4)(4x4)(4x3)=(1x3)의 array가 출력되는 것을\r\n알 수 있습니다. 여기서 만약 input을 (3x4)로 주게 된다면 output은\r\n(3x3)array로 출력될 것입니다.\r\n이처럼 입력데이터를 묶어서 계산할 수 있으며 입력 데이터 묶음을\r\n배치라고 하고 배치 안에 데이터의 수를 배치 크기(batch size)라고 합니다.\r\n그럼 배치 처리를 구현해보겠습니다.\r\n이전 코드와 다른 점은 소프트맥스 함수가 배치처리를 할 수 있도록\r\n수정했고 예측을 수행할 때도 배치 별로 예측할 수 있도록 했습니다.\r\n\r\ndef init_network():\r\n    network = {}\r\n    network[\"W1\"] = np.array([[0.1,0.3,0.1,0.5],[0.2,0.4,0.3,0.4],[0.5,0.2,0.3,0.5],[0.7,0.5,0.2,0.5]]) # 은닉층\r\n    network[\"W2\"] = np.array([[0.5,0.9,0.1],[0.1,0.2,0.7],[0.2,0.5,0.5],[0.5,0.5,0.5]]) # 출력층\r\n    \r\n    return network\r\n\r\ndef sigmoid(x):\r\n    return 1/(1+np.exp(-x))\r\n\r\ndef softmax(x):\r\n    c = np.max(x)\r\n    exp_x = np.exp(x-c)\r\n    \r\n    return np.multiply(exp_x.T, 1/np.sum(exp_x, axis=1)).T\r\n\r\ndef forward(network, x):\r\n    W1, W2 = network[\"W1\"], network[\"W2\"]\r\n    x = np.dot(x,W1)\r\n    x = sigmoid(x)\r\n    x = np.dot(x,W2)\r\n    x = softmax(x)\r\n    \r\n    return x\r\n\r\nnetwork = init_network()\r\nbatch_size = 10\r\n\r\ncount = 0\r\nfor i in range(0, len(x_data), batch_size):\r\n    x_batch = x_data[i:i+batch_size]\r\n    pred_p = forward(network, x_batch)\r\n    count += (np.argmax(pred_p, axis=1) == np.argmax(y_data[i:i+batch_size], axis=1)).sum()\r\n    \r\nprint(\"Accuacy: \", count/len(x_data))\r\nAccuacy:  0.3333333333333333\r\n\r\n배치 처리는 컴퓨터로 계산할 때 큰 이점을 줍니다. 배치처리를 하지\r\n않았을 때는 for문 루프가 데이터 수만큼 돌았지만 배치크기를 10개로 했을\r\n때에는 10분의 1만큼 감소했습니다. 즉, 더 큰 배열로 한번에 계산하는 것이\r\n컴퓨터는 더 빠릅니다.\r\n다음장은 신경망 학습과정을 통해 가중치를 최적화하는 방법입니다. 위\r\n모델도 학습과정을 통해 Accuacy를 높일 수 있습니다!\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-26T00:42:26+09:00",
    "input_file": {}
  },
  {
    "path": "til/2022-05-20-pytorch-tutorial/",
    "title": "Pytorch tutorial",
    "description": "딥러닝 프레임워크 파이토치에 대해 알아보자",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-20",
    "categories": [
      "pytorch"
    ],
    "contents": "\r\n\r\nContents\r\ntorch tensor\r\ntensor의 parameter\r\ntensor의 생성\r\ndevice 지정\r\ntensor 다루기\r\ntonsor의 연산\r\n\r\nDataset\r\nDataLoader\r\ntorch.nn.Module\r\nreference\r\n\r\nsocar 부트캠프의 강의 중 일부를 정리한 내용입니다.\r\n\r\nimport numpy as np\r\n\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.utils.data import Dataset, DataLoader\r\n\r\nfrom torchvision import datasets, transforms\r\n\r\nimport matplotlib.pyplot as plt\r\n\r\ntorch tensor\r\ntensor는 numpy의 ndarray처럼 다차원 데이터 배열입니다. tensor를\r\n생성할 때는 list를 사용할 수도 있지만 ndarray를 사용할 수도\r\n있습니다.\r\nndarray와 tensor의 차이점은 tensor에는 back propagation(역전파)를\r\n다루기 위해 forward pass(순전파)에서 전달된 값과 연산의 종류를 기억할 수\r\n있습니다.(gradient 값 저장)\r\ntensor의 parameter\r\ndata : list나 ndarray 등 array데이터\r\ndtype : 데이터 타입\r\ndevice : tensor가 위치해있는 device 지정\r\nrequires_grad : gradient 값 저장 유무\r\ntensor의 생성\r\nndarray와 비슷한 생성 방법입니다.\r\n\r\na = torch.tensor([[1.0, 4.0], [4.0, 3.0]])\r\nb = torch.tensor([[4, 3], [1, 4], [1, 2]])\r\nprint(a)\r\ntensor([[1., 4.],\r\n        [4., 3.]])\r\nprint(b)\r\ntensor([[4, 3],\r\n        [1, 4],\r\n        [1, 2]])\r\n\r\ntensor.dtype : tensor의 자료형 확인\r\ntensor.shape : tensor의 size 확인\r\n\r\nprint(a.dtype, a.shape)\r\ntorch.float32 torch.Size([2, 2])\r\nprint(b.dtype, b.shape)\r\ntorch.int64 torch.Size([3, 2])\r\n\r\ntorch.ones(*size) : 모든 원소가 1인 tensor\r\ntorch.zeros(*size) : 모든 원소가 0인 tensor\r\ntorch.eye(n, m) : 대각 원소가 1이고 나머지가 0인 \\(n*m\\) tensor를 생성. m = None이면 \\(n*n\\)tensor를 생성\r\ntorch.rand(*size) : 모든 원소를 랜덤한 값으로 채워진 tensor.\r\ndtype을 int로 지정시 에러가 발생\r\n\r\na = torch.ones([2, 3])\r\nb = torch.zeros([3, 2], dtype=torch.int64)\r\nc = torch.eye(4,3)\r\nd = torch.rand([2, 4, 3], dtype=torch.float)\r\n\r\nprint(a)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.]])\r\nprint(b)\r\ntensor([[0, 0],\r\n        [0, 0],\r\n        [0, 0]])\r\nprint(c)\r\ntensor([[1., 0., 0.],\r\n        [0., 1., 0.],\r\n        [0., 0., 1.],\r\n        [0., 0., 0.]])\r\nprint(d)\r\ntensor([[[0.1721, 0.5773, 0.8588],\r\n         [0.8192, 0.3488, 0.5716],\r\n         [0.3033, 0.4843, 0.1258],\r\n         [0.6734, 0.4262, 0.9605]],\r\n\r\n        [[0.0436, 0.7960, 0.0340],\r\n         [0.6968, 0.8373, 0.5634],\r\n         [0.0413, 0.0030, 0.7226],\r\n         [0.0329, 0.8479, 0.7469]]])\r\n\r\n앞서 언급했듯이 ndarray로도 tensor를 생성할 수 있습니다. tensor()는\r\n원본의 값을 복사하는 반면 as_tensor()와 from_numpy()는 원본의 값을\r\n참조하기 때문에 원본의 값을 바꾸면 같이 변하는 것을 알 수 있습니다.\r\n\r\nimport numpy as np\r\n\r\nd = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])\r\n\r\na = torch.tensor(d)\r\nb = torch.as_tensor(d)\r\nc = torch.from_numpy(d)\r\n\r\nprint(a, id(a))\r\ntensor([[1, 1, 1],\r\n        [2, 2, 2],\r\n        [3, 3, 3]], dtype=torch.int32) 883303344\r\nprint(b, id(b))\r\ntensor([[1, 1, 1],\r\n        [2, 2, 2],\r\n        [3, 3, 3]], dtype=torch.int32) 883288720\r\nprint(c, id(c))\r\ntensor([[1, 1, 1],\r\n        [2, 2, 2],\r\n        [3, 3, 3]], dtype=torch.int32) 883073808\r\nprint(d, id(d))\r\n[[1 1 1]\r\n [2 2 2]\r\n [3 3 3]] 883007184\r\nd[0,0] = 0\r\n\r\nprint(a)\r\ntensor([[1, 1, 1],\r\n        [2, 2, 2],\r\n        [3, 3, 3]], dtype=torch.int32)\r\nprint(b)\r\ntensor([[0, 1, 1],\r\n        [2, 2, 2],\r\n        [3, 3, 3]], dtype=torch.int32)\r\nprint(c)\r\ntensor([[0, 1, 1],\r\n        [2, 2, 2],\r\n        [3, 3, 3]], dtype=torch.int32)\r\nprint(d)\r\n[[0 1 1]\r\n [2 2 2]\r\n [3 3 3]]\r\n\r\ntensor.shape을 통해 동일한 size의 tensor를 만들 수도 있지만\r\n_like(tensor) 방법으로도 동일한 size의 tensor를 만들 수 있습니다.\r\n\r\na = torch.ones(a.shape)\r\nprint(a)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.],\r\n        [1., 1., 1.]])\r\nb = torch.ones_like(a)\r\nc = torch.zeros_like(a, dtype=torch.float)\r\nd = torch.rand_like(a, dtype=torch.float)\r\n\r\nprint(b)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.],\r\n        [1., 1., 1.]])\r\nprint(c)\r\ntensor([[0., 0., 0.],\r\n        [0., 0., 0.],\r\n        [0., 0., 0.]])\r\nprint(d)\r\ntensor([[0.0823, 0.9254, 0.6186],\r\n        [0.4919, 0.9770, 0.6451],\r\n        [0.4601, 0.9505, 0.9724]])\r\n\r\ndevice 지정\r\ngpu연산을 위해서는 tensor의 device를 cuda로 지정해줘야합니다.\r\ntorch.cuda.is_available()를 통해 gpu가 사용 가능한지 확인\r\n후 tensor.to(‘cuda’)를 통해 device를 지정할 수 있습니다.\r\n\r\ntorch.cuda.is_available() # 저는 gpu가 없어요 ㅠㅜ\r\nFalse\r\nif torch.cuda.is_available():\r\n    device = \"cuda\"\r\nelse:\r\n    device = \"cpu\"\r\n    \r\na.to(device)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.],\r\n        [1., 1., 1.]])\r\na.device\r\ndevice(type='cpu')\r\n\r\ntensor 다루기\r\n\r\na = torch.ones([2, 3])\r\nb = torch.zeros([2, 3])\r\n\r\ntorch.cat() : 텐서 합치기\r\n\r\nc = torch.cat([a, b], dim=0) # 열방향으로 합치기\r\nd = torch.cat([a, b], dim=1) # 행방향으로 합치기\r\nprint(c)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.],\r\n        [0., 0., 0.],\r\n        [0., 0., 0.]])\r\nprint(c.shape)\r\ntorch.Size([4, 3])\r\nprint(d)\r\ntensor([[1., 1., 1., 0., 0., 0.],\r\n        [1., 1., 1., 0., 0., 0.]])\r\nprint(d.shape)\r\ntorch.Size([2, 6])\r\n\r\ntorch.stack : 텐서 쌓기\r\n새로운 차원(dim)에 따라 tensor들 쌓아줍니다. dim=0일때에는 tensor\r\n전체를 기준으로 쌓고, dim=1일때는 tensor의 다음 차원을 기준으로\r\n쌓는식입니다. 그러므로 dim은 0부터 tensor의 차원의 수를 넘을 수 없고,\r\nstack 안의 tensor들은 서로 size가 같아야 합니다.\r\n\r\nc = torch.stack([a, b], dim=0)\r\nd = torch.stack([a, b], dim=1)\r\ne = torch.stack([a, b], dim=2)\r\nprint(c)\r\ntensor([[[1., 1., 1.],\r\n         [1., 1., 1.]],\r\n\r\n        [[0., 0., 0.],\r\n         [0., 0., 0.]]])\r\nprint(c.shape)\r\ntorch.Size([2, 2, 3])\r\nprint(d)\r\ntensor([[[1., 1., 1.],\r\n         [0., 0., 0.]],\r\n\r\n        [[1., 1., 1.],\r\n         [0., 0., 0.]]])\r\nprint(d.shape)\r\ntorch.Size([2, 2, 3])\r\nprint(e)\r\ntensor([[[1., 0.],\r\n         [1., 0.],\r\n         [1., 0.]],\r\n\r\n        [[1., 0.],\r\n         [1., 0.],\r\n         [1., 0.]]])\r\nprint(e.shape)\r\ntorch.Size([2, 3, 2])\r\n\r\nhstack : torch.cat([a, b], dim=0)\r\nvstack : torch.cat([a, b], dim=1)\r\n\r\nc = torch.hstack([a, b])\r\nprint(c)\r\ntensor([[1., 1., 1., 0., 0., 0.],\r\n        [1., 1., 1., 0., 0., 0.]])\r\nprint(c.shape)\r\ntorch.Size([2, 6])\r\nd = torch.vstack([a, b])\r\nprint(d)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.],\r\n        [0., 0., 0.],\r\n        [0., 0., 0.]])\r\nprint(d.shape)\r\ntorch.Size([4, 3])\r\n\r\ntorch.unsqueeze() : 더미차원 삭제\r\ntorch.squeeze() : 더미차원 추가\r\n\r\n# unsqueeze and squeeze\r\nprint(a)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.]])\r\na = torch.unsqueeze(a, dim=1)\r\nprint(a)\r\ntensor([[[1., 1., 1.]],\r\n\r\n        [[1., 1., 1.]]])\r\nprint(a.shape)\r\ntorch.Size([2, 1, 3])\r\nprint(torch.squeeze(a))\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.]])\r\nprint(a.shape)\r\ntorch.Size([2, 1, 3])\r\n\r\ntonsor의 연산\r\n행렬 곱\r\n\r\na = torch.tensor(np.array(list(range(12)))).reshape(3, 4)\r\nprint(a)\r\ntensor([[ 0,  1,  2,  3],\r\n        [ 4,  5,  6,  7],\r\n        [ 8,  9, 10, 11]], dtype=torch.int32)\r\nb = torch.tensor(np.array(list(range(8)))).reshape(4, 2)\r\nprint(b)\r\ntensor([[0, 1],\r\n        [2, 3],\r\n        [4, 5],\r\n        [6, 7]], dtype=torch.int32)\r\nc = a @ b\r\nprint(c)\r\ntensor([[ 28,  34],\r\n        [ 76,  98],\r\n        [124, 162]], dtype=torch.int32)\r\nd = torch.matmul(a, b)\r\nprint(d)\r\ntensor([[ 28,  34],\r\n        [ 76,  98],\r\n        [124, 162]], dtype=torch.int32)\r\n\r\nelement-wise product\r\n\r\na = torch.tensor(np.array(list(range(6)))).reshape(2, 3)\r\nprint(a)\r\ntensor([[0, 1, 2],\r\n        [3, 4, 5]], dtype=torch.int32)\r\nb = torch.tensor(list(range(10, 13))).reshape(1, 3)\r\nprint(b)\r\ntensor([[10, 11, 12]])\r\nc = a * b\r\nprint(c)\r\ntensor([[ 0, 11, 24],\r\n        [30, 44, 60]])\r\nd = a.mul(b)\r\nprint(d)\r\ntensor([[ 0, 11, 24],\r\n        [30, 44, 60]])\r\n\r\nitem() : tensor의 원소를 반환합니다. tensor 안에 원소가 1개만 있어야\r\n합니다.\r\n\r\n# item\r\nagg = d.sum()\r\nv = agg.item()\r\nprint(v, type(v))\r\n169 <class 'int'>\r\n\r\ninplace operations\r\n\r\nprint(a)\r\ntensor([[0, 1, 2],\r\n        [3, 4, 5]], dtype=torch.int32)\r\na.add(5) \r\ntensor([[ 5,  6,  7],\r\n        [ 8,  9, 10]], dtype=torch.int32)\r\nprint(a)\r\ntensor([[0, 1, 2],\r\n        [3, 4, 5]], dtype=torch.int32)\r\na.add_(5) # inplace operations\r\ntensor([[ 5,  6,  7],\r\n        [ 8,  9, 10]], dtype=torch.int32)\r\nprint(a)\r\ntensor([[ 5,  6,  7],\r\n        [ 8,  9, 10]], dtype=torch.int32)\r\n\r\nDataset\r\n파이토치에서 Dataset은 전체 데이터를 sample 단위로 처리해주는 역할을\r\n합니다. Dataset을 상속받아 오버라이딩을 통해 커스텀 Dataset을\r\n만들어보겠습니다.\r\n커스텀 Dataset 구조\r\n\r\nclass CustomDataset(torch.utils.data.Dataset): \r\n    def __init__(self): \r\n        # 데이터셋의 전처리를 해주는 부분\r\n        \r\n    def __len__(self):\r\n        # 데이터셋의 길이를 반환해주는 부분\r\n        \r\n    def __getitem__(self, idx):\r\n        # 데이터셋에서 샘플을 추출해주는 부분\r\n\r\n다음 커스텀 데이터 셋을 확인해보면 __init__에서\r\nfeatures와 target 데이터, 전처리 함수를 저장하도록 정의 되있고,\r\n__len__에서 data의 길이를 반환해주도록 정의되있습니다.\r\n마지막으로 __getitem__에서 저장된 데이터를 인덱싱 후 정의된\r\n전처리 함수를 거쳐 반환되도록 구현되있습니다.\r\n\r\nclass LionDataset(Dataset):\r\n    def __init__(self, data, target, transform=None, target_transform=None):\r\n        self.data = data # feature data\r\n        self.target = target # target data\r\n        self.transform = transform # featrue\r\n        self.target_transform = target_transform\r\n        pass\r\n    \r\n    def __len__(self):\r\n        return len(self.data)\r\n    def __getitem__(self, idx):\r\n        x = self.data[idx]\r\n        y = self.target[idx]\r\n        \r\n        if self.transform:\r\n          x = self.transform(x)\r\n        if self.target_transform:\r\n          y = self.target_transform(y)\r\n        \r\n        return x, y\r\n\r\n\r\ndata = np.array(list(range(100))).reshape(-1, 2)\r\ntarget = np.array([[i] * 5 for i in range(10)]).reshape(-1)\r\n\r\nprint(data)\r\n[[ 0  1]\r\n [ 2  3]\r\n [ 4  5]\r\n [ 6  7]\r\n [ 8  9]\r\n [10 11]\r\n [12 13]\r\n [14 15]\r\n [16 17]\r\n [18 19]\r\n [20 21]\r\n [22 23]\r\n [24 25]\r\n [26 27]\r\n [28 29]\r\n [30 31]\r\n [32 33]\r\n [34 35]\r\n [36 37]\r\n [38 39]\r\n [40 41]\r\n [42 43]\r\n [44 45]\r\n [46 47]\r\n [48 49]\r\n [50 51]\r\n [52 53]\r\n [54 55]\r\n [56 57]\r\n [58 59]\r\n [60 61]\r\n [62 63]\r\n [64 65]\r\n [66 67]\r\n [68 69]\r\n [70 71]\r\n [72 73]\r\n [74 75]\r\n [76 77]\r\n [78 79]\r\n [80 81]\r\n [82 83]\r\n [84 85]\r\n [86 87]\r\n [88 89]\r\n [90 91]\r\n [92 93]\r\n [94 95]\r\n [96 97]\r\n [98 99]]\r\nprint(target)\r\n[0 0 0 0 0 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 4 4 4 4 4 5 5 5 5 5 6 6 6 6 6 7 7\r\n 7 7 7 8 8 8 8 8 9 9 9 9 9]\r\n\r\n\r\nlion = LionDataset(data=data, target=target)\r\nprint(lion[0:4])\r\n(array([[0, 1],\r\n       [2, 3],\r\n       [4, 5],\r\n       [6, 7]]), array([0, 0, 0, 0]))\r\nprint(len(lion))\r\n50\r\n\r\nDataLoader\r\nDataLoader는 dataset을 batch 단위로 묶어주는 역할을 합니다.\r\nbatch_size : batch_size\r\nshuffle : True시 epoch마다 데이터가 학습되는 순서가 섞임\r\n\r\nloader = DataLoader(dataset=lion, batch_size=10, shuffle=True)\r\n\r\nfor i, batch in enumerate(loader):\r\n    x, y = batch\r\n    if i == 0:\r\n        print(x)\r\n        print(y)\r\n    print(x.shape)\r\ntensor([[38, 39],\r\n        [ 4,  5],\r\n        [88, 89],\r\n        [62, 63],\r\n        [84, 85],\r\n        [44, 45],\r\n        [40, 41],\r\n        [20, 21],\r\n        [66, 67],\r\n        [68, 69]], dtype=torch.int32)\r\ntensor([3, 0, 8, 6, 8, 4, 4, 2, 6, 6], dtype=torch.int32)\r\ntorch.Size([10, 2])\r\ntorch.Size([10, 2])\r\ntorch.Size([10, 2])\r\ntorch.Size([10, 2])\r\ntorch.Size([10, 2])\r\n\r\ntorch.nn.Module\r\npytorch 모델은 parameters를 추적하며 forward pass를 진행한 뒤 back\r\npropagation을 통해 학습을 진행합니다. torch.nn.Module은 여러 층의\r\nlayer로 이뤄진 모델을 쉽게 관리할 수 있는 class입니다.\r\npytorch 모델의 기본구조\r\n\r\nclass Model_Name(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        \"\"\"\r\n        모델에 사용될 Layer(nn.Linear, nn.Conv2d)와 \r\n        activation function(nn.functional.relu, nn.functional.sigmoid)등을 정의\r\n        \"\"\"\r\n\r\n    def forward(self, x):\r\n        \"\"\"\r\n        모델에서 실행되어야하는 계산을 정의\r\n        \"\"\"\r\n        return x\r\n\r\nexample\r\n\r\nclass LionLinear(nn.Module):\r\n    def __init__(self, input_dim, output_dim):\r\n        super().__init__()\r\n        self.input_dim = input_dim   # 입력차원 \r\n        self.output_dim = output_dim # 출력차원\r\n        \r\n        self.flatten = nn.Flatten()  # tensor 평탄화 정의\r\n        self.classifier = nn.Linear(input_dim, output_dim) # Linear layer 정의\r\n        self.act = nn.ReLU() # activation function(ReLU) 정의\r\n        \r\n    def forward(self, x):\r\n        x = self.flatten(x)    # data를 linear layers에 맞게 평탄화 후\r\n        x = self.classifier(x) # linear layer를 통과,\r\n        x = self.act(x)        # activation function을 통해 출력\r\n        \r\n        return x\r\n\r\npytorch 모델은 모델의 구조를 쉽게 파악할 수 있습니다.\r\n\r\nlinear_model = LionLinear(28*28, 10).to(device)\r\nprint(linear_model)\r\nLionLinear(\r\n  (flatten): Flatten(start_dim=1, end_dim=-1)\r\n  (classifier): Linear(in_features=784, out_features=10, bias=True)\r\n  (act): ReLU()\r\n)\r\nprint(linear_model.classifier)\r\nLinear(in_features=784, out_features=10, bias=True)\r\n\r\n다음은 MLP를 구현하는 module입니다.\r\n코드를 더 간결하게 하기 위해 다음과 같이 일부 layer 등을 따로 모듈로\r\n구현 후 전체 모듈에 합쳐서 구현할 수 있습니다.\r\n\r\nclass LionLayer(nn.Module):\r\n    def __init__(self, input_dim, output_dim):\r\n        super().__init__()\r\n        self.input_dim = input_dim\r\n        self.output_dim = output_dim\r\n        self.layer = nn.Linear(self.input_dim, self.output_dim)\r\n        pass\r\n\r\n    def forward(self, x):\r\n        assert x.shape[-1] == self.input_dim, \"Input dimension mismatch\"\r\n        return self.layer(x)\r\n\r\n\r\nclass LionMLP(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim):\r\n        super().__init__()\r\n        self.input_dim = input_dim\r\n        self.hidden_dim = hidden_dim\r\n        self.output_dim = output_dim\r\n        self.flatten = nn.Flatten()\r\n        self.linear_1 = LionLayer(self.input_dim, self.hidden_dim)\r\n        self.linear_2 = LionLayer(self.hidden_dim, self.output_dim)\r\n        self.act_1 = nn.ReLU()\r\n        self.act_2 = nn.Softmax()\r\n        pass\r\n    \r\n    def forward(self, x):\r\n        x = self.flatten(x)\r\n        x = self.act_1(self.linear_1(x))\r\n        x = self.act_2(self.linear_2(x))\r\n        return x\r\n\r\n\r\nmlp = LionMLP(28*28, 50, 10)\r\nprint(mlp)\r\nLionMLP(\r\n  (flatten): Flatten(start_dim=1, end_dim=-1)\r\n  (linear_1): LionLayer(\r\n    (layer): Linear(in_features=784, out_features=50, bias=True)\r\n  )\r\n  (linear_2): LionLayer(\r\n    (layer): Linear(in_features=50, out_features=10, bias=True)\r\n  )\r\n  (act_1): ReLU()\r\n  (act_2): Softmax(dim=None)\r\n)\r\n\r\nnn.Sequential()을 통해 forward() 부분을 짧게 작성할 수 있습니다.\r\n\r\nclass LionMLP(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim):\r\n        super().__init__()\r\n        self.input_dim = input_dim\r\n        self.hidden_dim = hidden_dim\r\n        self.output_dim = output_dim\r\n        self.flatten = nn.Flatten()\r\n        self.linear_1 = LionLayer(self.input_dim, self.hidden_dim)\r\n        self.linear_2 = LionLayer(self.hidden_dim, self.output_dim)\r\n        self.act_1 = nn.ReLU()\r\n        self.act_2 = nn.Softmax()\r\n        self.model = nn.Sequential(self.flatten, self.linear_1, self.act_1, self.linear_2, self.act_2)\r\n        pass\r\n    def forward(self, x):\r\n        return self.model(x)\r\n\r\n\r\nmlp = LionMLP(28 * 28, 40, 10)\r\nprint(mlp)\r\nLionMLP(\r\n  (flatten): Flatten(start_dim=1, end_dim=-1)\r\n  (linear_1): LionLayer(\r\n    (layer): Linear(in_features=784, out_features=40, bias=True)\r\n  )\r\n  (linear_2): LionLayer(\r\n    (layer): Linear(in_features=40, out_features=10, bias=True)\r\n  )\r\n  (act_1): ReLU()\r\n  (act_2): Softmax(dim=None)\r\n  (model): Sequential(\r\n    (0): Flatten(start_dim=1, end_dim=-1)\r\n    (1): LionLayer(\r\n      (layer): Linear(in_features=784, out_features=40, bias=True)\r\n    )\r\n    (2): ReLU()\r\n    (3): LionLayer(\r\n      (layer): Linear(in_features=40, out_features=10, bias=True)\r\n    )\r\n    (4): Softmax(dim=None)\r\n  )\r\n)\r\n\r\n모델 파라미터 확인\r\n\r\nfor name, param in mlp.named_parameters():\r\n    print(f\"Layer: {name} | Size: {param.size()} | Values: {param[:2]} \\n\")\r\nLayer: linear_1.layer.weight | Size: torch.Size([40, 784]) | Values: tensor([[ 0.0178, -0.0108, -0.0017,  ...,  0.0248, -0.0011,  0.0218],\r\n        [-0.0343,  0.0235, -0.0201,  ...,  0.0321, -0.0141,  0.0180]],\r\n       grad_fn=<SliceBackward0>) \r\n\r\nLayer: linear_1.layer.bias | Size: torch.Size([40]) | Values: tensor([ 0.0207, -0.0041], grad_fn=<SliceBackward0>) \r\n\r\nLayer: linear_2.layer.weight | Size: torch.Size([10, 40]) | Values: tensor([[ 0.0301, -0.0820, -0.1123, -0.0777, -0.0678, -0.1305, -0.0293,  0.0596,\r\n         -0.0557, -0.1315, -0.1003,  0.0239,  0.1351,  0.0894, -0.0559, -0.0034,\r\n          0.0682,  0.1417,  0.0025, -0.0303, -0.0004, -0.0029,  0.0281,  0.0950,\r\n          0.0631, -0.0958, -0.0276, -0.1566,  0.1130, -0.0361,  0.0906, -0.0657,\r\n          0.1251,  0.0872, -0.1033,  0.0821,  0.0856, -0.1505,  0.1350,  0.1250],\r\n        [ 0.0562,  0.0557,  0.0730, -0.1076,  0.0850, -0.1426,  0.1005,  0.1108,\r\n          0.0231,  0.1560,  0.1185, -0.0472,  0.0049,  0.0836,  0.0008, -0.1055,\r\n          0.1363,  0.1266, -0.0864,  0.1325,  0.1444,  0.1412,  0.1253, -0.0832,\r\n          0.0536,  0.0351, -0.1228, -0.0855, -0.0909,  0.0840, -0.0335,  0.0978,\r\n         -0.0824,  0.1048,  0.0254, -0.0287,  0.0238,  0.1337, -0.1085,  0.1532]],\r\n       grad_fn=<SliceBackward0>) \r\n\r\nLayer: linear_2.layer.bias | Size: torch.Size([10]) | Values: tensor([ 0.0235, -0.0153], grad_fn=<SliceBackward0>) \r\n\r\nreference\r\nPyTorch로 시작하는 딥 러닝 입문 : https://wikidocs.net/57165\r\nhttps://anweh.tistory.com/21\r\n\r\n\r\n\r\n",
    "preview": "til/2022-05-20-pytorch-tutorial/images/pytorch.jpg",
    "last_modified": "2022-05-22T15:10:31+09:00",
    "input_file": {}
  },
  {
    "path": "til/2022-05-19-cotepart1/",
    "title": "[백준 문제풀이] 기본기",
    "description": "코딩테스트를 위한 백준 문제 풀어보기",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-19",
    "categories": [
      "coding test"
    ],
    "contents": "\r\n\r\nContents\r\n약수 구하기\r\n이진수\r\nbin() 직접 구현하기(양의\r\n정수일때만)\r\n\r\n최대, 최소\r\n\r\n약수 구하기\r\n두 개의 자연수 N과 K가 주어졌을 때, N의 약수들 중 K번째로 작은 수를\r\n출력하는 프로그램을 작성하시오.\r\n입력\r\n첫째 줄에 N과 K가 빈칸을 사이에 두고 주어진다. N은 1 이상 10,000\r\n이하이다. K는 1 이상 N 이하이다.\r\n출력\r\n첫째 줄에 N의 약수들 중 K번째로 작은 수를 출력한다. 만일 N의 약수의\r\n개수가 K개보다 적어서 K번째 약수가 존재하지 않을 경우에는 0을\r\n출력하시오.\r\n\r\nN, K = map(int, input().split())\r\n\r\nresult = 0\r\n\r\nfor i in range(1, N + 1):\r\n    if N % i == 0:\r\n        K -= 1\r\n        if K == 0:\r\n            result = i\r\n            break\r\n\r\nprint(result)\r\n\r\n이진수\r\n양의 정수 n이 주어졌을 때, 이를 이진수로 나타냈을 때 1의 위치를 모두\r\n찾는 프로그램을 작성하시오. 최하위 비트(least significant bit, lsb)의\r\n위치는 0이다.\r\n입력\r\n첫째 줄에 테스트 케이스의 개수 T가 주어진다. 각 테스트 케이스는 한\r\n줄로 이루어져 있고, n이 주어진다. \\((1 ≤ T ≤\r\n10, 1 ≤ n ≤ 10^6)\\)\r\n출력\r\n각 테스트 케이스에 대해서, 1의 위치를 공백으로 구분해서 줄 하나에\r\n출력한다. 위치가 낮은 것부터 출력한다.\r\n\r\nt = int(input())\r\n\r\nfor _ in range(t):\r\n    n = int(input())\r\n    b = bin(n)[2:]\r\n    \r\n    for i in range(len(b)):\r\n        if b[::-1][i] == '1':\r\n            print(i, end=' ')\r\n\r\nbin() 직접 구현하기(양의\r\n정수일때만)\r\n\r\ndef my_bin(num):\r\n    a = num//2\r\n    b = num%2\r\n    if a == 0:\r\n        return '1'\r\n    else:\r\n        return my_bin(a)+str(b)\r\n\r\n최대, 최소\r\nN개의 정수가 주어진다. 이때, 최솟값과 최댓값을 구하는 프로그램을\r\n작성하시오.\r\n입력\r\n첫째 줄에 정수의 개수 N (1 ≤ N ≤ 1,000,000)이 주어진다. 둘째 줄에는\r\nN개의 정수를 공백으로 구분해서 주어진다. 모든 정수는 -1,000,000보다\r\n크거나 같고, 1,000,000보다 작거나 같은 정수이다.\r\n출력\r\n첫째 줄에 주어진 정수 N개의 최솟값과 최댓값을 공백으로 구분해\r\n출력한다.\r\n\r\ninput()\r\nl = list(map(int, input().split()))\r\n\r\nhigh = low = l[0]\r\n\r\nfor i in l[1:]:\r\n    if high < i:\r\n        high = i\r\n    if low > i:\r\n        low = i\r\n        \r\nprint(low, high)\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-20T19:04:09+09:00",
    "input_file": {}
  },
  {
    "path": "til/2022-05-18-dlfromscratchch2/",
    "title": "퍼셉트론이란",
    "description": "밑바닥부터 시작하는 딥러닝을 읽고 딥러닝의 기원이 되는 알고리즘인 퍼셉트론에 대해 정리해봤습니다.",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-18",
    "categories": [
      "밑바닥부터 시작하는 딥러닝"
    ],
    "contents": "\r\n\r\nContents\r\n1️⃣ 퍼셉트론이란\r\n2️⃣ 퍼셉트론의 한계\r\n3️⃣ 다층 퍼셉트론\r\n✅ 요약\r\n\r\n1️⃣ 퍼셉트론이란\r\n퍼셉트론은 다수의 신호를 입력(input)으로 받아 하나의 신호를\r\n출력(output)하는 알고리즘입니다.\r\n2개의 입력을 받아 하나의 신호를 출력하는 퍼셉트론을 수식으로\r\n나타내면,\r\n\\[y = \\begin{cases}0,(w_{1}x_{1} +\r\nw_{2}x_{2}\\leq\\theta)\\\\1,(w_{1}x_{1} +\r\nw_{2}x_{2}>\\theta)\\\\\\end{cases}\\] 또는 \\(\\theta\\)를 \\(-b\\)로 치환하여,\r\n\\[y = \\begin{cases}0,(b+w_{1}x_{1} +\r\nw_{2}x_{2}\\leq0)\\\\1,(b+w_{1}x_{1} +\r\nw_{2}x_{2}>0)\\\\\\end{cases}\\]\r\n\\(w\\) : weight, \\(b\\): bias\r\n2️⃣ 퍼셉트론의 한계\r\n퍼셉트론이 풀 수 있는 (예측할 수 있는) 문제는 AND, NAND,\r\nOR입니다.\r\nAND\r\n\r\nx1x2y000100010111\r\n\r\nNAND\r\n\r\nx1x2y001101011110\r\n\r\nOR\r\n\r\nx1x2y000101011111\r\n\r\n위 세가지 문제는 \\((w_1, w_2,\r\ntheta)\\)에 각각 \\((0.5, 0.5, 0.7),\r\n(-0.5, -0.5, -0.7), (1.5, 1.5, 0.7)\\) 등을 대입하면 풀 수\r\n있습니다.\r\n이를 파이썬 함수로 구현하면,\r\n\r\ndef AND(x1, x2):\r\n    w1, w2, theta = 0.5, 0.5, 0.5\r\n    tmp = x1*w1+x2*w2\r\n    if tmp <= theta:\r\n        return 0\r\n    else:\r\n        return 1\r\n\r\ndef NAND(x1, x2):\r\n    w1, w2, theta = -0.5, -0.5, -0.7\r\n    tmp = x1*w1+x2*w2\r\n    if tmp <= theta:\r\n        return 0\r\n    else:\r\n        return 1\r\n    \r\ndef OR(x1, x2):\r\n    w1, w2, theta = 1.5, 1.5, 0.7\r\n    tmp = x1*w1+x2*w2\r\n    if tmp <= theta:\r\n        return 0\r\n    else:\r\n        return 1\r\n\r\nprint('x1=0, x2=0 : ', AND(0,0))\r\nx1=0, x2=0 :  0\r\nprint('x1=0, x2=1 : ', AND(0,1))\r\nx1=0, x2=1 :  0\r\nprint('x1=1, x2=0 : ', AND(1,0))\r\nx1=1, x2=0 :  0\r\nprint('x1=1, x2=1 : ', AND(1,1))\r\nx1=1, x2=1 :  1\r\nprint('x1=0, x2=0 : ', NAND(0,0))\r\nx1=0, x2=0 :  1\r\nprint('x1=0, x2=1 : ', NAND(0,1))\r\nx1=0, x2=1 :  1\r\nprint('x1=1, x2=0 : ', NAND(1,0))\r\nx1=1, x2=0 :  1\r\nprint('x1=1, x2=1 : ', NAND(1,1))\r\nx1=1, x2=1 :  0\r\nprint('x1=0, x2=0 : ', OR(0,0))\r\nx1=0, x2=0 :  0\r\nprint('x1=0, x2=1 : ', OR(0,1))\r\nx1=0, x2=1 :  1\r\nprint('x1=1, x2=0 : ', OR(1,0))\r\nx1=1, x2=0 :  1\r\nprint('x1=1, x2=1 : ', OR(1,1))\r\nx1=1, x2=1 :  1\r\n\r\n하지만, 퍼셉트론으로 XOR은 해결할 수 없습니다.\r\nXOR\r\n\r\nx1x2y000101011110\r\n\r\n위 문제를 그림으로 나타내면 다음과 같이 빨간점과 검은점을 구분할 수\r\n있는 선을 그을 수 있어야 합니다.\r\n\r\n\r\n\r\n곡선으로 선을 그린다면 구분할 수 있겠지만 퍼셉트론으로는 직선밖에\r\n그릴 수 없으므로 해결할 수 없습니다. 즉, 퍼셉트론은 비선형의 영역을\r\n표현할 수 없습니다.\r\n3️⃣ 다층 퍼셉트론\r\n이를 해결하기 위해 나온 것이 다층 퍼셉트론입니다. 말 그대로 퍼셉트론\r\n층을 여러 개 갖는 알고리즘이며 앞에서 언급했던 퍼셉트론은 정확히 말하면\r\n단층 퍼셉트론입니다.\r\nXOR문제를 해결하려면 위에서 AND, NAND, OR 문제를 해결하기 위해\r\n만들었던 퍼셉트론을 적절히 쌓아 만들면 됩니다.\r\n\r\ndef XOR(x1, x2):\r\n    s1 = NAND(x1, x2)\r\n    s2 = OR(x1, x2)\r\n    y = AND(s1, s2)\r\n    return y\r\n\r\nprint('x1=0, x2=0 : ', XOR(0,0))\r\nx1=0, x2=0 :  0\r\nprint('x1=0, x2=1 : ', XOR(0,1))\r\nx1=0, x2=1 :  1\r\nprint('x1=1, x2=0 : ', XOR(1,0))\r\nx1=1, x2=0 :  1\r\nprint('x1=1, x2=1 : ', XOR(1,1))\r\nx1=1, x2=1 :  0\r\n\r\n첫번째 층에 NAND, OR를 풀기 위해 만들었던 퍼셉트론을 쌓고 두번째 층에\r\nAND를 풀기 위해 만들었던 퍼셉트론을 쌓아 XOR문제를 해결했습니다.\r\n이처럼 다층 퍼셉트론은 비선형의 영역까지도 표현할 수 있음을 알 수\r\n있습니다.\r\n✅ 요약\r\n퍼셉트론은 입출력을 갖춘 알고리즘\r\n단층 퍼셉트론은 선형 영역만 표현할 수 있고, 다층 퍼셉트론은\r\n비선형 영역도 표현 가능\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-19T00:59:30+09:00",
    "input_file": {}
  },
  {
    "path": "til/2022-05-13-first-til/",
    "title": "[멋쟁이사자처럼] OBG의 TIL 페이지 개설",
    "description": "Socar Bootcamp Peer Group OBG(Old But Gold)의 TIL 페이지입니다.",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-13",
    "categories": [],
    "contents": "\r\n\r\nContents\r\n🦁PG 필수 공통활동\r\n1️⃣주 1회 이상 study 및\r\nTIL작성\r\n2️⃣Peer Review\r\n\r\n🥇OBG만의 study 진행\r\n1️⃣밑바닥부터 시작하는\r\n딥러닝\r\n2️⃣백준 코딩테스트\r\n문제풀이\r\n\r\n👨‍💻OBG 팀원 소개\r\n\r\n🙇안녕하세요. OBG의 Peer Group Leader 김경환입니다. 어쩌다보니\r\nleader를 맡게 되었는데 부족하지만 알찬 스터디가 되도록\r\n노력하겠습니다.\r\n🦁PG 필수 공통활동\r\n1️⃣주 1회 이상 study 및\r\nTIL작성\r\nOBG는 일요일에 시간을 맞춰 스터디를 진행할 예정입니다. 일주일동안\r\n정해진 공부한 내용을 각자 스타일에 맞게 TIL을 작성 후 study 시간에\r\n공유합니다. study를 하면서 어려웠던 점, 새로 알았던 점, 보충해야할 점\r\n등을 파악하고 일주일동안 배운 내용을 회고하며 OBG의 TIL을 작성합니다.\r\n마지막으로 차주 스터디 위한 계획을 공유하며 마무리합니다.\r\n2️⃣Peer Review\r\nPeer\r\nReview Guide를 읽어주세요.\r\n코딩테스트나 대외비와 관련없는 데이터를 다루는 코드들은 github을\r\n통해 상호 리뷰\r\n🥇OBG만의 study 진행\r\n1️⃣밑바닥부터 시작하는 딥러닝\r\n딥러닝의 근본이라고 할 수 있는 이 책을 매주 1 chapter씩\r\nstudy합니다.\r\n2️⃣백준 코딩테스트 문제풀이\r\n매주 정해진 문제를 풀고 peer review를 통해 서로의 코드를\r\n발전시킵니다.\r\n👨‍💻OBG 팀원 소개\r\n김경환\r\n안녕하세요. 통계학과 졸업 후 작년 9월부터 올해 2월까지\r\n한국전자통신연구원 인턴 경험을 통해 딥러닝에 관심이 생겼습니다. 잘\r\n부탁드립니다!\r\ngithub\r\nTIL\r\n페이지\r\n\r\n최경석\r\n안녕하세요, 저는 산업경영공학부 3학년에 재학중인 최경석입니다.\r\n머신러닝과 관련된 경험은 캐글이나 데이콘을 몇번 참가한적이 있고,\r\n딥러닝은 한국조선해양에서 진행한 공모전에 간단한 이미지 분류를\r\n진행한적이 있습니다. 교내 데이터 사이언스 학회에서도 활동하지만 개념적인\r\n부분이 부족하고, 아직은 체득하지 못한 것 같아서 열심히 활동하며 배우고\r\n싶습니다.\r\ngithub\r\n[TIL 페이지]\r\n\r\n우선욱\r\n[github]\r\n[TIL 페이지]\r\n\r\n류기철\r\n안녕하세요 저는 리서치회사에 근무하고있는 류기철이라고 합니다.\r\n현재 기본적인 ML/DL에 대한 기본적인 이론정도만 있어서 조금더 심화된\r\n부분에 대한 실습을 하고싶어서 지원했습니다.\r\ngithub\r\n[TIL 페이지]\r\n\r\n",
    "preview": "til/2022-05-13-first-til/images/owl.png",
    "last_modified": "2022-05-27T00:59:59+09:00",
    "input_file": {},
    "preview_width": 640,
    "preview_height": 514
  }
]

[
  {
    "path": "til/2022-05-20-pytorch-tutorial/",
    "title": "Pytorch tutorial",
    "description": "ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ íŒŒì´í† ì¹˜ì— ëŒ€í•´ ì•Œì•„ë³´ì",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-20",
    "categories": [
      "pytorch"
    ],
    "contents": "\r\n\r\nContents\r\ntorch tensor\r\ntensorì˜ parameter\r\ntensorì˜ ìƒì„±\r\ndevice ì§€ì •\r\ntensor ë‹¤ë£¨ê¸°\r\ntonsorì˜ ì—°ì‚°\r\n\r\nDataset\r\nDataLoader\r\ntorch.nn.Module\r\nreference\r\n\r\nsocar ë¶€íŠ¸ìº í”„ì˜ ê°•ì˜ ì¤‘ ì¼ë¶€ë¥¼ ì •ë¦¬í•œ ë‚´ìš©ì…ë‹ˆë‹¤.\r\n\r\nimport numpy as np\r\n\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.utils.data import Dataset, DataLoader\r\n\r\nfrom torchvision import datasets, transforms\r\n\r\nimport matplotlib.pyplot as plt\r\n\r\ntorch tensor\r\ntensorëŠ” numpyì˜ ndarrayì²˜ëŸ¼ ë‹¤ì°¨ì› ë°ì´í„° ë°°ì—´ì…ë‹ˆë‹¤. tensorë¥¼\r\nìƒì„±í•  ë•ŒëŠ” listë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆì§€ë§Œ ndarrayë¥¼ ì‚¬ìš©í•  ìˆ˜ë„\r\nìˆìŠµë‹ˆë‹¤.\r\nndarrayì™€ tensorì˜ ì°¨ì´ì ì€ tensorì—ëŠ” back propagation(ì—­ì „íŒŒ)ë¥¼\r\në‹¤ë£¨ê¸° ìœ„í•´ forward pass(ìˆœì „íŒŒ)ì—ì„œ ì „ë‹¬ëœ ê°’ê³¼ ì—°ì‚°ì˜ ì¢…ë¥˜ë¥¼ ê¸°ì–µí•  ìˆ˜\r\nìˆìŠµë‹ˆë‹¤.(gradient ê°’ ì €ì¥)\r\ntensorì˜ parameter\r\ndata : listë‚˜ ndarray ë“± arrayë°ì´í„°\r\ndtype : ë°ì´í„° íƒ€ì…\r\ndevice : tensorê°€ ìœ„ì¹˜í•´ìˆëŠ” device ì§€ì •\r\nrequires_grad : gradient ê°’ ì €ì¥ ìœ ë¬´\r\ntensorì˜ ìƒì„±\r\nndarrayì™€ ë¹„ìŠ·í•œ ìƒì„± ë°©ë²•ì…ë‹ˆë‹¤.\r\n\r\na = torch.tensor([[1.0, 4.0], [4.0, 3.0]])\r\nb = torch.tensor([[4, 3], [1, 4], [1, 2]])\r\nprint(a)\r\ntensor([[1., 4.],\r\n        [4., 3.]])\r\nprint(b)\r\ntensor([[4, 3],\r\n        [1, 4],\r\n        [1, 2]])\r\n\r\ntensor.dtype : tensorì˜ ìë£Œí˜• í™•ì¸\r\ntensor.shape : tensorì˜ size í™•ì¸\r\n\r\nprint(a.dtype, a.shape)\r\ntorch.float32 torch.Size([2, 2])\r\nprint(b.dtype, b.shape)\r\ntorch.int64 torch.Size([3, 2])\r\n\r\ntorch.ones(*size) : ëª¨ë“  ì›ì†Œê°€ 1ì¸ tensor\r\ntorch.zeros(*size) : ëª¨ë“  ì›ì†Œê°€ 0ì¸ tensor\r\ntorch.eye(n, m) : ëŒ€ê° ì›ì†Œê°€ 1ì´ê³  ë‚˜ë¨¸ì§€ê°€ 0ì¸ \\(n*m\\) tensorë¥¼ ìƒì„±. m = Noneì´ë©´ \\(n*n\\)tensorë¥¼ ìƒì„±\r\ntorch.rand(*size) : ëª¨ë“  ì›ì†Œë¥¼ ëœë¤í•œ ê°’ìœ¼ë¡œ ì±„ì›Œì§„ tensor.\r\ndtypeì„ intë¡œ ì§€ì •ì‹œ ì—ëŸ¬ê°€ ë°œìƒ\r\n\r\na = torch.ones([2, 3])\r\nb = torch.zeros([3, 2], dtype=torch.int64)\r\nc = torch.eye(4,3)\r\nd = torch.rand([2, 4, 3], dtype=torch.float)\r\n\r\nprint(a)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.]])\r\nprint(b)\r\ntensor([[0, 0],\r\n        [0, 0],\r\n        [0, 0]])\r\nprint(c)\r\ntensor([[1., 0., 0.],\r\n        [0., 1., 0.],\r\n        [0., 0., 1.],\r\n        [0., 0., 0.]])\r\nprint(d)\r\ntensor([[[0.4267, 0.7558, 0.7581],\r\n         [0.9764, 0.5643, 0.0557],\r\n         [0.3452, 0.6512, 0.9713],\r\n         [0.0638, 0.0089, 0.6160]],\r\n\r\n        [[0.1207, 0.4361, 0.2639],\r\n         [0.5522, 0.8477, 0.4962],\r\n         [0.3894, 0.3661, 0.4973],\r\n         [0.0413, 0.8170, 0.7664]]])\r\n\r\nì•ì„œ ì–¸ê¸‰í–ˆë“¯ì´ ndarrayë¡œë„ tensorë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. tensor()ëŠ”\r\nì›ë³¸ì˜ ê°’ì„ ë³µì‚¬í•˜ëŠ” ë°˜ë©´ as_tensor()ì™€ from_numpy()ëŠ” ì›ë³¸ì˜ ê°’ì„\r\nì°¸ì¡°í•˜ê¸° ë•Œë¬¸ì— ì›ë³¸ì˜ ê°’ì„ ë°”ê¾¸ë©´ ê°™ì´ ë³€í•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n\r\nimport numpy as np\r\n\r\nd = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])\r\n\r\na = torch.tensor(d)\r\nb = torch.as_tensor(d)\r\nc = torch.from_numpy(d)\r\n\r\nprint(a, id(a))\r\ntensor([[1, 1, 1],\r\n        [2, 2, 2],\r\n        [3, 3, 3]], dtype=torch.int32) 883304064\r\nprint(b, id(b))\r\ntensor([[1, 1, 1],\r\n        [2, 2, 2],\r\n        [3, 3, 3]], dtype=torch.int32) 883289280\r\nprint(c, id(c))\r\ntensor([[1, 1, 1],\r\n        [2, 2, 2],\r\n        [3, 3, 3]], dtype=torch.int32) 883069472\r\nprint(d, id(d))\r\n[[1 1 1]\r\n [2 2 2]\r\n [3 3 3]] 883007184\r\nd[0,0] = 0\r\n\r\nprint(a)\r\ntensor([[1, 1, 1],\r\n        [2, 2, 2],\r\n        [3, 3, 3]], dtype=torch.int32)\r\nprint(b)\r\ntensor([[0, 1, 1],\r\n        [2, 2, 2],\r\n        [3, 3, 3]], dtype=torch.int32)\r\nprint(c)\r\ntensor([[0, 1, 1],\r\n        [2, 2, 2],\r\n        [3, 3, 3]], dtype=torch.int32)\r\nprint(d)\r\n[[0 1 1]\r\n [2 2 2]\r\n [3 3 3]]\r\n\r\ntensor.shapeì„ í†µí•´ ë™ì¼í•œ sizeì˜ tensorë¥¼ ë§Œë“¤ ìˆ˜ë„ ìˆì§€ë§Œ\r\n_like(tensor) ë°©ë²•ìœ¼ë¡œë„ ë™ì¼í•œ sizeì˜ tensorë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n\r\na = torch.ones(a.shape)\r\nprint(a)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.],\r\n        [1., 1., 1.]])\r\nb = torch.ones_like(a)\r\nc = torch.zeros_like(a, dtype=torch.float)\r\nd = torch.rand_like(a, dtype=torch.float)\r\n\r\nprint(b)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.],\r\n        [1., 1., 1.]])\r\nprint(c)\r\ntensor([[0., 0., 0.],\r\n        [0., 0., 0.],\r\n        [0., 0., 0.]])\r\nprint(d)\r\ntensor([[0.1732, 0.2645, 0.7020],\r\n        [0.5683, 0.8747, 0.1726],\r\n        [0.5885, 0.1139, 0.1493]])\r\n\r\ndevice ì§€ì •\r\ngpuì—°ì‚°ì„ ìœ„í•´ì„œëŠ” tensorì˜ deviceë¥¼ cudaë¡œ ì§€ì •í•´ì¤˜ì•¼í•©ë‹ˆë‹¤.\r\ntorch.cuda.is_available()ë¥¼ í†µí•´ gpuê°€ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸\r\ní›„ tensor.to(â€˜cudaâ€™)ë¥¼ í†µí•´ deviceë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n\r\ntorch.cuda.is_available() # ì €ëŠ” gpuê°€ ì—†ì–´ìš” ã… ã…œ\r\nFalse\r\nif torch.cuda.is_available():\r\n    device = \"cuda\"\r\nelse:\r\n    device = \"cpu\"\r\n    \r\na.to(device)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.],\r\n        [1., 1., 1.]])\r\na.device\r\ndevice(type='cpu')\r\n\r\ntensor ë‹¤ë£¨ê¸°\r\n\r\na = torch.ones([2, 3])\r\nb = torch.zeros([2, 3])\r\n\r\ntorch.cat() : í…ì„œ í•©ì¹˜ê¸°\r\n\r\nc = torch.cat([a, b], dim=0) # ì—´ë°©í–¥ìœ¼ë¡œ í•©ì¹˜ê¸°\r\nd = torch.cat([a, b], dim=1) # í–‰ë°©í–¥ìœ¼ë¡œ í•©ì¹˜ê¸°\r\nprint(c)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.],\r\n        [0., 0., 0.],\r\n        [0., 0., 0.]])\r\nprint(c.shape)\r\ntorch.Size([4, 3])\r\nprint(d)\r\ntensor([[1., 1., 1., 0., 0., 0.],\r\n        [1., 1., 1., 0., 0., 0.]])\r\nprint(d.shape)\r\ntorch.Size([2, 6])\r\n\r\ntorch.stack : í…ì„œ ìŒ“ê¸°\r\nìƒˆë¡œìš´ ì°¨ì›(dim)ì— ë”°ë¼ tensorë“¤ ìŒ“ì•„ì¤ë‹ˆë‹¤. dim=0ì¼ë•Œì—ëŠ” tensor\r\nì „ì²´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìŒ“ê³ , dim=1ì¼ë•ŒëŠ” tensorì˜ ë‹¤ìŒ ì°¨ì›ì„ ê¸°ì¤€ìœ¼ë¡œ\r\nìŒ“ëŠ”ì‹ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ dimì€ 0ë¶€í„° tensorì˜ ì°¨ì›ì˜ ìˆ˜ë¥¼ ë„˜ì„ ìˆ˜ ì—†ê³ ,\r\nstack ì•ˆì˜ tensorë“¤ì€ ì„œë¡œ sizeê°€ ê°™ì•„ì•¼ í•©ë‹ˆë‹¤.\r\n\r\nc = torch.stack([a, b], dim=0)\r\nd = torch.stack([a, b], dim=1)\r\ne = torch.stack([a, b], dim=2)\r\nprint(c)\r\ntensor([[[1., 1., 1.],\r\n         [1., 1., 1.]],\r\n\r\n        [[0., 0., 0.],\r\n         [0., 0., 0.]]])\r\nprint(c.shape)\r\ntorch.Size([2, 2, 3])\r\nprint(d)\r\ntensor([[[1., 1., 1.],\r\n         [0., 0., 0.]],\r\n\r\n        [[1., 1., 1.],\r\n         [0., 0., 0.]]])\r\nprint(d.shape)\r\ntorch.Size([2, 2, 3])\r\nprint(e)\r\ntensor([[[1., 0.],\r\n         [1., 0.],\r\n         [1., 0.]],\r\n\r\n        [[1., 0.],\r\n         [1., 0.],\r\n         [1., 0.]]])\r\nprint(e.shape)\r\ntorch.Size([2, 3, 2])\r\n\r\nhstack : torch.cat([a, b], dim=0)\r\nvstack : torch.cat([a, b], dim=1)\r\n\r\nc = torch.hstack([a, b])\r\nprint(c)\r\ntensor([[1., 1., 1., 0., 0., 0.],\r\n        [1., 1., 1., 0., 0., 0.]])\r\nprint(c.shape)\r\ntorch.Size([2, 6])\r\nd = torch.vstack([a, b])\r\nprint(d)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.],\r\n        [0., 0., 0.],\r\n        [0., 0., 0.]])\r\nprint(d.shape)\r\ntorch.Size([4, 3])\r\n\r\ntorch.unsqueeze() : ë”ë¯¸ì°¨ì› ì‚­ì œ\r\ntorch.squeeze() : ë”ë¯¸ì°¨ì› ì¶”ê°€\r\n\r\n# unsqueeze and squeeze\r\nprint(a)\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.]])\r\na = torch.unsqueeze(a, dim=1)\r\nprint(a)\r\ntensor([[[1., 1., 1.]],\r\n\r\n        [[1., 1., 1.]]])\r\nprint(a.shape)\r\ntorch.Size([2, 1, 3])\r\nprint(torch.squeeze(a))\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.]])\r\nprint(a.shape)\r\ntorch.Size([2, 1, 3])\r\n\r\ntonsorì˜ ì—°ì‚°\r\ní–‰ë ¬ ê³±\r\n\r\na = torch.tensor(np.array(list(range(12)))).reshape(3, 4)\r\nprint(a)\r\ntensor([[ 0,  1,  2,  3],\r\n        [ 4,  5,  6,  7],\r\n        [ 8,  9, 10, 11]], dtype=torch.int32)\r\nb = torch.tensor(np.array(list(range(8)))).reshape(4, 2)\r\nprint(b)\r\ntensor([[0, 1],\r\n        [2, 3],\r\n        [4, 5],\r\n        [6, 7]], dtype=torch.int32)\r\nc = a @ b\r\nprint(c)\r\ntensor([[ 28,  34],\r\n        [ 76,  98],\r\n        [124, 162]], dtype=torch.int32)\r\nd = torch.matmul(a, b)\r\nprint(d)\r\ntensor([[ 28,  34],\r\n        [ 76,  98],\r\n        [124, 162]], dtype=torch.int32)\r\n\r\nelement-wise product\r\n\r\na = torch.tensor(np.array(list(range(6)))).reshape(2, 3)\r\nprint(a)\r\ntensor([[0, 1, 2],\r\n        [3, 4, 5]], dtype=torch.int32)\r\nb = torch.tensor(list(range(10, 13))).reshape(1, 3)\r\nprint(b)\r\ntensor([[10, 11, 12]])\r\nc = a * b\r\nprint(c)\r\ntensor([[ 0, 11, 24],\r\n        [30, 44, 60]])\r\nd = a.mul(b)\r\nprint(d)\r\ntensor([[ 0, 11, 24],\r\n        [30, 44, 60]])\r\n\r\nitem() : tensorì˜ ì›ì†Œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. tensor ì•ˆì— ì›ì†Œê°€ 1ê°œë§Œ ìˆì–´ì•¼\r\ní•©ë‹ˆë‹¤.\r\n\r\n# item\r\nagg = d.sum()\r\nv = agg.item()\r\nprint(v, type(v))\r\n169 <class 'int'>\r\n\r\ninplace operations\r\n\r\nprint(a)\r\ntensor([[0, 1, 2],\r\n        [3, 4, 5]], dtype=torch.int32)\r\na.add(5) \r\ntensor([[ 5,  6,  7],\r\n        [ 8,  9, 10]], dtype=torch.int32)\r\nprint(a)\r\ntensor([[0, 1, 2],\r\n        [3, 4, 5]], dtype=torch.int32)\r\na.add_(5) # inplace operations\r\ntensor([[ 5,  6,  7],\r\n        [ 8,  9, 10]], dtype=torch.int32)\r\nprint(a)\r\ntensor([[ 5,  6,  7],\r\n        [ 8,  9, 10]], dtype=torch.int32)\r\n\r\nDataset\r\níŒŒì´í† ì¹˜ì—ì„œ Datasetì€ ì „ì²´ ë°ì´í„°ë¥¼ sample ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•´ì£¼ëŠ” ì—­í• ì„\r\ní•©ë‹ˆë‹¤. Datasetì„ ìƒì†ë°›ì•„ ì˜¤ë²„ë¼ì´ë”©ì„ í†µí•´ ì»¤ìŠ¤í…€ Datasetì„\r\në§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤.\r\nì»¤ìŠ¤í…€ Dataset êµ¬ì¡°\r\n\r\nclass CustomDataset(torch.utils.data.Dataset): \r\n    def __init__(self): \r\n        # ë°ì´í„°ì…‹ì˜ ì „ì²˜ë¦¬ë¥¼ í•´ì£¼ëŠ” ë¶€ë¶„\r\n        \r\n    def __len__(self):\r\n        # ë°ì´í„°ì…‹ì˜ ê¸¸ì´ë¥¼ ë°˜í™˜í•´ì£¼ëŠ” ë¶€ë¶„\r\n        \r\n    def __getitem__(self, idx):\r\n        # ë°ì´í„°ì…‹ì—ì„œ ìƒ˜í”Œì„ ì¶”ì¶œí•´ì£¼ëŠ” ë¶€ë¶„\r\n\r\në‹¤ìŒ ì»¤ìŠ¤í…€ ë°ì´í„° ì…‹ì„ í™•ì¸í•´ë³´ë©´ __init__ì—ì„œ\r\nfeaturesì™€ target ë°ì´í„°, ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì €ì¥í•˜ë„ë¡ ì •ì˜ ë˜ìˆê³ ,\r\n__len__ì—ì„œ dataì˜ ê¸¸ì´ë¥¼ ë°˜í™˜í•´ì£¼ë„ë¡ ì •ì˜ë˜ìˆìŠµë‹ˆë‹¤.\r\në§ˆì§€ë§‰ìœ¼ë¡œ __getitem__ì—ì„œ ì €ì¥ëœ ë°ì´í„°ë¥¼ ì¸ë±ì‹± í›„ ì •ì˜ëœ\r\nì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ê±°ì³ ë°˜í™˜ë˜ë„ë¡ êµ¬í˜„ë˜ìˆìŠµë‹ˆë‹¤.\r\n\r\nclass LionDataset(Dataset):\r\n    def __init__(self, data, target, transform=None, target_transform=None):\r\n        self.data = data # feature data\r\n        self.target = target # target data\r\n        self.transform = transform # featrue\r\n        self.target_transform = target_transform\r\n        pass\r\n    \r\n    def __len__(self):\r\n        return len(self.data)\r\n    def __getitem__(self, idx):\r\n        x = self.data[idx]\r\n        y = self.target[idx]\r\n        \r\n        if self.transform:\r\n          x = self.transform(x)\r\n        if self.target_transform:\r\n          y = self.target_transform(y)\r\n        \r\n        return x, y\r\n\r\n\r\ndata = np.array(list(range(100))).reshape(-1, 2)\r\ntarget = np.array([[i] * 5 for i in range(10)]).reshape(-1)\r\n\r\nprint(data)\r\n[[ 0  1]\r\n [ 2  3]\r\n [ 4  5]\r\n [ 6  7]\r\n [ 8  9]\r\n [10 11]\r\n [12 13]\r\n [14 15]\r\n [16 17]\r\n [18 19]\r\n [20 21]\r\n [22 23]\r\n [24 25]\r\n [26 27]\r\n [28 29]\r\n [30 31]\r\n [32 33]\r\n [34 35]\r\n [36 37]\r\n [38 39]\r\n [40 41]\r\n [42 43]\r\n [44 45]\r\n [46 47]\r\n [48 49]\r\n [50 51]\r\n [52 53]\r\n [54 55]\r\n [56 57]\r\n [58 59]\r\n [60 61]\r\n [62 63]\r\n [64 65]\r\n [66 67]\r\n [68 69]\r\n [70 71]\r\n [72 73]\r\n [74 75]\r\n [76 77]\r\n [78 79]\r\n [80 81]\r\n [82 83]\r\n [84 85]\r\n [86 87]\r\n [88 89]\r\n [90 91]\r\n [92 93]\r\n [94 95]\r\n [96 97]\r\n [98 99]]\r\nprint(target)\r\n[0 0 0 0 0 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 4 4 4 4 4 5 5 5 5 5 6 6 6 6 6 7 7\r\n 7 7 7 8 8 8 8 8 9 9 9 9 9]\r\n\r\n\r\nlion = LionDataset(data=data, target=target)\r\nprint(lion[0:4])\r\n(array([[0, 1],\r\n       [2, 3],\r\n       [4, 5],\r\n       [6, 7]]), array([0, 0, 0, 0]))\r\nprint(len(lion))\r\n50\r\n\r\nDataLoader\r\nDataLoaderëŠ” datasetì„ batch ë‹¨ìœ„ë¡œ ë¬¶ì–´ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.\r\nbatch_size : batch_size\r\nshuffle : Trueì‹œ epochë§ˆë‹¤ ë°ì´í„°ê°€ í•™ìŠµë˜ëŠ” ìˆœì„œê°€ ì„ì„\r\n\r\nloader = DataLoader(dataset=lion, batch_size=10, shuffle=True)\r\n\r\nfor i, batch in enumerate(loader):\r\n    x, y = batch\r\n    if i == 0:\r\n        print(x)\r\n        print(y)\r\n    print(x.shape)\r\ntensor([[76, 77],\r\n        [ 0,  1],\r\n        [16, 17],\r\n        [64, 65],\r\n        [42, 43],\r\n        [82, 83],\r\n        [86, 87],\r\n        [ 2,  3],\r\n        [36, 37],\r\n        [70, 71]], dtype=torch.int32)\r\ntensor([7, 0, 1, 6, 4, 8, 8, 0, 3, 7], dtype=torch.int32)\r\ntorch.Size([10, 2])\r\ntorch.Size([10, 2])\r\ntorch.Size([10, 2])\r\ntorch.Size([10, 2])\r\ntorch.Size([10, 2])\r\n\r\ntorch.nn.Module\r\npytorch ëª¨ë¸ì€ parametersë¥¼ ì¶”ì í•˜ë©° forward passë¥¼ ì§„í–‰í•œ ë’¤ back\r\npropagationì„ í†µí•´ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤. torch.nn.Moduleì€ ì—¬ëŸ¬ ì¸µì˜\r\nlayerë¡œ ì´ë¤„ì§„ ëª¨ë¸ì„ ì‰½ê²Œ ê´€ë¦¬í•  ìˆ˜ ìˆëŠ” classì…ë‹ˆë‹¤.\r\npytorch ëª¨ë¸ì˜ ê¸°ë³¸êµ¬ì¡°\r\n\r\nclass Model_Name(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        \"\"\"\r\n        ëª¨ë¸ì— ì‚¬ìš©ë  Layer(nn.Linear, nn.Conv2d)ì™€ \r\n        activation function(nn.functional.relu, nn.functional.sigmoid)ë“±ì„ ì •ì˜\r\n        \"\"\"\r\n\r\n    def forward(self, x):\r\n        \"\"\"\r\n        ëª¨ë¸ì—ì„œ ì‹¤í–‰ë˜ì–´ì•¼í•˜ëŠ” ê³„ì‚°ì„ ì •ì˜\r\n        \"\"\"\r\n        return x\r\n\r\nexample\r\n\r\nclass LionLinear(nn.Module):\r\n    def __init__(self, input_dim, output_dim):\r\n        super().__init__()\r\n        self.input_dim = input_dim   # ì…ë ¥ì°¨ì› \r\n        self.output_dim = output_dim # ì¶œë ¥ì°¨ì›\r\n        \r\n        self.flatten = nn.Flatten()  # tensor í‰íƒ„í™” ì •ì˜\r\n        self.classifier = nn.Linear(input_dim, output_dim) # Linear layer ì •ì˜\r\n        self.act = nn.ReLU() # activation function(ReLU) ì •ì˜\r\n        \r\n    def forward(self, x):\r\n        x = self.flatten(x)    # dataë¥¼ linear layersì— ë§ê²Œ í‰íƒ„í™” í›„\r\n        x = self.classifier(x) # linear layerë¥¼ í†µê³¼,\r\n        x = self.act(x)        # activation functionì„ í†µí•´ ì¶œë ¥\r\n        \r\n        return x\r\n\r\npytorch ëª¨ë¸ì€ ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ì‰½ê²Œ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n\r\nlinear_model = LionLinear(28*28, 10).to(device)\r\nprint(linear_model)\r\nLionLinear(\r\n  (flatten): Flatten(start_dim=1, end_dim=-1)\r\n  (classifier): Linear(in_features=784, out_features=10, bias=True)\r\n  (act): ReLU()\r\n)\r\nprint(linear_model.classifier)\r\nLinear(in_features=784, out_features=10, bias=True)\r\n\r\në‹¤ìŒì€ MLPë¥¼ êµ¬í˜„í•˜ëŠ” moduleì…ë‹ˆë‹¤.\r\nì½”ë“œë¥¼ ë” ê°„ê²°í•˜ê²Œ í•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì´ ì¼ë¶€ layer ë“±ì„ ë”°ë¡œ ëª¨ë“ˆë¡œ\r\nêµ¬í˜„ í›„ ì „ì²´ ëª¨ë“ˆì— í•©ì³ì„œ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n\r\nclass LionLayer(nn.Module):\r\n    def __init__(self, input_dim, output_dim):\r\n        super().__init__()\r\n        self.input_dim = input_dim\r\n        self.output_dim = output_dim\r\n        self.layer = nn.Linear(self.input_dim, self.output_dim)\r\n        pass\r\n\r\n    def forward(self, x):\r\n        assert x.shape[-1] == self.input_dim, \"Input dimension mismatch\"\r\n        return self.layer(x)\r\n\r\n\r\nclass LionMLP(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim):\r\n        super().__init__()\r\n        self.input_dim = input_dim\r\n        self.hidden_dim = hidden_dim\r\n        self.output_dim = output_dim\r\n        self.flatten = nn.Flatten()\r\n        self.linear_1 = LionLayer(self.input_dim, self.hidden_dim)\r\n        self.linear_2 = LionLayer(self.hidden_dim, self.output_dim)\r\n        self.act_1 = nn.ReLU()\r\n        self.act_2 = nn.Softmax()\r\n        pass\r\n    \r\n    def forward(self, x):\r\n        x = self.flatten(x)\r\n        x = self.act_1(self.linear_1(x))\r\n        x = self.act_2(self.linear_2(x))\r\n        return x\r\n\r\n\r\nmlp = LionMLP(28*28, 50, 10)\r\nprint(mlp)\r\nLionMLP(\r\n  (flatten): Flatten(start_dim=1, end_dim=-1)\r\n  (linear_1): LionLayer(\r\n    (layer): Linear(in_features=784, out_features=50, bias=True)\r\n  )\r\n  (linear_2): LionLayer(\r\n    (layer): Linear(in_features=50, out_features=10, bias=True)\r\n  )\r\n  (act_1): ReLU()\r\n  (act_2): Softmax(dim=None)\r\n)\r\n\r\nnn.Sequential()ì„ í†µí•´ forward() ë¶€ë¶„ì„ ì§§ê²Œ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n\r\nclass LionMLP(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim):\r\n        super().__init__()\r\n        self.input_dim = input_dim\r\n        self.hidden_dim = hidden_dim\r\n        self.output_dim = output_dim\r\n        self.flatten = nn.Flatten()\r\n        self.linear_1 = LionLayer(self.input_dim, self.hidden_dim)\r\n        self.linear_2 = LionLayer(self.hidden_dim, self.output_dim)\r\n        self.act_1 = nn.ReLU()\r\n        self.act_2 = nn.Softmax()\r\n        self.model = nn.Sequential(self.flatten, self.linear_1, self.act_1, self.linear_2, self.act_2)\r\n        pass\r\n    def forward(self, x):\r\n        return self.model(x)\r\n\r\n\r\nmlp = LionMLP(28 * 28, 40, 10)\r\nprint(mlp)\r\nLionMLP(\r\n  (flatten): Flatten(start_dim=1, end_dim=-1)\r\n  (linear_1): LionLayer(\r\n    (layer): Linear(in_features=784, out_features=40, bias=True)\r\n  )\r\n  (linear_2): LionLayer(\r\n    (layer): Linear(in_features=40, out_features=10, bias=True)\r\n  )\r\n  (act_1): ReLU()\r\n  (act_2): Softmax(dim=None)\r\n  (model): Sequential(\r\n    (0): Flatten(start_dim=1, end_dim=-1)\r\n    (1): LionLayer(\r\n      (layer): Linear(in_features=784, out_features=40, bias=True)\r\n    )\r\n    (2): ReLU()\r\n    (3): LionLayer(\r\n      (layer): Linear(in_features=40, out_features=10, bias=True)\r\n    )\r\n    (4): Softmax(dim=None)\r\n  )\r\n)\r\n\r\nëª¨ë¸ íŒŒë¼ë¯¸í„° í™•ì¸\r\n\r\nfor name, param in mlp.named_parameters():\r\n    print(f\"Layer: {name} | Size: {param.size()} | Values: {param[:2]} \\n\")\r\nLayer: linear_1.layer.weight | Size: torch.Size([40, 784]) | Values: tensor([[ 0.0223,  0.0277,  0.0059,  ...,  0.0214, -0.0128,  0.0094],\r\n        [ 0.0003, -0.0235, -0.0014,  ...,  0.0134,  0.0252, -0.0157]],\r\n       grad_fn=<SliceBackward0>) \r\n\r\nLayer: linear_1.layer.bias | Size: torch.Size([40]) | Values: tensor([-0.0290,  0.0290], grad_fn=<SliceBackward0>) \r\n\r\nLayer: linear_2.layer.weight | Size: torch.Size([10, 40]) | Values: tensor([[-1.3287e-01,  1.5117e-01,  1.0537e-01,  7.8630e-02,  1.0438e-01,\r\n         -5.3689e-02, -1.2888e-01,  8.2394e-02, -1.3274e-01, -1.0242e-03,\r\n          9.0983e-03, -4.1699e-03, -1.1986e-02,  1.0621e-01,  1.3446e-01,\r\n         -1.3404e-01,  2.3344e-02,  4.0762e-02, -1.4608e-01, -9.6388e-02,\r\n          7.6862e-02, -1.4166e-01, -1.2884e-01, -1.0316e-01, -1.8399e-02,\r\n          7.4728e-02, -1.3642e-01, -8.7340e-02,  1.3541e-01, -1.5621e-01,\r\n         -3.3360e-02,  1.3945e-01, -1.2610e-01, -1.7838e-02,  8.0318e-02,\r\n          1.0165e-01, -2.3599e-02, -1.8348e-02,  1.2724e-04, -9.8836e-03],\r\n        [ 2.4786e-02,  7.4154e-02,  2.7105e-02, -1.1759e-01, -1.5434e-01,\r\n         -8.9445e-02, -1.6053e-02, -1.3646e-01,  9.9987e-02, -4.5860e-02,\r\n         -1.0728e-01,  1.4374e-01,  6.7146e-02, -7.9271e-03,  1.3654e-01,\r\n          3.3813e-02, -1.3032e-01, -3.9476e-02,  1.4638e-01, -1.4502e-01,\r\n         -9.8445e-02, -7.6872e-02, -1.3876e-01,  4.7213e-02, -6.8424e-02,\r\n         -7.1604e-02, -6.8757e-02,  1.0555e-01, -2.9994e-02,  1.4658e-01,\r\n         -4.7815e-02, -1.0488e-02, -7.3219e-02,  3.6112e-02, -6.6246e-02,\r\n         -1.9281e-02, -2.6451e-02,  6.8400e-02,  1.4304e-01,  1.3780e-01]],\r\n       grad_fn=<SliceBackward0>) \r\n\r\nLayer: linear_2.layer.bias | Size: torch.Size([10]) | Values: tensor([ 0.1439, -0.0609], grad_fn=<SliceBackward0>) \r\n\r\nreference\r\nPyTorchë¡œ ì‹œì‘í•˜ëŠ” ë”¥ ëŸ¬ë‹ ì…ë¬¸ : https://wikidocs.net/57165\r\nhttps://anweh.tistory.com/21\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-22T14:56:20+09:00",
    "input_file": "pytorch-tutorial.knit.md"
  },
  {
    "path": "til/2022-05-19-cotepart1/",
    "title": "[ë°±ì¤€ ë¬¸ì œí’€ì´] ê¸°ë³¸ê¸°",
    "description": "ì½”ë”©í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ë°±ì¤€ ë¬¸ì œ í’€ì–´ë³´ê¸°",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-19",
    "categories": [
      "coding test"
    ],
    "contents": "\r\n\r\nContents\r\nì•½ìˆ˜ êµ¬í•˜ê¸°\r\nì´ì§„ìˆ˜\r\nbin() ì§ì ‘ êµ¬í˜„í•˜ê¸°(ì–‘ì˜\r\nì •ìˆ˜ì¼ë•Œë§Œ)\r\n\r\nìµœëŒ€, ìµœì†Œ\r\n\r\nì•½ìˆ˜ êµ¬í•˜ê¸°\r\në‘ ê°œì˜ ìì—°ìˆ˜ Nê³¼ Kê°€ ì£¼ì–´ì¡Œì„ ë•Œ, Nì˜ ì•½ìˆ˜ë“¤ ì¤‘ Kë²ˆì§¸ë¡œ ì‘ì€ ìˆ˜ë¥¼\r\nì¶œë ¥í•˜ëŠ” í”„ë¡œê·¸ë¨ì„ ì‘ì„±í•˜ì‹œì˜¤.\r\nì…ë ¥\r\nì²«ì§¸ ì¤„ì— Nê³¼ Kê°€ ë¹ˆì¹¸ì„ ì‚¬ì´ì— ë‘ê³  ì£¼ì–´ì§„ë‹¤. Nì€ 1 ì´ìƒ 10,000\r\nì´í•˜ì´ë‹¤. KëŠ” 1 ì´ìƒ N ì´í•˜ì´ë‹¤.\r\nì¶œë ¥\r\nì²«ì§¸ ì¤„ì— Nì˜ ì•½ìˆ˜ë“¤ ì¤‘ Kë²ˆì§¸ë¡œ ì‘ì€ ìˆ˜ë¥¼ ì¶œë ¥í•œë‹¤. ë§Œì¼ Nì˜ ì•½ìˆ˜ì˜\r\nê°œìˆ˜ê°€ Kê°œë³´ë‹¤ ì ì–´ì„œ Kë²ˆì§¸ ì•½ìˆ˜ê°€ ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš°ì—ëŠ” 0ì„\r\nì¶œë ¥í•˜ì‹œì˜¤.\r\n\r\nN, K = map(int, input().split())\r\n\r\nresult = 0\r\n\r\nfor i in range(1, N + 1):\r\n    if N % i == 0:\r\n        K -= 1\r\n        if K == 0:\r\n            result = i\r\n            break\r\n\r\nprint(result)\r\n\r\nì´ì§„ìˆ˜\r\nì–‘ì˜ ì •ìˆ˜ nì´ ì£¼ì–´ì¡Œì„ ë•Œ, ì´ë¥¼ ì´ì§„ìˆ˜ë¡œ ë‚˜íƒ€ëƒˆì„ ë•Œ 1ì˜ ìœ„ì¹˜ë¥¼ ëª¨ë‘\r\nì°¾ëŠ” í”„ë¡œê·¸ë¨ì„ ì‘ì„±í•˜ì‹œì˜¤. ìµœí•˜ìœ„ ë¹„íŠ¸(least significant bit, lsb)ì˜\r\nìœ„ì¹˜ëŠ” 0ì´ë‹¤.\r\nì…ë ¥\r\nì²«ì§¸ ì¤„ì— í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì˜ ê°œìˆ˜ Tê°€ ì£¼ì–´ì§„ë‹¤. ê° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ëŠ” í•œ\r\nì¤„ë¡œ ì´ë£¨ì–´ì ¸ ìˆê³ , nì´ ì£¼ì–´ì§„ë‹¤. \\((1 â‰¤ T â‰¤\r\n10, 1 â‰¤ n â‰¤ 10^6)\\)\r\nì¶œë ¥\r\nê° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì— ëŒ€í•´ì„œ, 1ì˜ ìœ„ì¹˜ë¥¼ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„í•´ì„œ ì¤„ í•˜ë‚˜ì—\r\nì¶œë ¥í•œë‹¤. ìœ„ì¹˜ê°€ ë‚®ì€ ê²ƒë¶€í„° ì¶œë ¥í•œë‹¤.\r\n\r\nt = int(input())\r\n\r\nfor _ in range(t):\r\n    n = int(input())\r\n    b = bin(n)[2:]\r\n    \r\n    for i in range(len(b)):\r\n        if b[::-1][i] == '1':\r\n            print(i, end=' ')\r\n\r\nbin() ì§ì ‘ êµ¬í˜„í•˜ê¸°(ì–‘ì˜\r\nì •ìˆ˜ì¼ë•Œë§Œ)\r\n\r\ndef my_bin(num):\r\n    a = num//2\r\n    b = num%2\r\n    if a == 0:\r\n        return '1'\r\n    else:\r\n        return my_bin(a)+str(b)\r\n\r\nìµœëŒ€, ìµœì†Œ\r\nNê°œì˜ ì •ìˆ˜ê°€ ì£¼ì–´ì§„ë‹¤. ì´ë•Œ, ìµœì†Ÿê°’ê³¼ ìµœëŒ“ê°’ì„ êµ¬í•˜ëŠ” í”„ë¡œê·¸ë¨ì„\r\nì‘ì„±í•˜ì‹œì˜¤.\r\nì…ë ¥\r\nì²«ì§¸ ì¤„ì— ì •ìˆ˜ì˜ ê°œìˆ˜ N (1 â‰¤ N â‰¤ 1,000,000)ì´ ì£¼ì–´ì§„ë‹¤. ë‘˜ì§¸ ì¤„ì—ëŠ”\r\nNê°œì˜ ì •ìˆ˜ë¥¼ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„í•´ì„œ ì£¼ì–´ì§„ë‹¤. ëª¨ë“  ì •ìˆ˜ëŠ” -1,000,000ë³´ë‹¤\r\ní¬ê±°ë‚˜ ê°™ê³ , 1,000,000ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ì€ ì •ìˆ˜ì´ë‹¤.\r\nì¶œë ¥\r\nì²«ì§¸ ì¤„ì— ì£¼ì–´ì§„ ì •ìˆ˜ Nê°œì˜ ìµœì†Ÿê°’ê³¼ ìµœëŒ“ê°’ì„ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„í•´\r\nì¶œë ¥í•œë‹¤.\r\n\r\ninput()\r\nl = list(map(int, input().split()))\r\n\r\nhigh = low = l[0]\r\n\r\nfor i in l[1:]:\r\n    if high < i:\r\n        high = i\r\n    if low > i:\r\n        low = i\r\n        \r\nprint(low, high)\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-20T19:04:09+09:00",
    "input_file": {}
  },
  {
    "path": "til/2022-05-18-dlfromscratchch2/",
    "title": "í¼ì…‰íŠ¸ë¡ ì´ë€",
    "description": "ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹ì„ ì½ê³  ë”¥ëŸ¬ë‹ì˜ ê¸°ì›ì´ ë˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì¸ í¼ì…‰íŠ¸ë¡ ì— ëŒ€í•´ ì •ë¦¬í•´ë´¤ìŠµë‹ˆë‹¤.",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-18",
    "categories": [
      "ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹"
    ],
    "contents": "\r\n\r\nContents\r\n1ï¸âƒ£ í¼ì…‰íŠ¸ë¡ ì´ë€\r\n2ï¸âƒ£ í¼ì…‰íŠ¸ë¡ ì˜ í•œê³„\r\n3ï¸âƒ£ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ \r\nâœ… ìš”ì•½\r\n\r\n1ï¸âƒ£ í¼ì…‰íŠ¸ë¡ ì´ë€\r\ní¼ì…‰íŠ¸ë¡ ì€ ë‹¤ìˆ˜ì˜ ì‹ í˜¸ë¥¼ ì…ë ¥(input)ìœ¼ë¡œ ë°›ì•„ í•˜ë‚˜ì˜ ì‹ í˜¸ë¥¼\r\nì¶œë ¥(output)í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.\r\n2ê°œì˜ ì…ë ¥ì„ ë°›ì•„ í•˜ë‚˜ì˜ ì‹ í˜¸ë¥¼ ì¶œë ¥í•˜ëŠ” í¼ì…‰íŠ¸ë¡ ì„ ìˆ˜ì‹ìœ¼ë¡œ\r\në‚˜íƒ€ë‚´ë©´,\r\n\\[y = \\begin{cases}0,(w_{1}x_{1} +\r\nw_{2}x_{2}\\leq\\theta)\\\\1,(w_{1}x_{1} +\r\nw_{2}x_{2}>\\theta)\\\\\\end{cases}\\] ë˜ëŠ” \\(\\theta\\)ë¥¼ \\(-b\\)ë¡œ ì¹˜í™˜í•˜ì—¬,\r\n\\[y = \\begin{cases}0,(b+w_{1}x_{1} +\r\nw_{2}x_{2}\\leq0)\\\\1,(b+w_{1}x_{1} +\r\nw_{2}x_{2}>0)\\\\\\end{cases}\\]\r\n\\(w\\) : weight, \\(b\\): bias\r\n2ï¸âƒ£ í¼ì…‰íŠ¸ë¡ ì˜ í•œê³„\r\ní¼ì…‰íŠ¸ë¡ ì´ í’€ ìˆ˜ ìˆëŠ” (ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ”) ë¬¸ì œëŠ” AND, NAND,\r\nORì…ë‹ˆë‹¤.\r\nAND\r\n\r\nx1x2y000100010111\r\n\r\nNAND\r\n\r\nx1x2y001101011110\r\n\r\nOR\r\n\r\nx1x2y000101011111\r\n\r\nìœ„ ì„¸ê°€ì§€ ë¬¸ì œëŠ” \\((w_1, w_2,\r\ntheta)\\)ì— ê°ê° \\((0.5, 0.5, 0.7),\r\n(-0.5, -0.5, -0.7), (1.5, 1.5, 0.7)\\) ë“±ì„ ëŒ€ì…í•˜ë©´ í’€ ìˆ˜\r\nìˆìŠµë‹ˆë‹¤.\r\nì´ë¥¼ íŒŒì´ì¬ í•¨ìˆ˜ë¡œ êµ¬í˜„í•˜ë©´,\r\n\r\ndef AND(x1, x2):\r\n    w1, w2, theta = 0.5, 0.5, 0.5\r\n    tmp = x1*w1+x2*w2\r\n    if tmp <= theta:\r\n        return 0\r\n    else:\r\n        return 1\r\n\r\ndef NAND(x1, x2):\r\n    w1, w2, theta = -0.5, -0.5, -0.7\r\n    tmp = x1*w1+x2*w2\r\n    if tmp <= theta:\r\n        return 0\r\n    else:\r\n        return 1\r\n    \r\ndef OR(x1, x2):\r\n    w1, w2, theta = 1.5, 1.5, 0.7\r\n    tmp = x1*w1+x2*w2\r\n    if tmp <= theta:\r\n        return 0\r\n    else:\r\n        return 1\r\n\r\nprint('x1=0, x2=0 : ', AND(0,0))\r\nx1=0, x2=0 :  0\r\nprint('x1=0, x2=1 : ', AND(0,1))\r\nx1=0, x2=1 :  0\r\nprint('x1=1, x2=0 : ', AND(1,0))\r\nx1=1, x2=0 :  0\r\nprint('x1=1, x2=1 : ', AND(1,1))\r\nx1=1, x2=1 :  1\r\nprint('x1=0, x2=0 : ', NAND(0,0))\r\nx1=0, x2=0 :  1\r\nprint('x1=0, x2=1 : ', NAND(0,1))\r\nx1=0, x2=1 :  1\r\nprint('x1=1, x2=0 : ', NAND(1,0))\r\nx1=1, x2=0 :  1\r\nprint('x1=1, x2=1 : ', NAND(1,1))\r\nx1=1, x2=1 :  0\r\nprint('x1=0, x2=0 : ', OR(0,0))\r\nx1=0, x2=0 :  0\r\nprint('x1=0, x2=1 : ', OR(0,1))\r\nx1=0, x2=1 :  1\r\nprint('x1=1, x2=0 : ', OR(1,0))\r\nx1=1, x2=0 :  1\r\nprint('x1=1, x2=1 : ', OR(1,1))\r\nx1=1, x2=1 :  1\r\n\r\ní•˜ì§€ë§Œ, í¼ì…‰íŠ¸ë¡ ìœ¼ë¡œ XORì€ í•´ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\r\nXOR\r\n\r\nx1x2y000101011110\r\n\r\nìœ„ ë¬¸ì œë¥¼ ê·¸ë¦¼ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ì´ ë¹¨ê°„ì ê³¼ ê²€ì€ì ì„ êµ¬ë¶„í•  ìˆ˜\r\nìˆëŠ” ì„ ì„ ê·¸ì„ ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\r\n\r\n\r\n\r\nê³¡ì„ ìœ¼ë¡œ ì„ ì„ ê·¸ë¦°ë‹¤ë©´ êµ¬ë¶„í•  ìˆ˜ ìˆê² ì§€ë§Œ í¼ì…‰íŠ¸ë¡ ìœ¼ë¡œëŠ” ì§ì„ ë°–ì—\r\nê·¸ë¦´ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ í•´ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¦‰, í¼ì…‰íŠ¸ë¡ ì€ ë¹„ì„ í˜•ì˜ ì˜ì—­ì„\r\ní‘œí˜„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\r\n3ï¸âƒ£ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ \r\nì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‚˜ì˜¨ ê²ƒì´ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ ì…ë‹ˆë‹¤. ë§ ê·¸ëŒ€ë¡œ í¼ì…‰íŠ¸ë¡ \r\nì¸µì„ ì—¬ëŸ¬ ê°œ ê°–ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë©° ì•ì—ì„œ ì–¸ê¸‰í–ˆë˜ í¼ì…‰íŠ¸ë¡ ì€ ì •í™•íˆ ë§í•˜ë©´\r\në‹¨ì¸µ í¼ì…‰íŠ¸ë¡ ì…ë‹ˆë‹¤.\r\nXORë¬¸ì œë¥¼ í•´ê²°í•˜ë ¤ë©´ ìœ„ì—ì„œ AND, NAND, OR ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´\r\në§Œë“¤ì—ˆë˜ í¼ì…‰íŠ¸ë¡ ì„ ì ì ˆíˆ ìŒ“ì•„ ë§Œë“¤ë©´ ë©ë‹ˆë‹¤.\r\n\r\ndef XOR(x1, x2):\r\n    s1 = NAND(x1, x2)\r\n    s2 = OR(x1, x2)\r\n    y = AND(s1, s2)\r\n    return y\r\n\r\nprint('x1=0, x2=0 : ', XOR(0,0))\r\nx1=0, x2=0 :  0\r\nprint('x1=0, x2=1 : ', XOR(0,1))\r\nx1=0, x2=1 :  1\r\nprint('x1=1, x2=0 : ', XOR(1,0))\r\nx1=1, x2=0 :  1\r\nprint('x1=1, x2=1 : ', XOR(1,1))\r\nx1=1, x2=1 :  0\r\n\r\nì²«ë²ˆì§¸ ì¸µì— NAND, ORë¥¼ í’€ê¸° ìœ„í•´ ë§Œë“¤ì—ˆë˜ í¼ì…‰íŠ¸ë¡ ì„ ìŒ“ê³  ë‘ë²ˆì§¸ ì¸µì—\r\nANDë¥¼ í’€ê¸° ìœ„í•´ ë§Œë“¤ì—ˆë˜ í¼ì…‰íŠ¸ë¡ ì„ ìŒ“ì•„ XORë¬¸ì œë¥¼ í•´ê²°í–ˆìŠµë‹ˆë‹¤.\r\nì´ì²˜ëŸ¼ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ ì€ ë¹„ì„ í˜•ì˜ ì˜ì—­ê¹Œì§€ë„ í‘œí˜„í•  ìˆ˜ ìˆìŒì„ ì•Œ ìˆ˜\r\nìˆìŠµë‹ˆë‹¤.\r\nâœ… ìš”ì•½\r\ní¼ì…‰íŠ¸ë¡ ì€ ì…ì¶œë ¥ì„ ê°–ì¶˜ ì•Œê³ ë¦¬ì¦˜\r\në‹¨ì¸µ í¼ì…‰íŠ¸ë¡ ì€ ì„ í˜• ì˜ì—­ë§Œ í‘œí˜„í•  ìˆ˜ ìˆê³ , ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ ì€\r\në¹„ì„ í˜• ì˜ì—­ë„ í‘œí˜„ ê°€ëŠ¥\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-19T00:59:30+09:00",
    "input_file": {}
  },
  {
    "path": "til/2022-05-13-first-til/",
    "title": "TIL(Today I Learn) í˜ì´ì§€ ê°œì„¤!!",
    "description": "ë§¤ì¼ ê³µë¶€í•˜ëŠ” ê²ƒì„ ê¸°ë¡í•˜ê¸°",
    "author": [
      {
        "name": "nackta",
        "url": {}
      }
    ],
    "date": "2022-05-13",
    "categories": [],
    "contents": "\r\në§¤ì¼ë§¤ì¼ ê³µë¶€í•œ ê²ƒì„ ê¸°ë¡í•˜ëŠ” í˜ì´ì§€ë¥¼ ë§Œë“¤ì—ˆë‹¤. ì¼ê¸°ì²˜ëŸ¼ ë‚˜ì¤‘ì— ë´ë„\r\në¿Œë“¯í•œ í˜ì´ì§€ê°€ ë˜ê¸¸..\r\nğŸ˜„í™”ì´íŒ…!!\r\n\r\n\r\n\r\n",
    "preview": "til/2022-05-13-first-til/images/owl.png",
    "last_modified": "2022-05-16T00:44:05+09:00",
    "input_file": {},
    "preview_width": 640,
    "preview_height": 514
  }
]

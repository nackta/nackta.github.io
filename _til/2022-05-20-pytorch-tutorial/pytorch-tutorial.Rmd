---
title: "Pytorch tutorial"
description: |
  딥러닝 프레임워크 파이토치에 대해 알아보자
author:
  - name: nackta
    url: {}
date: 2022-05-20
categories:
  - 밑바닥부터 시작하는 딥러닝
output:
  distill::distill_article:
    self_contained: false
    toc: true
---

```{r}
matplotlib <- reticulate::import("matplotlib")
matplotlib$use("Agg", force = TRUE)
```

```{python}
import numpy as np

import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader

from torchvision import datasets, transforms

import matplotlib.pyplot as plt
```




## torch tensor

tensor는 numpy의 ndarray처럼 다차원 데이터 배열입니다. tensor를 생성할 때는 list를 사용할 수도 있지만 ndarray를 사용할 수도 있습니다.

ndarray와 tensor의 차이점은 tensor에는 back propagation(역전파)를 다루기 위해 forward pass(순전파)에서 전달된 값과 연산의 종류를 기억할 수 있습니다.

### tensor의 생성

```{python}
a = torch.tensor([[1.0, 4.0], [4.0, 3.0]])
b = torch.tensor([[4, 3], [1, 4], [1, 2]])
print(a)
print(b)
```

ndarray와 비슷한 생성 방법입니다.

```{python}
print(a.dtype, a.shape)
print(b.dtype, b.shape)
```
- torch.ones(*size) : 모든 원소가 1인 tensor

- torch.zeros(*size) : 모든 원소가 0인 tensor

- torch.eye(n, m) : 대각 원소가 1이고 나머지가 0인 $n*m$ tensor를 생성. m = None이면 $n*n$tensor를 생성

- torch.rand(*size) : 모든 원소를 랜덤한 값으로 채워진 tensor. dtype을 int로 지정시 에러가 발생

```{python}
a = torch.ones([2, 3])
b = torch.zeros([3, 2], dtype=torch.int64)
c = torch.eye(4,3)
d = torch.rand([2, 4, 3], dtype=torch.float)

print(a)
print(b)
print(c)
print(d)
```

앞서 언급했듯이 ndarray로도 tensor를 생성할 수 있다. as_tensor()와 from_numpy()는 원본의 값을 참조하기 때문에 원본의 값을 바꾸면 같이 변하는 것을 알 수 있습니다.

```{python}
import numpy as np

d = np.array([[2, 5, 4], [4, 2, 1], [5, 7, 4]])

a = torch.tensor(d)
b = torch.as_tensor(d)
c = torch.from_numpy(d)

print(a, id(a))
print(b, id(b))
print(c, id(c))
print(d, id(d))

d[0,0] = 0

print(a)
print(b)
print(c)
print(d)
```

tensor.shape을 통해 동일한 size의 tensor를 만들 수도 있지만 _like(tensor) 방법으로도 동일한 size의 tensor를 만들 수 있습니다.

```{python}
a = torch.ones([2, 3])
print(a)
b = torch.ones_like(a)
c = torch.zeros_like(a, dtype=torch.float)
d = torch.rand_like(a, dtype=torch.float)

print(b)
print(c)
print(d)
```

### device 지정

gpu연산을 위해서는 tensor의 device를 cuda로 지정해줘야합니다.
`torch.cuda.is_available()`를 통해 gpu가 사용 가능한지 확인 후 tensor.to('cuda')를 통해 device를 지정할 수 있습니다.

```{python}
torch.cuda.is_available() # 저는 gpu가 없어요 ㅠㅜ

if torch.cuda.is_available():
    device = "cuda"
else:
    device = "cpu"
device
```

### tensor 다루기

```{python}
a = torch.rand([2, 3])
b = torch.rand([2, 3])
```



```{python}
# concat
c = torch.cat([a, b], dim=0)
d = torch.cat([a, b], dim=1)
print(c)
print(c.shape)
print(d)
print(d.shape)
```

```{python}
# stack
c = torch.stack([a, b], dim=0)
d = torch.stack([a, b], dim=1)
e = torch.stack([a, b], dim=2)
print(c)
print(c.shape)
print(d)
print(d.shape)
print(e)
print(e.shape)
```

```{python}
# hstack and vstack
c = torch.hstack([a, b])
print(c)
print(c.shape)
d = torch.vstack([a, b])
print(d)
print(d.shape)
```

```{python}
# unsqueeze and squeeze
print(a)
a = torch.unsqueeze(a, dim=1)
print(a)
print(a.shape)

print(torch.squeeze(a))
print(a.shape)
```

```{python}
# matrix multiplication
a = torch.tensor(np.array(list(range(12)))).reshape(3, 4)
print(a)
b = torch.tensor(np.array(list(range(8)))).reshape(4, 2)
print(b)

c = a @ b
print(c)

d = torch.matmul(a, b)
print(d)
```

```{python}
# element-wise product
a = torch.tensor(np.array(list(range(6)))).reshape(2, 3)
print(a)
b = torch.tensor(list(range(10, 13))).reshape(1, 3)
print(b)

c = a * b
print(c)

d = a.mul(b)
print(d)
```

```{python}
# item
agg = d.sum()
v = agg.item()
print(v, type(v))
```

```{python}
# inplace operations
print(a)
a.add_(5)
print(a)
a.add_(b)
print(a)
```

## dataset and dataloader

```{python}
# Custom dataset

class LionDataset(Dataset):
  def __init__(self, data, target, transform=None, target_transform=None):
    self.data = data
    self.target = target
    self.transform = transform
    self.target_transform = target_transform
    pass

  def __len__(self):
    return len(self.data)

  def __getitem__(self, idx):
    x = self.data[idx]
    y = self.target[idx]

    if self.transform:
      x = self.transform(x)
    if self.target_transform:
      y = self.target_transform(y)

    return x, y
```

```{python}
data = np.array(list(range(100)), dtype=np.float).reshape(-1, 2)
target = np.array([[i] * 5 for i in range(10)]).reshape(-1)

print(data)
print(target)
```

```{python}
data = np.array(list(range(100)), dtype=np.float).reshape(-1, 2)
target = np.array([[i] * 5 for i in range(10)]).reshape(-1)

print(data)
print(target)
```

```{python}
lion = LionDataset(data=data, target=target)

print(lion[12])
print(lion[27])
```

```{python}
loader = DataLoader(dataset=lion, batch_size=10, shuffle=True)

for i, batch in enumerate(loader):
  x, y = batch
  if i == 0:
    print(x)
    print(y)
  print(x.shape)
```

```{python}
ds = datasets.FashionMNIST(root = ".", train=True, download=True, transform=transforms.ToTensor())
```

```{python}
loader = DataLoader(dataset=ds, batch_size=256, shuffle=True)
batch = next(iter(loader))

fig = plt.figure(figsize=(10, 10))
for i in range(9):
  x = batch[0][i]
  ax = fig.add_subplot(3, 3, i+1)
  ax.imshow(x.squeeze(), cmap="gray")
fig.show()
```

## torch.nn.Module

```{python}
class LionLinear(nn.Module):
  def __init__(self, input_dim, output_dim):
    super().__init__()
    
    self.input_dim = input_dim
    self.output_dim = output_dim

    self.flatten = nn.Flatten()
    self.classifier = nn.Linear(input_dim, output_dim)
    self.act = nn.ReLU()

  def forward(self, x):
    x = self.flatten(x)
    x = self.classifier(x)
    x = self.act(x)
    return x
```


```{python}
linear_model = LionLinear(28*28, 10).to(device)
print(linear_model)
print(linear_model.classifier)
```


```{python}
X = torch.rand(5, 28, 28).to(device)
print(X.shape)
y = linear_model(X)
print(y.shape)
prob = nn.Softmax(dim=1)(y)
print(prob)
y_hat = prob.argmax(1)
print(y_hat)
```


```{python}
class LionFlatten(nn.Module):
  def __init__(self):
    super().__init__()
    pass

  def forward(self, x):
    return x.reshape(x.shape[0], -1)
```


```{python}
print(X.shape)
fla = LionFlatten()
print(fla(X).shape)
```


```{python}
class LionLayer(nn.Module):
  def __init__(self, input_dim, output_dim):
    super().__init__()
    
    self.input_dim = input_dim
    self.output_dim = output_dim

    self.layer = nn.Linear(self.input_dim, self.output_dim)
    pass

  def forward(self, x):
    assert x.shape[-1] == self.input_dim, "Input dimension mismatch"
    return self.layer(x)
```


```{python}
lin = LionLayer(28 * 28, 20).to(device)
print(lin)
print(lin(fla(X)).shape)
```

```{python}
class LionMLP(nn.Module):
  def __init__(self, input_dim, hidden_dim, output_dim):
    super().__init__()

    self.input_dim = input_dim
    self.hidden_dim = hidden_dim
    self.output_dim = output_dim

    self.flatten = LionFlatten()
    self.linear_1 = LionLayer(self.input_dim, self.hidden_dim)
    self.linear_2 = LionLayer(self.hidden_dim, self.output_dim)
    self.act_1 = nn.ReLU()
    self.act_2 = nn.Softmax()
    pass

  def forward(self, x):
    x = self.flatten(x)
    x = self.act_1(self.linear_1(x))
    x = self.act_2(self.linear_2(x))
    return x

```


```{python}
mlp = LionMLP(28*28, 50, 10)
print(mlp)
```

```{python}
class LionMLP(nn.Module):
  def __init__(self, input_dim, hidden_dim, output_dim):
    super().__init__()

    self.input_dim = input_dim
    self.hidden_dim = hidden_dim
    self.output_dim = output_dim

    self.flatten = LionFlatten()
    self.linear_1 = LionLayer(self.input_dim, self.hidden_dim)
    self.linear_2 = LionLayer(self.hidden_dim, self.output_dim)
    self.act_1 = nn.ReLU()
    self.act_2 = nn.Softmax()

    self.model = nn.Sequential(self.flatten, self.linear_1, self.act_1, self.linear_2, self.act_2)
    pass

  def forward(self, x):
    return self.model(x)
```

```{python}
mlp = LionMLP(28 * 28, 40, 10)
print(mlp)
```

```{python}
for name, param in mlp.named_parameters():
  print(f"Layer: {name} | Size: {param.size()} | Values: {param[:2]} \n")
```


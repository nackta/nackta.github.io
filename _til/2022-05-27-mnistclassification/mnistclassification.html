<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { color: #00769e; background-color: #f1f3f5; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span { color: #00769e; } /* Normal */
code span.al { color: #ad0000; } /* Alert */
code span.an { color: #5e5e5e; } /* Annotation */
code span.at { color: #657422; } /* Attribute */
code span.bn { color: #ad0000; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #00769e; } /* ControlFlow */
code span.ch { color: #20794d; } /* Char */
code span.cn { color: #8f5902; } /* Constant */
code span.co { color: #5e5e5e; } /* Comment */
code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
code span.dt { color: #ad0000; } /* DataType */
code span.dv { color: #ad0000; } /* DecVal */
code span.er { color: #ad0000; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #ad0000; } /* Float */
code span.fu { color: #4758ab; } /* Function */
code span.im { } /* Import */
code span.in { color: #5e5e5e; } /* Information */
code span.kw { color: #00769e; } /* Keyword */
code span.op { color: #5e5e5e; } /* Operator */
code span.ot { color: #00769e; } /* Other */
code span.pp { color: #ad0000; } /* Preprocessor */
code span.sc { color: #5e5e5e; } /* SpecialChar */
code span.ss { color: #20794d; } /* SpecialString */
code span.st { color: #20794d; } /* String */
code span.va { color: #111111; } /* Variable */
code span.vs { color: #20794d; } /* VerbatimString */
code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
</style>


  <!--radix_placeholder_meta_tags-->
  <title>pytorch로 mnist 분류하기</title>

  <meta property="description" itemprop="description" content="pytorch로 mnist 분류 MLP모델을 구현하고 optimizer, lr scheduler, regularization, dropout, batch normalization의 적용 전 후 성능차이를 비교해봤습니다."/>


  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2022-05-27"/>
  <meta property="article:created" itemprop="dateCreated" content="2022-05-27"/>
  <meta name="article:author" content="nackta"/>

  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="pytorch로 mnist 분류하기"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="pytorch로 mnist 분류 MLP모델을 구현하고 optimizer, lr scheduler, regularization, dropout, batch normalization의 적용 전 후 성능차이를 비교해봤습니다."/>
  <meta property="og:locale" content="en_US"/>

  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="pytorch로 mnist 분류하기"/>
  <meta property="twitter:description" content="pytorch로 mnist 분류 MLP모델을 구현하고 optimizer, lr scheduler, regularization, dropout, batch normalization의 적용 전 후 성능차이를 비교해봤습니다."/>

  <!--/radix_placeholder_meta_tags-->
  <!--radix_placeholder_rmarkdown_metadata-->

  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","date","preview","categories","output"]}},"value":[{"type":"character","attributes":{},"value":["pytorch로 mnist 분류하기"]},{"type":"character","attributes":{},"value":["pytorch로 mnist 분류 MLP모델을 구현하고 optimizer, lr scheduler, regularization, dropout, batch normalization의 적용 전 후 성능차이를 비교해봤습니다. \n"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url"]}},"value":[{"type":"character","attributes":{},"value":["nackta"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":[]}},"value":[]}]}]},{"type":"character","attributes":{},"value":["2022-05-27"]},{"type":"character","attributes":{},"value":["output_15_0.png"]},{"type":"character","attributes":{},"value":["pytorch"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained","toc"]}},"value":[{"type":"logical","attributes":{},"value":[false]},{"type":"logical","attributes":{},"value":[true]}]}]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["mnistclassification_files/anchor-4.2.2/anchor.min.js","mnistclassification_files/bowser-1.9.3/bowser.min.js","mnistclassification_files/distill-2.2.21/template.v2.js","mnistclassification_files/header-attrs-2.14/header-attrs.js","mnistclassification_files/jquery-3.6.0/jquery-3.6.0.js","mnistclassification_files/jquery-3.6.0/jquery-3.6.0.min.js","mnistclassification_files/jquery-3.6.0/jquery-3.6.0.min.map","mnistclassification_files/popper-2.6.0/popper.min.js","mnistclassification_files/tippy-6.2.7/tippy-bundle.umd.min.js","mnistclassification_files/tippy-6.2.7/tippy-light-border.css","mnistclassification_files/tippy-6.2.7/tippy.css","mnistclassification_files/tippy-6.2.7/tippy.umd.min.js","mnistclassification_files/webcomponents-2.0.0/webcomponents.js","output_15_0.png","output_31_0.png","output_44_1.png","output_46_1.png","output_49_1.png","output_54_1.png","output_55_1.png","output_56_1.png"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

  <style type="text/css">

  body {
    background-color: white;
  }

  .pandoc-table {
    width: 100%;
  }

  .pandoc-table>caption {
    margin-bottom: 10px;
  }

  .pandoc-table th:not([align]) {
    text-align: left;
  }

  .pagedtable-footer {
    font-size: 15px;
  }

  d-byline .byline {
    grid-template-columns: 2fr 2fr;
  }

  d-byline .byline h3 {
    margin-block-start: 1.5em;
  }

  d-byline .byline .authors-affiliations h3 {
    margin-block-start: 0.5em;
  }

  .authors-affiliations .orcid-id {
    width: 16px;
    height:16px;
    margin-left: 4px;
    margin-right: 4px;
    vertical-align: middle;
    padding-bottom: 2px;
  }

  d-title .dt-tags {
    margin-top: 1em;
    grid-column: text;
  }

  .dt-tags .dt-tag {
    text-decoration: none;
    display: inline-block;
    color: rgba(0,0,0,0.6);
    padding: 0em 0.4em;
    margin-right: 0.5em;
    margin-bottom: 0.4em;
    font-size: 70%;
    border: 1px solid rgba(0,0,0,0.2);
    border-radius: 3px;
    text-transform: uppercase;
    font-weight: 500;
  }

  d-article table.gt_table td,
  d-article table.gt_table th {
    border-bottom: none;
  }

  .html-widget {
    margin-bottom: 2.0em;
  }

  .l-screen-inset {
    padding-right: 16px;
  }

  .l-screen .caption {
    margin-left: 10px;
  }

  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }

  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }

  .shaded .shaded-content {
    background: white;
  }

  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }

  .hidden {
    display: none !important;
  }

  d-article {
    padding-top: 2.5rem;
    padding-bottom: 30px;
  }

  d-appendix {
    padding-top: 30px;
  }

  d-article>p>img {
    width: 100%;
  }

  d-article h2 {
    margin: 1rem 0 1.5rem 0;
  }

  d-article h3 {
    margin-top: 1.5rem;
  }

  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }

  /* Tweak code blocks */

  d-article div.sourceCode code,
  d-article pre code {
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  }

  d-article pre,
  d-article div.sourceCode,
  d-article div.sourceCode pre {
    overflow: auto;
  }

  d-article div.sourceCode {
    background-color: white;
  }

  d-article div.sourceCode pre {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }

  d-article pre {
    font-size: 12px;
    color: black;
    background: none;
    margin-top: 0;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;

    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;

    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }

  d-article pre a {
    border-bottom: none;
  }

  d-article pre a:hover {
    border-bottom: none;
    text-decoration: underline;
  }

  d-article details {
    grid-column: text;
    margin-bottom: 0.8em;
  }

  @media(min-width: 768px) {

  d-article pre,
  d-article div.sourceCode,
  d-article div.sourceCode pre {
    overflow: visible !important;
  }

  d-article div.sourceCode pre {
    padding-left: 18px;
    font-size: 14px;
  }

  d-article pre {
    font-size: 14px;
  }

  }

  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }

  /* CSS for d-contents */

  .d-contents {
    grid-column: text;
    color: rgba(0,0,0,0.8);
    font-size: 0.9em;
    padding-bottom: 1em;
    margin-bottom: 1em;
    padding-bottom: 0.5em;
    margin-bottom: 1em;
    padding-left: 0.25em;
    justify-self: start;
  }

  @media(min-width: 1000px) {
    .d-contents.d-contents-float {
      height: 0;
      grid-column-start: 1;
      grid-column-end: 4;
      justify-self: center;
      padding-right: 3em;
      padding-left: 2em;
    }
  }

  .d-contents nav h3 {
    font-size: 18px;
    margin-top: 0;
    margin-bottom: 1em;
  }

  .d-contents li {
    list-style-type: none
  }

  .d-contents nav > ul {
    padding-left: 0;
  }

  .d-contents ul {
    padding-left: 1em
  }

  .d-contents nav ul li {
    margin-top: 0.6em;
    margin-bottom: 0.2em;
  }

  .d-contents nav a {
    font-size: 13px;
    border-bottom: none;
    text-decoration: none
    color: rgba(0, 0, 0, 0.8);
  }

  .d-contents nav a:hover {
    text-decoration: underline solid rgba(0, 0, 0, 0.6)
  }

  .d-contents nav > ul > li > a {
    font-weight: 600;
  }

  .d-contents nav > ul > li > ul {
    font-weight: inherit;
  }

  .d-contents nav > ul > li > ul > li {
    margin-top: 0.2em;
  }


  .d-contents nav ul {
    margin-top: 0;
    margin-bottom: 0.25em;
  }

  .d-article-with-toc h2:nth-child(2) {
    margin-top: 0;
  }


  /* Figure */

  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }

  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }

  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }

  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }

  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }

  /* Citations */

  d-article .citation {
    color: inherit;
    cursor: inherit;
  }

  div.hanging-indent{
    margin-left: 1em; text-indent: -1em;
  }

  /* Citation hover box */

  .tippy-box[data-theme~=light-border] {
    background-color: rgba(250, 250, 250, 0.95);
  }

  .tippy-content > p {
    margin-bottom: 0;
    padding: 2px;
  }


  /* Tweak 1000px media break to show more text */

  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }

    .grid {
      grid-column-gap: 16px;
    }

    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }

  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }

    .grid {
      grid-column-gap: 32px;
    }
  }


  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */

  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  /* Include appendix styles here so they can be overridden */

  d-appendix {
    contain: layout style;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-top: 60px;
    margin-bottom: 0;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    color: rgba(0,0,0,0.5);
    padding-top: 60px;
    padding-bottom: 48px;
  }

  d-appendix h3 {
    grid-column: page-start / text-start;
    font-size: 15px;
    font-weight: 500;
    margin-top: 1em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.65);
  }

  d-appendix h3 + * {
    margin-top: 1em;
  }

  d-appendix ol {
    padding: 0 0 0 15px;
  }

  @media (min-width: 768px) {
    d-appendix ol {
      padding: 0 0 0 30px;
      margin-left: -30px;
    }
  }

  d-appendix li {
    margin-bottom: 1em;
  }

  d-appendix a {
    color: rgba(0, 0, 0, 0.6);
  }

  d-appendix > * {
    grid-column: text;
  }

  d-appendix > d-footnote-list,
  d-appendix > d-citation-list,
  d-appendix > distill-appendix {
    grid-column: screen;
  }

  /* Include footnote styles here so they can be overridden */

  d-footnote-list {
    contain: layout style;
  }

  d-footnote-list > * {
    grid-column: text;
  }

  d-footnote-list a.footnote-backlink {
    color: rgba(0,0,0,0.3);
    padding-left: 0.5em;
  }



  /* Anchor.js */

  .anchorjs-link {
    /*transition: all .25s linear; */
    text-decoration: none;
    border-bottom: none;
  }
  *:hover > .anchorjs-link {
    margin-left: -1.125em !important;
    text-decoration: none;
    border-bottom: none;
  }

  /* Social footer */

  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }

  .disqus-comments {
    margin-right: 30px;
  }

  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }

  #disqus_thread {
    margin-top: 30px;
  }

  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }

  .article-sharing a:hover {
    border-bottom: none;
  }

  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }

  .subscribe p {
    margin-bottom: 0.5em;
  }


  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }


  .sidebar-section.custom {
    font-size: 12px;
    line-height: 1.6em;
  }

  .custom p {
    margin-bottom: 0.5em;
  }

  /* Styles for listing layout (hide title) */
  .layout-listing d-title, .layout-listing .d-title {
    display: none;
  }

  /* Styles for posts lists (not auto-injected) */


  .posts-with-sidebar {
    padding-left: 45px;
    padding-right: 45px;
  }

  .posts-list .description h2,
  .posts-list .description p {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  }

  .posts-list .description h2 {
    font-weight: 700;
    border-bottom: none;
    padding-bottom: 0;
  }

  .posts-list h2.post-tag {
    border-bottom: 1px solid rgba(0, 0, 0, 0.2);
    padding-bottom: 12px;
  }
  .posts-list {
    margin-top: 60px;
    margin-bottom: 24px;
  }

  .posts-list .post-preview {
    text-decoration: none;
    overflow: hidden;
    display: block;
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
    padding: 24px 0;
  }

  .post-preview-last {
    border-bottom: none !important;
  }

  .posts-list .posts-list-caption {
    grid-column: screen;
    font-weight: 400;
  }

  .posts-list .post-preview h2 {
    margin: 0 0 6px 0;
    line-height: 1.2em;
    font-style: normal;
    font-size: 24px;
  }

  .posts-list .post-preview p {
    margin: 0 0 12px 0;
    line-height: 1.4em;
    font-size: 16px;
  }

  .posts-list .post-preview .thumbnail {
    box-sizing: border-box;
    margin-bottom: 24px;
    position: relative;
    max-width: 500px;
  }
  .posts-list .post-preview img {
    width: 100%;
    display: block;
  }

  .posts-list .metadata {
    font-size: 12px;
    line-height: 1.4em;
    margin-bottom: 18px;
  }

  .posts-list .metadata > * {
    display: inline-block;
  }

  .posts-list .metadata .publishedDate {
    margin-right: 2em;
  }

  .posts-list .metadata .dt-authors {
    display: block;
    margin-top: 0.3em;
    margin-right: 2em;
  }

  .posts-list .dt-tags {
    display: block;
    line-height: 1em;
  }

  .posts-list .dt-tags .dt-tag {
    display: inline-block;
    color: rgba(0,0,0,0.6);
    padding: 0.3em 0.4em;
    margin-right: 0.2em;
    margin-bottom: 0.4em;
    font-size: 60%;
    border: 1px solid rgba(0,0,0,0.2);
    border-radius: 3px;
    text-transform: uppercase;
    font-weight: 500;
  }

  .posts-list img {
    opacity: 1;
  }

  .posts-list img[data-src] {
    opacity: 0;
  }

  .posts-more {
    clear: both;
  }


  .posts-sidebar {
    font-size: 16px;
  }

  .posts-sidebar h3 {
    font-size: 16px;
    margin-top: 0;
    margin-bottom: 0.5em;
    font-weight: 400;
    text-transform: uppercase;
  }

  .sidebar-section {
    margin-bottom: 30px;
  }

  .categories ul {
    list-style-type: none;
    margin: 0;
    padding: 0;
  }

  .categories li {
    color: rgba(0, 0, 0, 0.8);
    margin-bottom: 0;
  }

  .categories li>a {
    border-bottom: none;
  }

  .categories li>a:hover {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  }

  .categories .active {
    font-weight: 600;
  }

  .categories .category-count {
    color: rgba(0, 0, 0, 0.4);
  }


  @media(min-width: 768px) {
    .posts-list .post-preview h2 {
      font-size: 26px;
    }
    .posts-list .post-preview .thumbnail {
      float: right;
      width: 30%;
      margin-bottom: 0;
    }
    .posts-list .post-preview .description {
      float: left;
      width: 45%;
    }
    .posts-list .post-preview .metadata {
      float: left;
      width: 20%;
      margin-top: 8px;
    }
    .posts-list .post-preview p {
      margin: 0 0 12px 0;
      line-height: 1.5em;
      font-size: 16px;
    }
    .posts-with-sidebar .posts-list {
      float: left;
      width: 75%;
    }
    .posts-with-sidebar .posts-sidebar {
      float: right;
      width: 20%;
      margin-top: 60px;
      padding-top: 24px;
      padding-bottom: 24px;
    }
  }


  /* Improve display for browsers without grid (IE/Edge <= 15) */

  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }

  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }

  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }

  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }

  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }

  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }

  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }


  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }

  .downlevel .footnotes ol {
    padding-left: 13px;
  }

  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }

  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }

  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }

  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;

    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;

    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }

  .downlevel .posts-list .post-preview {
    color: inherit;
  }



  </style>

  <script type="application/javascript">

  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }

  // show body when load is complete
  function on_load_complete() {

    // add anchors
    if (window.anchors) {
      window.anchors.options.placement = 'left';
      window.anchors.add('d-article > h2, d-article > h3, d-article > h4, d-article > h5');
    }


    // set body to visible
    document.body.style.visibility = 'visible';

    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }

    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }

  function init_distill() {

    init_common();

    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);

    // create d-title
    $('.d-title').changeElementType('d-title');

    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);

    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();

    // move posts container into article
    $('.posts-container').appendTo($('d-article'));

    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');

    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;

    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.html();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.html(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();

    // move refs into #references-listing
    $('#references-listing').replaceWith($('#refs'));

    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-contents a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });

    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');

    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {

      // capture layout
      var layout = $(this).attr('data-layout');

      // apply layout to markdown level block elements
      var elements = $(this).children().not('details, div.sourceCode, pre, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });


      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });

    // remove code block used to force  highlighting css
    $('.distill-force-highlighting-css').parent().remove();

    // remove empty line numbers inserted by pandoc when using a
    // custom syntax highlighting theme
    $('code.sourceCode a:empty').remove();

    // load distill framework
    load_distill_framework();

    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {

      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;

      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');

      // article with toc class
      $('.d-contents').parent().addClass('d-article-with-toc');

      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

      // add orcid ids
      $('.authors-affiliations').find('.author').each(function(i, el) {
        var orcid_id = front_matter.authors[i].orcidID;
        if (orcid_id) {
          var a = $('<a></a>');
          a.attr('href', 'https://orcid.org/' + orcid_id);
          var img = $('<img></img>');
          img.addClass('orcid-id');
          img.attr('alt', 'ORCID ID');
          img.attr('src','data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==');
          a.append(img);
          $(this).append(a);
        }
      });

      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }

      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");

      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }

       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }

      // remove d-appendix and d-footnote-list local styles
      $('d-appendix > style:first-child').remove();
      $('d-footnote-list > style:first-child').remove();

      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();

      // hoverable references
      $('span.citation[data-cites]').each(function() {
        if ($(this).children()[0].nodeName == "D-FOOTNOTE") {
          var fn = $(this).children()[0]
          $(this).html(fn.shadowRoot.querySelector("sup"))
          $(this).id = fn.id
          fn.remove()
        }
        var refs = $(this).attr('data-cites').split(" ");
        var refHtml = refs.map(function(ref) {
          return "<p>" + $('#ref-' + ref).html() + "</p>";
        }).join("\n");
        window.tippy(this, {
          allowHTML: true,
          content: refHtml,
          maxWidth: 500,
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start'
        });
      });

      // fix footnotes in tables (#411)
      // replacing broken distill.pub feature
      $('table d-footnote').each(function() {
        // we replace internal showAtNode methode which is triggered when hovering a footnote
        this.hoverBox.showAtNode = function(node) {
          // ported from https://github.com/distillpub/template/pull/105/files
          calcOffset = function(elem) {
              let x = elem.offsetLeft;
              let y = elem.offsetTop;
              // Traverse upwards until an `absolute` element is found or `elem`
              // becomes null.
              while (elem = elem.offsetParent && elem.style.position != 'absolute') {
                  x += elem.offsetLeft;
                  y += elem.offsetTop;
              }

              return { left: x, top: y };
          }
          // https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/offsetTop
          const bbox = node.getBoundingClientRect();
          const offset = calcOffset(node);
          this.show([offset.left + bbox.width, offset.top + bbox.height]);
        }
      })

      // clear polling timer
      clearInterval(tid);

      // show body now that everything is ready
      on_load_complete();
    }

    var tid = setInterval(distill_post_process, 50);
    distill_post_process();

  }

  function init_downlevel() {

    init_common();

     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));

    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;

    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();

    // remove toc
    $('.d-contents').remove();

    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });


    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);

    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();

    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });

    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));

    $('body').addClass('downlevel');

    on_load_complete();
  }


  function init_common() {

    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};

        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });

        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);

    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});

    // mark non-body figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      var figures = $(this).find('img, .html-widget');
      // ignore leaflet img layers (#106)
      figures = figures.filter(':not(img[class*="leaflet"])')
      if ($(this).attr('data-layout') !== "l-body") {
        figures.css('width', '100%');
      } else {
        figures.css('max-width', '100%');
        figures.filter("[width]").each(function(i, val) {
          var fig = $(this);
          fig.css('width', fig.attr('width') + 'px');
        });

      }
    });

    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });

    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.distill-site-header a[href]').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace(/^index[.]html/, "./"));
      });
    }

    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');

    // add figcaption style to table captions
    $('caption').parent('table').addClass("figcaption");

    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();

    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $(window).resize();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }

  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });

  </script>

  <!--/radix_placeholder_distill-->
  <script src="mnistclassification_files/header-attrs-2.14/header-attrs.js"></script>
  <script src="mnistclassification_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
  <script src="mnistclassification_files/popper-2.6.0/popper.min.js"></script>
  <link href="mnistclassification_files/tippy-6.2.7/tippy.css" rel="stylesheet" />
  <link href="mnistclassification_files/tippy-6.2.7/tippy-light-border.css" rel="stylesheet" />
  <script src="mnistclassification_files/tippy-6.2.7/tippy.umd.min.js"></script>
  <script src="mnistclassification_files/anchor-4.2.2/anchor.min.js"></script>
  <script src="mnistclassification_files/bowser-1.9.3/bowser.min.js"></script>
  <script src="mnistclassification_files/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="mnistclassification_files/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"pytorch로 mnist 분류하기","description":"pytorch로 mnist 분류 MLP모델을 구현하고 optimizer, lr scheduler, regularization, dropout, batch normalization의 적용 전 후 성능차이를 비교해봤습니다.","authors":[{"author":"nackta","authorURL":{},"affiliation":"&nbsp;","affiliationURL":"#","orcidID":""}],"publishedDate":"2022-05-27T00:00:00.000+09:00","citationText":"nackta, 2022"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>pytorch로 mnist 분류하기</h1>
<!--radix_placeholder_categories-->
<div class="dt-tags">
<div class="dt-tag">pytorch</div>
</div>
<!--/radix_placeholder_categories-->
<p><p>pytorch로 mnist 분류 MLP모델을 구현하고 optimizer, lr scheduler,
regularization, dropout, batch normalization의 적용 전 후 성능차이를
비교해봤습니다.</p></p>
</div>

<div class="d-byline">
  nackta true 
  
<br/>2022-05-27
</div>

<div class="d-article">
<div class="d-contents d-contents-float">
<nav class="l-text toc figcaption" id="TOC">
<h3>Contents</h3>
<ul>
<li><a href="#mnist-데이터-로드">Mnist 데이터 로드</a></li>
<li><a href="#모델-구현">모델 구현</a></li>
<li><a href="#train">Train</a></li>
<li><a href="#test">Test</a></li>
<li><a href="#split-train-vaild">split train vaild</a></li>
<li><a href="#trainer-class-구현">Trainer class 구현</a>
<ul>
<li><a href="#init-_-_">_ _ init _ _ ()</a></li>
<li><a href="#get_optimizer">_get_optimizer()</a></li>
<li><a href="#get_scheduler">_get_scheduler()</a></li>
<li><a href="#train-1">train()</a></li>
</ul></li>
<li><a href="#opimizer-비교-sgd-adam">opimizer 비교 (sgd, adam)</a>
<ul>
<li><a href="#sgd">SGD</a></li>
<li><a href="#adam">adam</a></li>
</ul></li>
<li><a href="#learning-rate-scheduler-사용-유무-비교">learning rate
scheduler 사용 유무 비교</a></li>
<li><a href="#dropout-batch-normalization">dropout, batch
normalization</a></li>
</ul>
</nav>
</div>
<p>socar 부트캠프의 강의 중 일부를 정리한 내용입니다.</p>
<p>torch를 이용하여 MLP 모델을 구현했습니다. 또한 여러 종류의 optimizer,
lr scheduler, regularization, dropout, batch normalization을 사용해보고
어떤 효과가 있는지 실험을 하며 비교해봤습니다.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch </span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn </span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader </span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> datasets, transforms </span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt </span></code></pre></div>
<p>gpu가 없지만 있다고 가정하고 구현했습니다.</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">&quot;cuda&quot;</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">&quot;cpu&quot;</span> </span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>device </span></code></pre></div>
<pre><code>&#39;cpu&#39; </code></pre>
<h2 id="mnist-데이터-로드">Mnist 데이터 로드</h2>
<p>datasets.MNIST()을 이용하여 mnist 데이터를 저장 후 로드합니다. root에
이미 저장이 되있다면 로드만 됩니다.</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 저장될 데이터 위치 지정 </span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>root <span class="op">=</span> os.path.join(os.getcwd(), <span class="st">&quot;data&quot;</span>) </span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>root </span></code></pre></div>
<pre><code>&#39;/home/nackta/python_project/socar_bootcamp/data&#39; </code></pre>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>mnist_train <span class="op">=</span> datasets.MNIST( </span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span>root,  </span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    train<span class="op">=</span><span class="va">True</span>,  </span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    download<span class="op">=</span><span class="va">True</span>,  </span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    transform<span class="op">=</span>transforms.ToTensor() </span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>) </span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>mnist_test <span class="op">=</span> datasets.MNIST( </span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span>root,  </span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    train<span class="op">=</span><span class="va">False</span>,  </span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    download<span class="op">=</span><span class="va">True</span>,  </span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    transform<span class="op">=</span>transforms.ToTensor() </span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>)  </span></code></pre></div>
<p>mnist train 데이터는 60000개, test 데이터는 10000개임을 알 수
있습니다.</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mnist_train) </span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mnist_test) </span></code></pre></div>
<pre><code>Dataset MNIST 
    Number of datapoints: 60000 
    Root location: /home/nackta/python_project/socar_bootcamp/data 
    Split: Train 
    StandardTransform 
Transform: ToTensor() 
Dataset MNIST 
    Number of datapoints: 10000 
    Root location: /home/nackta/python_project/socar_bootcamp/data 
    Split: Test 
    StandardTransform 
Transform: ToTensor() </code></pre>
<p>mnist data에는 feature data와 label data가 tuple로 한 쌍으로 묶여져
있습니다.</p>
<p>feature의 데이터 하나당 size는 (1x28x28)임을 알 수 있으며 매칭되는
label data로 정수가 있습니다.</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>sample_x, sample_y <span class="op">=</span> mnist_train[<span class="dv">0</span>] </span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sample_x.shape) </span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sample_y) </span></code></pre></div>
<pre><code>torch.Size([1, 28, 28]) 
5 </code></pre>
<p>라벨 데이터의 범위는 0~9까지의 정수입니다.</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>mnist_train.targets.unique() </span></code></pre></div>
<pre><code>tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) </code></pre>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_mnist(data, figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">10</span>)): </span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>figsize) </span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">18</span>): </span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>        img <span class="op">=</span> data[i][<span class="dv">0</span>] </span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>        ax <span class="op">=</span> fig.add_subplot(<span class="dv">3</span>, <span class="dv">6</span>, i<span class="op">+</span><span class="dv">1</span>) <span class="co"># 3x6, i+1번째 자리에 plot </span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>        ax.imshow(img.reshape(<span class="dv">28</span>, <span class="dv">28</span>), cmap<span class="op">=</span><span class="st">&quot;gray&quot;</span>) </span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        ax.set_title(<span class="ss">f&quot;Label: </span><span class="sc">{</span>data[i][<span class="dv">1</span>]<span class="sc">}</span><span class="ss">&quot;</span>) </span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    fig.show() </span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">pass</span> </span></code></pre></div>
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>plot_mnist(data<span class="op">=</span>mnist_train) </span></code></pre></div>
<figure>
<img src="output_15_0.png" alt="png" />
<figcaption aria-hidden="true">png</figcaption>
</figure>
<p>이처럼 mnist 데이터는 feature data로 손글씨 이미지가 있고 그에 맞게
label data가 있습니다.</p>
<p>그럼 DataLoader를 통해 batch size를 64로 지정하겠습니다.
drop_last=True이면 전체 데이터를 배치단위로 묶은 뒤 남은 데이터를 버리게
됩니다.</p>
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(dataset<span class="op">=</span>mnist_train, batch_size<span class="op">=</span><span class="dv">64</span>, shuffle<span class="op">=</span><span class="va">True</span>, drop_last<span class="op">=</span><span class="va">True</span>) </span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>test_loader <span class="op">=</span> DataLoader(dataset<span class="op">=</span>mnist_test, batch_size<span class="op">=</span><span class="dv">64</span>, shuffle<span class="op">=</span><span class="va">False</span>, drop_last<span class="op">=</span><span class="va">True</span>) </span></code></pre></div>
<p>각 배치의 shape은 64x1x28x28인 feature data와 64개의 label data로
이뤄져 있고 총 937의 배치가 만들어졌습니다.</p>
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(train_loader): </span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    x, y <span class="op">=</span> batch </span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>: </span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(x.shape) </span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(y) </span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(i<span class="op">+</span><span class="dv">1</span>) </span></code></pre></div>
<pre><code>torch.Size([64, 1, 28, 28]) 
tensor([0, 2, 0, 0, 1, 7, 3, 2, 4, 5, 8, 8, 4, 0, 9, 8, 3, 1, 3, 3, 5, 1, 4, 7, 
        0, 4, 8, 4, 9, 4, 3, 7, 9, 7, 6, 1, 3, 8, 7, 0, 5, 1, 1, 3, 3, 0, 6, 3, 
        2, 2, 2, 5, 4, 5, 5, 7, 3, 7, 8, 3, 0, 7, 9, 9]) 
937 </code></pre>
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(train_loader) </span></code></pre></div>
<pre><code>937 </code></pre>
<h2 id="모델-구현">모델 구현</h2>
<p>기본적인 mlp 모델을 구현해보겠습니다. 2차원 input데이터를 Flatten()을
통해 1차원 행렬로 변환 후 2개의 은닉층을 거쳐 10개의 값을 출력하는
모델을 만듭니다.</p>
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LionMNISTClassifier(nn.Module): </span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_class<span class="op">=</span><span class="dv">10</span>): </span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>() </span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> nn.Sequential( </span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>            nn.Flatten(), </span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>, <span class="dv">256</span>), </span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(), </span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">256</span>, <span class="dv">128</span>), </span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(), </span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">128</span>, n_class), </span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>        ) </span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x): </span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.model(x) </span></code></pre></div>
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>mlp <span class="op">=</span> LionMNISTClassifier().to(device) <span class="co"># 모델을 gpu에서 계산할 수 있도록 지정 </span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>mlp </span></code></pre></div>
<pre><code>LionMNISTClassifier( 
  (model): Sequential( 
    (0): Flatten(start_dim=1, end_dim=-1) 
    (1): Linear(in_features=784, out_features=256, bias=True) 
    (2): ReLU() 
    (3): Linear(in_features=256, out_features=128, bias=True) 
    (4): ReLU() 
    (5): Linear(in_features=128, out_features=10, bias=True) 
  ) 
) </code></pre>
<p>loss 함수는 CrossEntropy, optimizer는 SGD, learning rate는 0.001을
사용합니다</p>
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss() </span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>optim <span class="op">=</span> torch.optim.SGD(mlp.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>) </span></code></pre></div>
<h2 id="train">Train</h2>
<p>train 과정에서 가장 핵심인 부분은 loss.backward(), optim.step(),
optim.zero_grad()로 이어지는 3단계입니다.</p>
<ul>
<li>loss.backward()</li>
</ul>
<p>계산된 loss 텐서에 .backward()를 실행하면 역전파가 실행됩니다. loss
안에 pred의 각 parameters에 대한 변화도를 계산하고 저장합니다.</p>
<ul>
<li>optim.step()</li>
</ul>
<p>저장된 변화도에 따라 각 parameters를 조정합니다.</p>
<ul>
<li>optim.zero_grad()</li>
</ul>
<p>각 parameters에 저장된 변화도를 0으로 초기화합니다. 초기화하지 않으면
변화도가 누적되어 계산됩니다.</p>
<div class="sourceCode" id="cb24"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">20</span> </span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>train_history <span class="op">=</span> {<span class="st">&quot;loss&quot;</span>: [], <span class="st">&quot;acc&quot;</span>: []} </span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> e <span class="kw">in</span> <span class="bu">range</span>(num_epochs): </span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Epoch: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss"> start.&quot;</span>) </span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>    epoch_loss, epoch_acc <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span> </span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>    size <span class="op">=</span> <span class="bu">len</span>(train_loader.dataset) </span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> b, (x, y) <span class="kw">in</span> <span class="bu">enumerate</span>(train_loader): </span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>        x.to(device) </span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>        y.to(device) </span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> mlp(x) </span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(pred, y) </span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>        optim.zero_grad() <span class="co"># parameters의 변화량을 0으로 초기화 </span></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>        loss.backward() <span class="co"># 에러의 변화도를 계산하여 저장 </span></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>        optim.step() <span class="co"># loss를 최소화 할 수 있도록 parameters 수정 </span></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss.to(<span class="st">&quot;cpu&quot;</span>).item() </span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>        acc <span class="op">=</span> (pred.argmax(<span class="dv">1</span>) <span class="op">==</span> y).<span class="bu">type</span>(torch.<span class="bu">float</span>).to(<span class="st">&quot;cpu&quot;</span>).mean().item() </span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> b <span class="op">%</span> <span class="dv">200</span> <span class="op">==</span> <span class="dv">0</span>: </span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f&quot;loss: </span><span class="sc">{</span>loss<span class="sc">:&gt;7f}</span><span class="ss">, acc: </span><span class="sc">{</span>acc<span class="sc">:&gt;4f}</span><span class="ss"> [</span><span class="sc">{</span>b <span class="op">*</span> <span class="bu">len</span>(x)<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>size<span class="sc">}</span><span class="ss">]&quot;</span>) </span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>        epoch_loss <span class="op">+=</span> loss </span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>        epoch_acc <span class="op">+=</span> acc </span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a>    train_history[<span class="st">&quot;loss&quot;</span>].append(epoch_loss <span class="op">/</span> <span class="bu">len</span>(train_loader)) </span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>    train_history[<span class="st">&quot;acc&quot;</span>].append(epoch_acc <span class="op">/</span> <span class="bu">len</span>(train_loader)) </span></code></pre></div>
<pre><code>Epoch: 0 start. 
loss: 2.306067, acc: 0.093750 [0/60000] 
loss: 2.307073, acc: 0.046875 [12800/60000] 
loss: 2.293016, acc: 0.109375 [25600/60000] 
loss: 2.285884, acc: 0.140625 [38400/60000] 
loss: 2.294895, acc: 0.156250 [51200/60000] 
Epoch: 1 start. 
loss: 2.277667, acc: 0.218750 [0/60000] 
loss: 2.266313, acc: 0.250000 [12800/60000] 
loss: 2.267338, acc: 0.250000 [25600/60000] 
loss: 2.254096, acc: 0.296875 [38400/60000] 
loss: 2.242105, acc: 0.453125 [51200/60000] 
Epoch: 2 start. 
loss: 2.255541, acc: 0.390625 [0/60000] 
loss: 2.236494, acc: 0.484375 [12800/60000] 
loss: 2.213965, acc: 0.500000 [25600/60000] 
loss: 2.202532, acc: 0.593750 [38400/60000] 
loss: 2.200587, acc: 0.437500 [51200/60000] 
Epoch: 3 start. 
loss: 2.181767, acc: 0.578125 [0/60000] 
loss: 2.161530, acc: 0.640625 [12800/60000] 
loss: 2.147703, acc: 0.625000 [25600/60000] 
loss: 2.144805, acc: 0.593750 [38400/60000] 
loss: 2.088173, acc: 0.656250 [51200/60000] 
Epoch: 4 start. 
loss: 2.067614, acc: 0.671875 [0/60000] 
loss: 2.047233, acc: 0.671875 [12800/60000] 
loss: 2.004169, acc: 0.718750 [25600/60000] 
loss: 1.925291, acc: 0.750000 [38400/60000] 
loss: 1.996897, acc: 0.593750 [51200/60000] 
Epoch: 5 start. 
loss: 1.886086, acc: 0.703125 [0/60000] 
loss: 1.824186, acc: 0.718750 [12800/60000] 
loss: 1.785833, acc: 0.750000 [25600/60000] 
loss: 1.820181, acc: 0.593750 [38400/60000] 
loss: 1.707675, acc: 0.734375 [51200/60000] 
Epoch: 6 start. 
loss: 1.637798, acc: 0.703125 [0/60000] 
loss: 1.547363, acc: 0.656250 [12800/60000] 
loss: 1.555369, acc: 0.734375 [25600/60000] 
loss: 1.479918, acc: 0.781250 [38400/60000] 
loss: 1.312469, acc: 0.796875 [51200/60000] 
Epoch: 7 start. 
loss: 1.329046, acc: 0.718750 [0/60000] 
loss: 1.342677, acc: 0.765625 [12800/60000] 
loss: 1.278799, acc: 0.765625 [25600/60000] 
loss: 1.217570, acc: 0.734375 [38400/60000] 
loss: 1.073273, acc: 0.781250 [51200/60000] 
Epoch: 8 start. 
loss: 1.050970, acc: 0.765625 [0/60000] 
loss: 0.996948, acc: 0.718750 [12800/60000] 
loss: 0.857504, acc: 0.843750 [25600/60000] 
loss: 0.988426, acc: 0.750000 [38400/60000] 
loss: 0.861934, acc: 0.812500 [51200/60000] 
Epoch: 9 start. 
loss: 0.766343, acc: 0.875000 [0/60000] 
loss: 0.836710, acc: 0.812500 [12800/60000] 
loss: 0.742993, acc: 0.828125 [25600/60000] 
loss: 0.921689, acc: 0.765625 [38400/60000] 
loss: 0.722942, acc: 0.843750 [51200/60000] 
Epoch: 10 start. 
loss: 0.701428, acc: 0.843750 [0/60000] 
loss: 0.671582, acc: 0.875000 [12800/60000] 
loss: 0.853458, acc: 0.765625 [25600/60000] 
loss: 0.622920, acc: 0.875000 [38400/60000] 
loss: 0.490978, acc: 0.937500 [51200/60000] 
Epoch: 11 start. 
loss: 0.700725, acc: 0.750000 [0/60000] 
loss: 0.636329, acc: 0.875000 [12800/60000] 
loss: 0.546823, acc: 0.875000 [25600/60000] 
loss: 0.531841, acc: 0.875000 [38400/60000] 
loss: 0.744622, acc: 0.828125 [51200/60000] 
Epoch: 12 start. 
loss: 0.895801, acc: 0.718750 [0/60000] 
loss: 0.559750, acc: 0.828125 [12800/60000] 
loss: 0.458211, acc: 0.921875 [25600/60000] 
loss: 0.624367, acc: 0.859375 [38400/60000] 
loss: 0.529751, acc: 0.890625 [51200/60000] 
Epoch: 13 start. 
loss: 0.457774, acc: 0.828125 [0/60000] 
loss: 0.428744, acc: 0.890625 [12800/60000] 
loss: 0.594997, acc: 0.843750 [25600/60000] 
loss: 0.552424, acc: 0.859375 [38400/60000] 
loss: 0.562785, acc: 0.890625 [51200/60000] 
Epoch: 14 start. 
loss: 0.538646, acc: 0.843750 [0/60000] 
loss: 0.445669, acc: 0.875000 [12800/60000] 
loss: 0.538240, acc: 0.890625 [25600/60000] 
loss: 0.518024, acc: 0.890625 [38400/60000] 
loss: 0.471751, acc: 0.875000 [51200/60000] 
Epoch: 15 start. 
loss: 0.483013, acc: 0.828125 [0/60000] 
loss: 0.470244, acc: 0.890625 [12800/60000] 
loss: 0.593319, acc: 0.828125 [25600/60000] 
loss: 0.383731, acc: 0.937500 [38400/60000] 
loss: 0.705848, acc: 0.812500 [51200/60000] 
Epoch: 16 start. 
loss: 0.380595, acc: 0.906250 [0/60000] 
loss: 0.301426, acc: 0.921875 [12800/60000] 
loss: 0.450054, acc: 0.921875 [25600/60000] 
loss: 0.438868, acc: 0.875000 [38400/60000] 
loss: 0.573046, acc: 0.843750 [51200/60000] 
Epoch: 17 start. 
loss: 0.500203, acc: 0.875000 [0/60000] 
loss: 0.595378, acc: 0.812500 [12800/60000] 
loss: 0.625601, acc: 0.843750 [25600/60000] 
loss: 0.523467, acc: 0.828125 [38400/60000] 
loss: 0.544994, acc: 0.859375 [51200/60000] 
Epoch: 18 start. 
loss: 0.590928, acc: 0.875000 [0/60000] 
loss: 0.597640, acc: 0.890625 [12800/60000] 
loss: 0.659121, acc: 0.781250 [25600/60000] 
loss: 0.361116, acc: 0.906250 [38400/60000] 
loss: 0.741920, acc: 0.843750 [51200/60000] 
Epoch: 19 start. 
loss: 0.407344, acc: 0.875000 [0/60000] 
loss: 0.564885, acc: 0.843750 [12800/60000] 
loss: 0.557660, acc: 0.796875 [25600/60000] 
loss: 0.335187, acc: 0.921875 [38400/60000] 
loss: 0.486878, acc: 0.812500 [51200/60000] </code></pre>
<p>훈련이 진행될수록 loss는 감소하고 정확도는 증가했습니다.</p>
<div class="sourceCode" id="cb26"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>)) </span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> fig.add_subplot(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>) </span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>ax.plot(train_history[<span class="st">&quot;acc&quot;</span>], color<span class="op">=</span><span class="st">&quot;red&quot;</span>, label<span class="op">=</span><span class="st">&quot;acc&quot;</span>) </span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>ax.plot(train_history[<span class="st">&quot;loss&quot;</span>], color<span class="op">=</span><span class="st">&quot;blue&quot;</span>, label<span class="op">=</span><span class="st">&quot;loss&quot;</span>) </span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>fig.show() </span></code></pre></div>
<figure>
<img src="output_31_0.png" alt="png" />
<figcaption aria-hidden="true">png</figcaption>
</figure>
<h2 id="test">Test</h2>
<p>train을 통해 최적화된 model로 test data를 예측합니다. 여기서 with
torch.no_grad()문 안에서 실행되는 pytorch는 autograd engine 기능을
중단합니다. 이를 통해 모델의 parameters가 변화하는 것을 방지 할 수 있고
메모리 사용량을 줄이고 연산속도을 높일 수 있습니다.</p>
<div class="sourceCode" id="cb27"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Test start.&quot;</span>) </span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>test_loss, test_acc <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span> </span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>size <span class="op">=</span> <span class="bu">len</span>(test_loader.dataset) </span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad(): </span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> b, (x, y) <span class="kw">in</span> <span class="bu">enumerate</span>(test_loader): </span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>        x.to(device) </span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>        y.to(device) </span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> mlp(x) </span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>        test_loss <span class="op">+=</span> loss_fn(pred, y).to(<span class="st">&quot;cpu&quot;</span>).item() </span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>        test_acc <span class="op">+=</span> (pred.argmax(<span class="dv">1</span>) <span class="op">==</span> y).<span class="bu">type</span>(torch.<span class="bu">float</span>).to(<span class="st">&quot;cpu&quot;</span>).mean().item() </span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">/=</span> <span class="bu">len</span>(test_loader) </span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>    test_acc <span class="op">/=</span> <span class="bu">len</span>(test_loader) </span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;test loss: </span><span class="sc">{</span>test_loss<span class="sc">:&gt;7f}</span><span class="ss">, test_acc: </span><span class="sc">{</span>test_acc<span class="sc">:&gt;4f}</span><span class="ss">.&quot;</span>) </span></code></pre></div>
<pre><code>Test start. 
test loss: 0.409632, test_acc: 0.886719. </code></pre>
<p>지금까지는 각 노드의 parameters를 최적화하여 모델의 성능을
높였습니다. 하지만 모델의 성능을 높이기 위해 optimizer, lr scheduler,
regularization, dropout과 batch normalization 등을 적용할 수
있습니다.</p>
<h2 id="split-train-vaild">split train vaild</h2>
<div class="sourceCode" id="cb29"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> mnist_dataset(Dataset): </span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, data, targets, transform<span class="op">=</span><span class="va">None</span>, target_transform<span class="op">=</span><span class="va">None</span>): </span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>() </span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.data <span class="op">=</span> data </span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.targets <span class="op">=</span> targets </span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transform <span class="op">=</span> transform </span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.target_transform <span class="op">=</span> target_transform </span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span> </span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>): </span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.data) </span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx): </span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>        x, y <span class="op">=</span> <span class="va">self</span>.data[idx], <span class="va">self</span>.targets[idx] </span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.transform: </span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> <span class="va">self</span>.transform(x) </span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.target_transform: </span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> <span class="va">self</span>.target_transform(y) </span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x, y </span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> split_train_valid(dataset, valid_ratio<span class="op">=</span><span class="fl">0.1</span>): </span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a>    n_valid <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(dataset) <span class="op">*</span> valid_ratio) </span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a>     </span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a>    train_data <span class="op">=</span> dataset.data[:<span class="op">-</span>n_valid].numpy() </span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a>    valid_data <span class="op">=</span> dataset.data[<span class="op">-</span>n_valid:].numpy() </span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a>    train_targets <span class="op">=</span> dataset.targets[:<span class="op">-</span>n_valid] </span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a>    valid_targets <span class="op">=</span> dataset.targets[<span class="op">-</span>n_valid:] </span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb29-32"><a href="#cb29-32" aria-hidden="true" tabindex="-1"></a>    train <span class="op">=</span> mnist_dataset(data<span class="op">=</span>train_data, targets<span class="op">=</span>train_targets, transform<span class="op">=</span>dataset.transform, target_transform<span class="op">=</span>dataset.target_transform) </span>
<span id="cb29-33"><a href="#cb29-33" aria-hidden="true" tabindex="-1"></a>    valid <span class="op">=</span> mnist_dataset(data<span class="op">=</span>valid_data, targets<span class="op">=</span>valid_targets, transform<span class="op">=</span>dataset.transform, target_transform<span class="op">=</span>dataset.target_transform) </span>
<span id="cb29-34"><a href="#cb29-34" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb29-35"><a href="#cb29-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train, valid </span></code></pre></div>
<div class="sourceCode" id="cb30"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>mnist_train, mnist_valid <span class="op">=</span> split_train_valid(dataset<span class="op">=</span>mnist_train) </span></code></pre></div>
<div class="sourceCode" id="cb31"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(dataset<span class="op">=</span>mnist_train, batch_size<span class="op">=</span> <span class="dv">64</span>, shuffle <span class="op">=</span> <span class="va">True</span>, drop_last<span class="op">=</span><span class="va">True</span>) </span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>valid_loader <span class="op">=</span> DataLoader(dataset<span class="op">=</span>mnist_valid, batch_size<span class="op">=</span> <span class="dv">64</span>, shuffle <span class="op">=</span> <span class="va">False</span>, drop_last<span class="op">=</span><span class="va">True</span>) </span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>test_loader <span class="op">=</span> DataLoader(dataset<span class="op">=</span>mnist_test, batch_size<span class="op">=</span> <span class="dv">64</span>, shuffle <span class="op">=</span> <span class="va">False</span>, drop_last<span class="op">=</span><span class="va">True</span>) </span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>loaders <span class="op">=</span> { </span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;train&#39;</span> : train_loader, </span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;valid&#39;</span> : valid_loader, </span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;test&#39;</span> : test_loader </span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>} </span></code></pre></div>
<h2 id="trainer-class-구현">Trainer class 구현</h2>
<h3 id="init-_-_">_ _ init _ _ ()</h3>
<p>model, train/valid/test로 나눠진 dataloder, optimizer 함수, learning
rate, learning rate scheduler 사용 여부를 받아서 정의합니다.</p>
<h3 id="get_optimizer">_get_optimizer()</h3>
<p>optimizer 함수를 정의합니다.</p>
<h3 id="get_scheduler">_get_scheduler()</h3>
<p>learning rate scheduler를 정의합니다. StepLR()는 일정한 Step 마다
learning rate에 gamma를 곱해주는 방식입니다.</p>
<h3 id="train-1">train()</h3>
<p>epoch 마다 훈련과 검증을 실시하고 loss와 accuracy를 저장합니다.</p>
<div class="sourceCode" id="cb32"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Trainer(nn.Module): </span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model_class, loaders, n_class<span class="op">=</span><span class="dv">10</span>, opt<span class="op">=</span><span class="st">&quot;sgd&quot;</span>, lr<span class="op">=</span><span class="fl">0.001</span>, has_scheduler<span class="op">=</span><span class="va">False</span>, device<span class="op">=</span><span class="st">&quot;cpu&quot;</span>): </span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>() </span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> model_class(n_class<span class="op">=</span>n_class) </span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.loss <span class="op">=</span> nn.CrossEntropyLoss() </span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.train_loader <span class="op">=</span> loaders[<span class="st">&quot;train&quot;</span>] </span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.valid_loader <span class="op">=</span> loaders[<span class="st">&quot;valid&quot;</span>] </span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.test_loader <span class="op">=</span> loaders[<span class="st">&quot;test&quot;</span>] </span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._get_optimizer(opt<span class="op">=</span>opt.lower(), lr<span class="op">=</span>lr) </span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.has_scheduler <span class="op">=</span> has_scheduler </span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.has_scheduler: </span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._get_scheduler() </span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.device <span class="op">=</span> device </span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span> </span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _get_optimizer(<span class="va">self</span>, opt, lr<span class="op">=</span><span class="fl">0.001</span>): </span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> opt <span class="op">==</span> <span class="st">&quot;sgd&quot;</span>: </span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.optimizer <span class="op">=</span> torch.optim.SGD( </span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>                params<span class="op">=</span><span class="va">self</span>.model.parameters(), lr<span class="op">=</span>lr) </span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> opt <span class="op">==</span> <span class="st">&quot;adam&quot;</span>: </span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.optimizer <span class="op">=</span> torch.optim.Adam( </span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>                params<span class="op">=</span><span class="va">self</span>.model.parameters(), lr<span class="op">=</span>lr) </span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>: </span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f&quot;optimizer </span><span class="sc">{</span>opt<span class="sc">}</span><span class="ss"> is not supproted&quot;</span>) </span>
<span id="cb32-29"><a href="#cb32-29" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-30"><a href="#cb32-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _get_scheduler(<span class="va">self</span>): </span>
<span id="cb32-31"><a href="#cb32-31" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scheduler <span class="op">=</span> torch.optim.lr_scheduler.StepLR( </span>
<span id="cb32-32"><a href="#cb32-32" aria-hidden="true" tabindex="-1"></a>            optimizer<span class="op">=</span><span class="va">self</span>.optimizer, step_size<span class="op">=</span><span class="dv">5</span>, gamma<span class="op">=</span><span class="fl">0.5</span>, verbose<span class="op">=</span><span class="va">True</span>) </span>
<span id="cb32-33"><a href="#cb32-33" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-34"><a href="#cb32-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> train(<span class="va">self</span>, max_epochs<span class="op">=</span><span class="dv">10</span>): </span>
<span id="cb32-35"><a href="#cb32-35" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">&quot;===== Train Start =====&quot;</span>) </span>
<span id="cb32-36"><a href="#cb32-36" aria-hidden="true" tabindex="-1"></a>        history <span class="op">=</span> {<span class="st">&quot;train_loss&quot;</span>: [], <span class="st">&quot;train_acc&quot;</span>: [], </span>
<span id="cb32-37"><a href="#cb32-37" aria-hidden="true" tabindex="-1"></a>                   <span class="st">&quot;valid_loss&quot;</span>: [], <span class="st">&quot;valid_acc&quot;</span>: []} </span>
<span id="cb32-38"><a href="#cb32-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> e <span class="kw">in</span> <span class="bu">range</span>(max_epochs): </span>
<span id="cb32-39"><a href="#cb32-39" aria-hidden="true" tabindex="-1"></a>            train_loss, train_acc <span class="op">=</span> <span class="va">self</span>._train_epoch() </span>
<span id="cb32-40"><a href="#cb32-40" aria-hidden="true" tabindex="-1"></a>            valid_loss, valid_acc <span class="op">=</span> <span class="va">self</span>._valid_epoch() </span>
<span id="cb32-41"><a href="#cb32-41" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-42"><a href="#cb32-42" aria-hidden="true" tabindex="-1"></a>            history[<span class="st">&quot;train_loss&quot;</span>].append(train_loss) </span>
<span id="cb32-43"><a href="#cb32-43" aria-hidden="true" tabindex="-1"></a>            history[<span class="st">&quot;train_acc&quot;</span>].append(train_acc) </span>
<span id="cb32-44"><a href="#cb32-44" aria-hidden="true" tabindex="-1"></a>            history[<span class="st">&quot;valid_loss&quot;</span>].append(valid_loss) </span>
<span id="cb32-45"><a href="#cb32-45" aria-hidden="true" tabindex="-1"></a>            history[<span class="st">&quot;valid_acc&quot;</span>].append(valid_acc) </span>
<span id="cb32-46"><a href="#cb32-46" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-47"><a href="#cb32-47" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.has_scheduler: </span>
<span id="cb32-48"><a href="#cb32-48" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.scheduler.step() </span>
<span id="cb32-49"><a href="#cb32-49" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-50"><a href="#cb32-50" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> e <span class="op">%</span> <span class="dv">5</span> <span class="op">==</span> <span class="dv">0</span>: </span>
<span id="cb32-51"><a href="#cb32-51" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>( </span>
<span id="cb32-52"><a href="#cb32-52" aria-hidden="true" tabindex="-1"></a>                    <span class="ss">f&quot;Epoch: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">, train loss: </span><span class="sc">{</span>train_loss<span class="sc">:&gt;6f}</span><span class="ss">, train acc: </span><span class="sc">{</span>train_acc<span class="sc">:&gt;3f}</span><span class="ss">, valid loss: </span><span class="sc">{</span>valid_loss<span class="sc">:&gt;6f}</span><span class="ss">, valid acc: </span><span class="sc">{</span>valid_acc<span class="sc">:&gt;3f}</span><span class="ss">&quot;</span>) </span>
<span id="cb32-53"><a href="#cb32-53" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-54"><a href="#cb32-54" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.plot_history(history) </span>
<span id="cb32-55"><a href="#cb32-55" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-56"><a href="#cb32-56" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _train_epoch(<span class="va">self</span>): </span>
<span id="cb32-57"><a href="#cb32-57" aria-hidden="true" tabindex="-1"></a>        epoch_loss, epoch_acc <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span> </span>
<span id="cb32-58"><a href="#cb32-58" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model.train() </span>
<span id="cb32-59"><a href="#cb32-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (x, y) <span class="kw">in</span> <span class="va">self</span>.train_loader: </span>
<span id="cb32-60"><a href="#cb32-60" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> x.to(<span class="va">self</span>.device) </span>
<span id="cb32-61"><a href="#cb32-61" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> y.to(<span class="va">self</span>.device) </span>
<span id="cb32-62"><a href="#cb32-62" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-63"><a href="#cb32-63" aria-hidden="true" tabindex="-1"></a>            y_hat <span class="op">=</span> <span class="va">self</span>.model(x) </span>
<span id="cb32-64"><a href="#cb32-64" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> <span class="va">self</span>.loss(y_hat, y) </span>
<span id="cb32-65"><a href="#cb32-65" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-66"><a href="#cb32-66" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.optimizer.zero_grad() </span>
<span id="cb32-67"><a href="#cb32-67" aria-hidden="true" tabindex="-1"></a>            loss.backward() </span>
<span id="cb32-68"><a href="#cb32-68" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.optimizer.step() </span>
<span id="cb32-69"><a href="#cb32-69" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-70"><a href="#cb32-70" aria-hidden="true" tabindex="-1"></a>            epoch_loss <span class="op">+=</span> loss.to(<span class="st">&quot;cpu&quot;</span>).item() </span>
<span id="cb32-71"><a href="#cb32-71" aria-hidden="true" tabindex="-1"></a>            epoch_acc <span class="op">+=</span> (y_hat.argmax(<span class="dv">1</span>) <span class="op">==</span> </span>
<span id="cb32-72"><a href="#cb32-72" aria-hidden="true" tabindex="-1"></a>                          y).<span class="bu">type</span>(torch.<span class="bu">float</span>).to(<span class="st">&quot;cpu&quot;</span>).mean().item() </span>
<span id="cb32-73"><a href="#cb32-73" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-74"><a href="#cb32-74" aria-hidden="true" tabindex="-1"></a>        epoch_loss <span class="op">/=</span> <span class="bu">len</span>(<span class="va">self</span>.train_loader) </span>
<span id="cb32-75"><a href="#cb32-75" aria-hidden="true" tabindex="-1"></a>        epoch_acc <span class="op">/=</span> <span class="bu">len</span>(<span class="va">self</span>.train_loader) </span>
<span id="cb32-76"><a href="#cb32-76" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-77"><a href="#cb32-77" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> epoch_loss, epoch_acc </span>
<span id="cb32-78"><a href="#cb32-78" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-79"><a href="#cb32-79" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _valid_epoch(<span class="va">self</span>): </span>
<span id="cb32-80"><a href="#cb32-80" aria-hidden="true" tabindex="-1"></a>        epoch_loss, epoch_acc <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span> </span>
<span id="cb32-81"><a href="#cb32-81" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model.<span class="bu">eval</span>() </span>
<span id="cb32-82"><a href="#cb32-82" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad(): </span>
<span id="cb32-83"><a href="#cb32-83" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> (x, y) <span class="kw">in</span> <span class="va">self</span>.valid_loader: </span>
<span id="cb32-84"><a href="#cb32-84" aria-hidden="true" tabindex="-1"></a>                x <span class="op">=</span> x.to(<span class="va">self</span>.device) </span>
<span id="cb32-85"><a href="#cb32-85" aria-hidden="true" tabindex="-1"></a>                y <span class="op">=</span> y.to(<span class="va">self</span>.device) </span>
<span id="cb32-86"><a href="#cb32-86" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-87"><a href="#cb32-87" aria-hidden="true" tabindex="-1"></a>                y_hat <span class="op">=</span> <span class="va">self</span>.model(x) </span>
<span id="cb32-88"><a href="#cb32-88" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">=</span> <span class="va">self</span>.loss(y_hat, y) </span>
<span id="cb32-89"><a href="#cb32-89" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-90"><a href="#cb32-90" aria-hidden="true" tabindex="-1"></a>                epoch_loss <span class="op">+=</span> loss.to(<span class="st">&quot;cpu&quot;</span>).item() </span>
<span id="cb32-91"><a href="#cb32-91" aria-hidden="true" tabindex="-1"></a>                epoch_acc <span class="op">+=</span> (y_hat.argmax(<span class="dv">1</span>) <span class="op">==</span> </span>
<span id="cb32-92"><a href="#cb32-92" aria-hidden="true" tabindex="-1"></a>                              y).<span class="bu">type</span>(torch.<span class="bu">float</span>).to(<span class="st">&quot;cpu&quot;</span>).mean().item() </span>
<span id="cb32-93"><a href="#cb32-93" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-94"><a href="#cb32-94" aria-hidden="true" tabindex="-1"></a>        epoch_loss <span class="op">/=</span> <span class="bu">len</span>(<span class="va">self</span>.valid_loader) </span>
<span id="cb32-95"><a href="#cb32-95" aria-hidden="true" tabindex="-1"></a>        epoch_acc <span class="op">/=</span> <span class="bu">len</span>(<span class="va">self</span>.valid_loader) </span>
<span id="cb32-96"><a href="#cb32-96" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-97"><a href="#cb32-97" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> epoch_loss, epoch_acc </span>
<span id="cb32-98"><a href="#cb32-98" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-99"><a href="#cb32-99" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> plot_history(<span class="va">self</span>, history): </span>
<span id="cb32-100"><a href="#cb32-100" aria-hidden="true" tabindex="-1"></a>        fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">10</span>)) </span>
<span id="cb32-101"><a href="#cb32-101" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-102"><a href="#cb32-102" aria-hidden="true" tabindex="-1"></a>        ax <span class="op">=</span> fig.add_subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>) </span>
<span id="cb32-103"><a href="#cb32-103" aria-hidden="true" tabindex="-1"></a>        ax.plot(history[<span class="st">&quot;train_loss&quot;</span>], color<span class="op">=</span><span class="st">&quot;red&quot;</span>, label<span class="op">=</span><span class="st">&quot;train loss&quot;</span>) </span>
<span id="cb32-104"><a href="#cb32-104" aria-hidden="true" tabindex="-1"></a>        ax.plot(history[<span class="st">&quot;valid_loss&quot;</span>], color<span class="op">=</span><span class="st">&quot;blue&quot;</span>, label<span class="op">=</span><span class="st">&quot;valid loss&quot;</span>) </span>
<span id="cb32-105"><a href="#cb32-105" aria-hidden="true" tabindex="-1"></a>        ax.set_title(<span class="st">&quot;Loss&quot;</span>) </span>
<span id="cb32-106"><a href="#cb32-106" aria-hidden="true" tabindex="-1"></a>        ax.legend() </span>
<span id="cb32-107"><a href="#cb32-107" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-108"><a href="#cb32-108" aria-hidden="true" tabindex="-1"></a>        ax <span class="op">=</span> fig.add_subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>) </span>
<span id="cb32-109"><a href="#cb32-109" aria-hidden="true" tabindex="-1"></a>        ax.plot(history[<span class="st">&quot;train_acc&quot;</span>], color<span class="op">=</span><span class="st">&quot;red&quot;</span>, label<span class="op">=</span><span class="st">&quot;train acc&quot;</span>) </span>
<span id="cb32-110"><a href="#cb32-110" aria-hidden="true" tabindex="-1"></a>        ax.plot(history[<span class="st">&quot;valid_acc&quot;</span>], color<span class="op">=</span><span class="st">&quot;blue&quot;</span>, label<span class="op">=</span><span class="st">&quot;valid acc&quot;</span>) </span>
<span id="cb32-111"><a href="#cb32-111" aria-hidden="true" tabindex="-1"></a>        ax.set_title(<span class="st">&quot;Acc&quot;</span>) </span>
<span id="cb32-112"><a href="#cb32-112" aria-hidden="true" tabindex="-1"></a>        ax.legend() </span>
<span id="cb32-113"><a href="#cb32-113" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-114"><a href="#cb32-114" aria-hidden="true" tabindex="-1"></a>        fig.show() </span>
<span id="cb32-115"><a href="#cb32-115" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-116"><a href="#cb32-116" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test(<span class="va">self</span>): </span>
<span id="cb32-117"><a href="#cb32-117" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">&quot;===== Test Start =====&quot;</span>) </span>
<span id="cb32-118"><a href="#cb32-118" aria-hidden="true" tabindex="-1"></a>        epoch_loss, epoch_acc <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span> </span>
<span id="cb32-119"><a href="#cb32-119" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model.<span class="bu">eval</span>() </span>
<span id="cb32-120"><a href="#cb32-120" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad(): </span>
<span id="cb32-121"><a href="#cb32-121" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> (x, y) <span class="kw">in</span> <span class="va">self</span>.test_loader: </span>
<span id="cb32-122"><a href="#cb32-122" aria-hidden="true" tabindex="-1"></a>                x <span class="op">=</span> x.to(<span class="va">self</span>.device) </span>
<span id="cb32-123"><a href="#cb32-123" aria-hidden="true" tabindex="-1"></a>                y <span class="op">=</span> y.to(<span class="va">self</span>.device) </span>
<span id="cb32-124"><a href="#cb32-124" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-125"><a href="#cb32-125" aria-hidden="true" tabindex="-1"></a>                y_hat <span class="op">=</span> <span class="va">self</span>.model(x) </span>
<span id="cb32-126"><a href="#cb32-126" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">=</span> <span class="va">self</span>.loss(y_hat, y) </span>
<span id="cb32-127"><a href="#cb32-127" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-128"><a href="#cb32-128" aria-hidden="true" tabindex="-1"></a>                epoch_loss <span class="op">+=</span> loss.to(<span class="st">&quot;cpu&quot;</span>).item() </span>
<span id="cb32-129"><a href="#cb32-129" aria-hidden="true" tabindex="-1"></a>                epoch_acc <span class="op">+=</span> (y_hat.argmax(<span class="dv">1</span>) <span class="op">==</span> </span>
<span id="cb32-130"><a href="#cb32-130" aria-hidden="true" tabindex="-1"></a>                              y).<span class="bu">type</span>(torch.<span class="bu">float</span>).to(<span class="st">&quot;cpu&quot;</span>).mean().item() </span>
<span id="cb32-131"><a href="#cb32-131" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-132"><a href="#cb32-132" aria-hidden="true" tabindex="-1"></a>        epoch_loss <span class="op">/=</span> <span class="bu">len</span>(<span class="va">self</span>.test_loader) </span>
<span id="cb32-133"><a href="#cb32-133" aria-hidden="true" tabindex="-1"></a>        epoch_acc <span class="op">/=</span> <span class="bu">len</span>(<span class="va">self</span>.test_loader) </span>
<span id="cb32-134"><a href="#cb32-134" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-135"><a href="#cb32-135" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;Test loss: </span><span class="sc">{</span>epoch_loss<span class="sc">:&gt;6f}</span><span class="ss">, Test acc: </span><span class="sc">{</span>epoch_acc<span class="sc">:&gt;3f}</span><span class="ss">&quot;</span>) </span>
<span id="cb32-136"><a href="#cb32-136" aria-hidden="true" tabindex="-1"></a> </span></code></pre></div>
<h2 id="opimizer-비교-sgd-adam">opimizer 비교 (sgd, adam)</h2>
<h3 id="sgd">SGD</h3>
<div class="sourceCode" id="cb33"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(model_class<span class="op">=</span>LionMNISTClassifier, loaders<span class="op">=</span>loaders, n_class<span class="op">=</span><span class="dv">10</span>, opt<span class="op">=</span><span class="st">&quot;sgd&quot;</span>, lr<span class="op">=</span><span class="fl">0.001</span>, device<span class="op">=</span>device).to(device) </span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>trainer.train(max_epochs<span class="op">=</span><span class="dv">50</span>) </span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>trainer.test() </span></code></pre></div>
<pre><code>===== Train Start ===== 
Epoch: 0, train loss: 2.293853, train acc: 0.140866, valid loss: 2.280568, valid acc: 0.206149 
Epoch: 5, train loss: 1.959832, train acc: 0.643127, valid loss: 1.855423, valid acc: 0.681788 
Epoch: 10, train loss: 0.902655, train acc: 0.794799, valid loss: 0.778864, valid acc: 0.839550 
Epoch: 15, train loss: 0.577160, train acc: 0.848291, valid loss: 0.483942, valid acc: 0.883401 
Epoch: 20, train loss: 0.465347, train acc: 0.872461, valid loss: 0.383265, valid acc: 0.903058 
Epoch: 25, train loss: 0.410083, train acc: 0.886343, valid loss: 0.335912, valid acc: 0.910282 
Epoch: 30, train loss: 0.377565, train acc: 0.894276, valid loss: 0.308982, valid acc: 0.916667 
Epoch: 35, train loss: 0.354993, train acc: 0.899726, valid loss: 0.289612, valid acc: 0.919691 
Epoch: 40, train loss: 0.337928, train acc: 0.903321, valid loss: 0.276297, valid acc: 0.920867 
Epoch: 45, train loss: 0.323637, train acc: 0.906880, valid loss: 0.265034, valid acc: 0.923723 
===== Test Start ===== 
Test loss: 0.295112, Test acc: 0.914764 </code></pre>
<figure>
<img src="output_44_1.png" alt="png" />
<figcaption aria-hidden="true">png</figcaption>
</figure>
<h3 id="adam">adam</h3>
<div class="sourceCode" id="cb35"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(model_class<span class="op">=</span>LionMNISTClassifier, loaders<span class="op">=</span>loaders, n_class<span class="op">=</span><span class="dv">10</span>, opt<span class="op">=</span><span class="st">&quot;adam&quot;</span>, lr<span class="op">=</span><span class="fl">0.001</span>, device<span class="op">=</span>device).to(device) </span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>trainer.train(max_epochs<span class="op">=</span><span class="dv">50</span>) </span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>trainer.test() </span></code></pre></div>
<pre><code>===== Train Start ===== 
Epoch: 0, train loss: 0.302497, train acc: 0.911477, valid loss: 0.114387, valid acc: 0.966566 
Epoch: 5, train loss: 0.033073, train acc: 0.989417, valid loss: 0.066883, valid acc: 0.982527 
Epoch: 10, train loss: 0.015452, train acc: 0.994921, valid loss: 0.093913, valid acc: 0.980679 
Epoch: 15, train loss: 0.010835, train acc: 0.996664, valid loss: 0.107089, valid acc: 0.981183 
Epoch: 20, train loss: 0.006893, train acc: 0.997924, valid loss: 0.132105, valid acc: 0.980175 
Epoch: 25, train loss: 0.008649, train acc: 0.997294, valid loss: 0.121791, valid acc: 0.979839 
Epoch: 30, train loss: 0.003595, train acc: 0.998869, valid loss: 0.136822, valid acc: 0.983031 
Epoch: 35, train loss: 0.000891, train acc: 0.999778, valid loss: 0.126824, valid acc: 0.984039 
Epoch: 40, train loss: 0.003926, train acc: 0.998647, valid loss: 0.164973, valid acc: 0.981351 
Epoch: 45, train loss: 0.003763, train acc: 0.998869, valid loss: 0.165165, valid acc: 0.979335 
===== Test Start ===== 
Test loss: 0.142515, Test acc: 0.980869 </code></pre>
<figure>
<img src="output_46_1.png" alt="png" />
<figcaption aria-hidden="true">png</figcaption>
</figure>
<p>그래프를 비교해보면 adam이 sgd보다 더 빠르게 accuracy=1에 가까워지고
있음을 알 수 있습니다. 그래프 모양 또한 sgd는 부드러운 곡선을 그리지만
adam은 삐뚤삐뚤한 형태로 수렴하고 있습니다. adam이 더 높은 accuracy를
갖는 모델이지만 훈련시간 또한 긴 것을 확인할 수 있습니다.</p>
<h2 id="learning-rate-scheduler-사용-유무-비교">learning rate scheduler
사용 유무 비교</h2>
<div class="sourceCode" id="cb37"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(model_class<span class="op">=</span>LionMNISTClassifier, loaders<span class="op">=</span>loaders, n_class<span class="op">=</span><span class="dv">10</span>, opt<span class="op">=</span><span class="st">&quot;adam&quot;</span>, lr<span class="op">=</span><span class="fl">0.001</span>, has_scheduler<span class="op">=</span><span class="va">True</span>, device<span class="op">=</span>device).to(device) </span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>trainer.train(max_epochs<span class="op">=</span><span class="dv">50</span>) </span></code></pre></div>
<pre><code>Adjusting learning rate of group 0 to 1.0000e-03. 
===== Train Start ===== 
Adjusting learning rate of group 0 to 1.0000e-03. 
Epoch: 0, train loss: 0.296774, train acc: 0.914591, valid loss: 0.112701, valid acc: 0.966230 
Adjusting learning rate of group 0 to 1.0000e-03. 
Adjusting learning rate of group 0 to 1.0000e-03. 
Adjusting learning rate of group 0 to 1.0000e-03. 
Adjusting learning rate of group 0 to 5.0000e-04. 
Adjusting learning rate of group 0 to 5.0000e-04. 
Epoch: 5, train loss: 0.018088, train acc: 0.994940, valid loss: 0.077383, valid acc: 0.978663 
Adjusting learning rate of group 0 to 5.0000e-04. 
Adjusting learning rate of group 0 to 5.0000e-04. 
Adjusting learning rate of group 0 to 5.0000e-04. 
Adjusting learning rate of group 0 to 2.5000e-04. 
Adjusting learning rate of group 0 to 2.5000e-04. 
Epoch: 10, train loss: 0.002736, train acc: 0.999462, valid loss: 0.073415, valid acc: 0.985047 
Adjusting learning rate of group 0 to 2.5000e-04. 
Adjusting learning rate of group 0 to 2.5000e-04. 
Adjusting learning rate of group 0 to 2.5000e-04. 
Adjusting learning rate of group 0 to 1.2500e-04. 
Adjusting learning rate of group 0 to 1.2500e-04. 
Epoch: 15, train loss: 0.000520, train acc: 0.999944, valid loss: 0.085636, valid acc: 0.984207 
Adjusting learning rate of group 0 to 1.2500e-04. 
Adjusting learning rate of group 0 to 1.2500e-04. 
Adjusting learning rate of group 0 to 1.2500e-04. 
Adjusting learning rate of group 0 to 6.2500e-05. 
Adjusting learning rate of group 0 to 6.2500e-05. 
Epoch: 20, train loss: 0.000101, train acc: 1.000000, valid loss: 0.096484, valid acc: 0.984207 
Adjusting learning rate of group 0 to 6.2500e-05. 
Adjusting learning rate of group 0 to 6.2500e-05. 
Adjusting learning rate of group 0 to 6.2500e-05. 
Adjusting learning rate of group 0 to 3.1250e-05. 
Adjusting learning rate of group 0 to 3.1250e-05. 
Epoch: 25, train loss: 0.000039, train acc: 1.000000, valid loss: 0.103663, valid acc: 0.983703 
Adjusting learning rate of group 0 to 3.1250e-05. 
Adjusting learning rate of group 0 to 3.1250e-05. 
Adjusting learning rate of group 0 to 3.1250e-05. 
Adjusting learning rate of group 0 to 1.5625e-05. 
Adjusting learning rate of group 0 to 1.5625e-05. 
Epoch: 30, train loss: 0.000020, train acc: 1.000000, valid loss: 0.110385, valid acc: 0.983535 
Adjusting learning rate of group 0 to 1.5625e-05. 
Adjusting learning rate of group 0 to 1.5625e-05. 
Adjusting learning rate of group 0 to 1.5625e-05. 
Adjusting learning rate of group 0 to 7.8125e-06. 
Adjusting learning rate of group 0 to 7.8125e-06. 
Epoch: 35, train loss: 0.000013, train acc: 1.000000, valid loss: 0.113576, valid acc: 0.983535 
Adjusting learning rate of group 0 to 7.8125e-06. 
Adjusting learning rate of group 0 to 7.8125e-06. 
Adjusting learning rate of group 0 to 7.8125e-06. 
Adjusting learning rate of group 0 to 3.9063e-06. 
Adjusting learning rate of group 0 to 3.9063e-06. 
Epoch: 40, train loss: 0.000010, train acc: 1.000000, valid loss: 0.115529, valid acc: 0.983535 
Adjusting learning rate of group 0 to 3.9063e-06. 
Adjusting learning rate of group 0 to 3.9063e-06. 
Adjusting learning rate of group 0 to 3.9063e-06. 
Adjusting learning rate of group 0 to 1.9531e-06. 
Adjusting learning rate of group 0 to 1.9531e-06. 
Epoch: 45, train loss: 0.000009, train acc: 1.000000, valid loss: 0.116594, valid acc: 0.983535 
Adjusting learning rate of group 0 to 1.9531e-06. 
Adjusting learning rate of group 0 to 1.9531e-06. 
Adjusting learning rate of group 0 to 1.9531e-06. 
Adjusting learning rate of group 0 to 9.7656e-07. </code></pre>
<figure>
<img src="output_49_1.png" alt="png" />
<figcaption aria-hidden="true">png</figcaption>
</figure>
<div class="sourceCode" id="cb39"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>trainer.test() </span></code></pre></div>
<pre><code>===== Test Start ===== 
Test loss: 0.102079, Test acc: 0.983574 </code></pre>
<p>learning rate scheduler를 사용한 결과 accuracy의 소폭 향상이
있었습니다. 훈련 과정을 보면 점점 learning rate가 줄어들면서 학습이
진행되는 것을 볼 수 있습니다.</p>
<h2 id="dropout-batch-normalization">dropout, batch normalization</h2>
<p>dropout과 batch normalization을 적용하는 방법은 각 layer에
nn.Dropout(), nn.BatchNorm1d()을 넣어주면 됩니다. 모두 accuracy가 소폭
향상한 것을 알 수 있습니다.</p>
<div class="sourceCode" id="cb41"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LionMNISTDropoutClassifier(nn.Module): </span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_class<span class="op">=</span><span class="dv">10</span>): </span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>() </span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> nn.Sequential( </span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>            nn.Flatten(), </span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>, <span class="dv">256</span>), </span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(p<span class="op">=</span><span class="fl">0.5</span>), </span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(), </span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">256</span>, <span class="dv">128</span>), </span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(p<span class="op">=</span><span class="fl">0.5</span>), </span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(), </span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">128</span>, n_class), </span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>        ) </span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x): </span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.model(x) </span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LionMNISTBNClassifier(nn.Module): </span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_class<span class="op">=</span><span class="dv">10</span>): </span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>() </span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb41-24"><a href="#cb41-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> nn.Sequential( </span>
<span id="cb41-25"><a href="#cb41-25" aria-hidden="true" tabindex="-1"></a>            nn.Flatten(), </span>
<span id="cb41-26"><a href="#cb41-26" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>, <span class="dv">256</span>), </span>
<span id="cb41-27"><a href="#cb41-27" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm1d(<span class="dv">256</span>), </span>
<span id="cb41-28"><a href="#cb41-28" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(), </span>
<span id="cb41-29"><a href="#cb41-29" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">256</span>, <span class="dv">128</span>), </span>
<span id="cb41-30"><a href="#cb41-30" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm1d(<span class="dv">128</span>), </span>
<span id="cb41-31"><a href="#cb41-31" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(), </span>
<span id="cb41-32"><a href="#cb41-32" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">128</span>, n_class), </span>
<span id="cb41-33"><a href="#cb41-33" aria-hidden="true" tabindex="-1"></a>        ) </span>
<span id="cb41-34"><a href="#cb41-34" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb41-35"><a href="#cb41-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x): </span>
<span id="cb41-36"><a href="#cb41-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.model(x) </span>
<span id="cb41-37"><a href="#cb41-37" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb41-38"><a href="#cb41-38" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb41-39"><a href="#cb41-39" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LionMNISTDropoutBNClassifier(nn.Module): </span>
<span id="cb41-40"><a href="#cb41-40" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_class<span class="op">=</span><span class="dv">10</span>): </span>
<span id="cb41-41"><a href="#cb41-41" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>() </span>
<span id="cb41-42"><a href="#cb41-42" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb41-43"><a href="#cb41-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> nn.Sequential( </span>
<span id="cb41-44"><a href="#cb41-44" aria-hidden="true" tabindex="-1"></a>            nn.Flatten(), </span>
<span id="cb41-45"><a href="#cb41-45" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>, <span class="dv">256</span>), </span>
<span id="cb41-46"><a href="#cb41-46" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm1d(<span class="dv">256</span>), </span>
<span id="cb41-47"><a href="#cb41-47" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(p<span class="op">=</span><span class="fl">0.5</span>), </span>
<span id="cb41-48"><a href="#cb41-48" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(), </span>
<span id="cb41-49"><a href="#cb41-49" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">256</span>, <span class="dv">128</span>), </span>
<span id="cb41-50"><a href="#cb41-50" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm1d(<span class="dv">128</span>), </span>
<span id="cb41-51"><a href="#cb41-51" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(p<span class="op">=</span><span class="fl">0.5</span>), </span>
<span id="cb41-52"><a href="#cb41-52" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(), </span>
<span id="cb41-53"><a href="#cb41-53" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">128</span>, n_class), </span>
<span id="cb41-54"><a href="#cb41-54" aria-hidden="true" tabindex="-1"></a>        ) </span>
<span id="cb41-55"><a href="#cb41-55" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb41-56"><a href="#cb41-56" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x): </span>
<span id="cb41-57"><a href="#cb41-57" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.model(x) </span>
<span id="cb41-58"><a href="#cb41-58" aria-hidden="true" tabindex="-1"></a> </span></code></pre></div>
<div class="sourceCode" id="cb42"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(model_class<span class="op">=</span>LionMNISTDropoutClassifier, loaders<span class="op">=</span>loaders, n_class<span class="op">=</span><span class="dv">10</span>, opt<span class="op">=</span><span class="st">&quot;adam&quot;</span>, lr<span class="op">=</span><span class="fl">0.001</span>, device<span class="op">=</span>device).to(device) </span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>trainer.train(max_epochs<span class="op">=</span><span class="dv">30</span>) </span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>trainer.test() </span></code></pre></div>
<pre><code>===== Train Start ===== 
Epoch: 0, train loss: 0.471969, train acc: 0.858708, valid loss: 0.140879, valid acc: 0.960853 
Epoch: 5, train loss: 0.131920, train acc: 0.961410, valid loss: 0.073890, valid acc: 0.977991 
Epoch: 10, train loss: 0.099638, train acc: 0.969195, valid loss: 0.063448, valid acc: 0.980847 
Epoch: 15, train loss: 0.086451, train acc: 0.974032, valid loss: 0.061495, valid acc: 0.982359 
Epoch: 20, train loss: 0.076410, train acc: 0.976479, valid loss: 0.066648, valid acc: 0.982023 
Epoch: 25, train loss: 0.070171, train acc: 0.977832, valid loss: 0.065918, valid acc: 0.983031 
===== Test Start ===== 
Test loss: 0.073770, Test acc: 0.981671 </code></pre>
<figure>
<img src="output_54_1.png" alt="png" />
<figcaption aria-hidden="true">png</figcaption>
</figure>
<div class="sourceCode" id="cb44"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(model_class<span class="op">=</span>LionMNISTBNClassifier, loaders<span class="op">=</span>loaders, n_class<span class="op">=</span><span class="dv">10</span>, opt<span class="op">=</span><span class="st">&quot;adam&quot;</span>, lr<span class="op">=</span><span class="fl">0.001</span>, device<span class="op">=</span>device).to(device) </span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>trainer.train(max_epochs<span class="op">=</span><span class="dv">30</span>) </span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>trainer.test() </span></code></pre></div>
<pre><code>===== Train Start ===== 
Epoch: 0, train loss: 0.215938, train acc: 0.940410, valid loss: 0.088783, valid acc: 0.974630 
Epoch: 5, train loss: 0.031933, train acc: 0.989824, valid loss: 0.077726, valid acc: 0.977991 
Epoch: 10, train loss: 0.017131, train acc: 0.993624, valid loss: 0.066482, valid acc: 0.983703 
Epoch: 15, train loss: 0.008775, train acc: 0.997220, valid loss: 0.077114, valid acc: 0.982527 
Epoch: 20, train loss: 0.008320, train acc: 0.997146, valid loss: 0.074517, valid acc: 0.983031 
Epoch: 25, train loss: 0.008707, train acc: 0.996942, valid loss: 0.072421, valid acc: 0.985719 
===== Test Start ===== 
Test loss: 0.072791, Test acc: 0.983774 </code></pre>
<figure>
<img src="output_55_1.png" alt="png" />
<figcaption aria-hidden="true">png</figcaption>
</figure>
<div class="sourceCode" id="cb46"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(model_class<span class="op">=</span>LionMNISTDropoutBNClassifier, loaders<span class="op">=</span>loaders, n_class<span class="op">=</span><span class="dv">10</span>, opt<span class="op">=</span><span class="st">&quot;adam&quot;</span>, lr<span class="op">=</span><span class="fl">0.001</span>, device<span class="op">=</span>device).to(device) </span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>trainer.train(max_epochs<span class="op">=</span><span class="dv">30</span>) </span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>trainer.test() </span></code></pre></div>
<pre><code>===== Train Start ===== 
Epoch: 0, train loss: 0.440807, train acc: 0.872961, valid loss: 0.126382, valid acc: 0.964214 
Epoch: 5, train loss: 0.157185, train acc: 0.952365, valid loss: 0.067443, valid acc: 0.980511 
Epoch: 10, train loss: 0.116907, train acc: 0.963820, valid loss: 0.059610, valid acc: 0.982863 
Epoch: 15, train loss: 0.099826, train acc: 0.968546, valid loss: 0.058426, valid acc: 0.984711 
Epoch: 20, train loss: 0.090204, train acc: 0.971716, valid loss: 0.060830, valid acc: 0.983199 
Epoch: 25, train loss: 0.080905, train acc: 0.974385, valid loss: 0.059024, valid acc: 0.985887 
===== Test Start ===== 
Test loss: 0.057088, Test acc: 0.983073 </code></pre>
<figure>
<img src="output_56_1.png" alt="png" />
<figcaption aria-hidden="true">png</figcaption>
</figure>
<div class="sourceCode" id="cb48"><pre
class="sourceCode r distill-force-highlighting-css"><code class="sourceCode r"></code></pre></div>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>


<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom"></div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>

---
title: "신경망 학습"
description: |
  신경망이 잘 학습되고 있는지 보여줄 수 있는 손실함수(loss function)과 가중치 매개변수 조정을 통해 손실함수를 줄이는 방법에 대해 알아봤습니다.
author:
  - name: nackta
    url: {}
date: 2022-06-21
output:
  distill::distill_article:
    self_contained: false
---



## `r emo::ji("keycap_1")` 손실함수란

신경망의 성능을 나타내는 지표입니다. 신경망은 손실함수를 통해 최적의 매개변수 값을 탐색합니다.

### 오차제곱합(sum of squares for error, SSE)

$$E = \frac{1}{2}\Sigma{(y_k-t_k)^2}$$

$k$ : 데이터 차원의 수

$y_k$ : 예측 값

$t_k$ : 실제 값

SSE는 주로 regression 문제를 해결하기 위해 사용됩니다. 여기서 $\frac{1}{2}$을 곱해주는 이유는 컴퓨터에서 미분을 수월하게 하기 위함입니다.

### 크로스 엔트로피(cross entropy error, CEE)

$$E=-\Sigma{t_k\log{y_k}}$$

$k$ : 데이터 차원의 수

$y_k$ : 예측 값

$t_k$ : 실제 값

하지만 컴퓨터에서 구현될 때는 다음과 같이 delta를 추가합니다. log안에 값이 0이라면 -inf가 되어 더 이상 계산할 수 없기 때문입니다. 그래서 아주 작은 값 delta를 더해서 이를 방지합니다.

```{python eval=FALSE, include=TRUE}
def cross_entropy_error(y, t):
    delta = 1e-7
    retrun -np.sum(t * np.log(y + delta))
```


### 경사하강법(gradient descent method)

신경망에서 최적의 매개변수는 손실함수가 가장 낮은 값을 취하도록 작용해야합니다. 이를 위해 초기값 위치에서 손실함수의 기울기를 구해 최솟값을 찾아 초기값을 갱신하는 방법으로 매개변수를 최적화합니다. 만약 매개변수가 2개라면,

$$x_0 = x_0-\eta\frac{\partial f}{\partial x_0} \\ x_1 = x_1-\eta\frac{\partial f}{\partial x_1}$$

$\eta$ : 학습률(learning rate)

위와 같은 방법을 반복하면서 서서히 손실함수의 값을 줄여갈 수 있습니다. 학습률은 한 번 학습할 때마다 매개변수의 값을 얼마나 갱신할지를 정하는 값입니다.

### 미니배치(mini batch) 학습

손실함수, 기울기, 경사하강법을 알았다면 간단한 신경망을 구현할 수 있습니다. 여기서 미니배치 방법을 추가하면 좀더 효율적인 학습 알고리즘을 구현할 수 있습니다. 데이터가 많아지면 많아질수록 각 데이터별로 손실함수를 구하는 것은 버거울 수 있습니다. 이를 해결하기 위해 학습을 반복할 때마다 훈련데이터의 일부(미니배치)만을 추출해서 학습하는 것을 미니배치 학습이라고 합니다. 

미니배치를 통해 경사하강법으로 매개변수를 갱신하게 되면 무작위로 선정된 데이터를 학습하게 되므로 확룰적 경사하강법(stochastic gradent descent, SGD)라고 합니다. 